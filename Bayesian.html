
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Chatper 6. Bayesian Approaches &#8212; Brain Computation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'Bayesian';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Bayesian Approaches: Exercise" href="Bayesian_Exercise.html" />
    <link rel="prev" title="Unsupervised Learning: Exercise" href="Unsupervised_Exercise.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="BrainComputation.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/BC_logo.png" class="logo__image only-light" alt="Brain Computation - Home"/>
    <script>document.write(`<img src="_static/BC_logo.png" class="logo__image only-dark" alt="Brain Computation - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="BrainComputation.html">
                    Brain Computation: A Hands-on Guidebook
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="Introduction.html">Chapter 1. Introduction</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="Neurons.html">Chapter 2. Neural Modeling and Analysis</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="Neurons_Exercise.html">Chapter 2. Neural Modeling and Analysis: Exercise</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="Supervised.html">Chapter 3: Supervised Learning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="Supervised_Exercise.html">Supervised Learning: Exercise</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="Reinforcement.html">Chapter 4. Reinforcement Learning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="Reinforcement_Exercise.html">Chapter 4. Reinforcement Learning: Exercise</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="Unsupervised.html">Chapter 5. Unsupervised Learning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="Unsupervised_Exercise.html">Unsupervised Learning: Exercise</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="current reference internal" href="#">Chatper 6. Bayesian Approaches</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="Bayesian_Exercise.html">Bayesian Approaches: Exercise</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="Deep.html">Chapter 7: Deep Learning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="Deep_Exercise.html">Chapter 7: Deep Learning: Exercise</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="Multiple.html">Chapter 8. Multiple Agents</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="Multiple_Exercise.html">Chapter 8. Multiple Agents: Exercise</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="Meta.html">Chapter 9. Meta-Learning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="Meta_Exercise.html">Chapter 9. Meta-Learning: Exercise</a></li>
</ul>
</details></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://mybinder.org/v2/gh/oist/BrainComputation/master?urlpath=tree/Bayesian.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Binder"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Binder logo" src="_static/images/logo_binder.svg">
  </span>
<span class="btn__text-container">Binder</span>
</a>
</li>
      
      
      
      
      <li><a href="https://colab.research.google.com/github/oist/BrainComputation/blob/master/Bayesian.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Colab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Colab logo" src="_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>



<a href="https://github.com/oist/BrainComputation" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/Bayesian.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Chatper 6. Bayesian Approaches</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#contents">Contents</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayes-theorem">Bayes’ Theorem</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-mouse-in-a-bush">Example: mouse in a bush</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#iterative-bayesian-inference">Iterative Bayesian Inference</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#coin-toss">Coin toss</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gaussian-observations">Gaussian observations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-approaches-in-machine-learning">Bayesian approaches in machine learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-linear-regression">Bayesian Linear Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#predictive-distribution">Predictive distribution</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-model-comparison">Bayesian model comparison</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#computing-model-evidence">Computing model evidence</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bic-and-aic">BIC and AIC</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-networks">Bayesian networks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#inference-on-a-chain">Inference on a chain</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#markov-chain">Markov chain</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dynamic-bayesian-inference">Dynamic Bayesian Inference</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hidden-markov-model">Hidden Markov model</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#approximate-bayesian-inference">Approximate Bayesian inference</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#variational-inference">Variational inference</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sampling-methods">Sampling methods:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-sensorimotor-processing">Bayesian sensorimotor processing</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-computation-in-the-brain">Bayesian computation in the brain</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-sensorimotor-integration">Bayesian sensorimotor integration</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#probabilistic-population-codes">Probabilistic population codes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#baysian-inference-in-the-cortical-circuit">Baysian inference in the cortical circuit</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="chatper-6-bayesian-approaches">
<h1>Chatper 6. Bayesian Approaches<a class="headerlink" href="#chatper-6-bayesian-approaches" title="Link to this heading">#</a></h1>
<div class="math notranslate nohighlight">
\[ % Latex macros
\newcommand{\mat}[1]{\begin{pmatrix} #1 \end{pmatrix}}
\newcommand{\p}[2]{\frac{\partial #1}{\partial #2}}
\renewcommand{\b}[1]{\boldsymbol{#1}}
\newcommand{\c}[1]{\mathcal{#1}}
\]</div>
<section id="contents">
<h2>Contents<a class="headerlink" href="#contents" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Bayes’ theorem</p></li>
<li><p>Bayesian linear regression (Bishop, Chater 3)</p></li>
<li><p>Bayesian model comparison</p></li>
<li><p>Bayesian networks  (Bishop, Chater 8)</p></li>
<li><p>Dynamic Bayesian inference</p></li>
<li><p>Bayesian Brain</p>
<ul>
<li><p>Sensory psychophysics</p></li>
<li><p>Cortical circuit</p></li>
</ul>
</li>
</ul>
</section>
<section id="bayes-theorem">
<h2>Bayes’ Theorem<a class="headerlink" href="#bayes-theorem" title="Link to this heading">#</a></h2>
<p>From the product rule of the joint probability</p>
<div class="math notranslate nohighlight">
\[  p(X,Y) = p(Y|X) p(X) \]</div>
<p>and the symmetry of joint probability</p>
<div class="math notranslate nohighlight">
\[ p(X,Y)=p(Y,X),\]</div>
<p>we have the relationship</p>
<div class="math notranslate nohighlight">
\[ p(X|Y)p(Y) = p(Y|X)p(X),\]</div>
<p>which brings us to Bayes’ theorem:</p>
<div class="math notranslate nohighlight">
\[  p(X|Y) = \frac{p(Y|X)p(X)}{p(Y)}. \]</div>
<p>Bayes’ theorem relates a conditional probability <span class="math notranslate nohighlight">\(p(X|Y)\)</span> to the one in the opposite direction <span class="math notranslate nohighlight">\(p(Y|X)\)</span>.
This simple formula, however, has turned out to be very insightful in the context of sensory processing and learning.</p>
<p>Suppose <span class="math notranslate nohighlight">\(X\)</span> is an invisible state of your interest, such as a prey or predator hiding in a bush, and <span class="math notranslate nohighlight">\(Y\)</span> is a noisey sensory observation.</p>
<ul class="simple">
<li><p>Your knowledge or assumption about the state probability of different states is represented by <span class="math notranslate nohighlight">\(p(X)\)</span>, called <em>prior probability</em>.</p></li>
<li><p>What sensory input <span class="math notranslate nohighlight">\(Y=y\)</span> is observed if the state is <span class="math notranslate nohighlight">\(X=x\)</span> is represented by a sensory observation model <span class="math notranslate nohighlight">\(p(Y|X)\)</span>.</p></li>
<li><p>For a given sensory observation <span class="math notranslate nohighlight">\(y\)</span>, the probability for such an observation with the state <span class="math notranslate nohighlight">\(x\)</span> <span class="math notranslate nohighlight">\(p(Y=y|X=x)\)</span> is as the <em>likelihood</em> of the state <span class="math notranslate nohighlight">\(x\)</span>. As a function of differnt states <span class="math notranslate nohighlight">\(X\)</span>, <span class="math notranslate nohighlight">\(p(Y=y|X)\)</span> is called the <em>likelihood function</em>.</p></li>
<li><p>The probability of the state <span class="math notranslate nohighlight">\(X\)</span> after observing <span class="math notranslate nohighlight">\(y\)</span>, <span class="math notranslate nohighlight">\(p(X|Y=y)\)</span> is called the <em>posterior probability</em>.</p></li>
</ul>
<p>Bayes’ theorem in this case</p>
<div class="math notranslate nohighlight">
\[  p(X|Y=y) = \frac{p(Y=y|X)p(X)}{p(Y=y)} \]</div>
<div class="math notranslate nohighlight">
\[  \propto p(Y=y|X)p(X) \]</div>
<p>gives a theoretical basis of how to combine the prior knowledge <span class="math notranslate nohighlight">\(p(X)\)</span> and sensory evidence <span class="math notranslate nohighlight">\(y\)</span>.<br />
It is intuitive that the posterior probability <span class="math notranslate nohighlight">\(p(X|Y=y)\)</span> is proportional to the product of the prior prbability <span class="math notranslate nohighlight">\(p(X)\)</span> and the likelihood <span class="math notranslate nohighlight">\(p(Y=y|X)\)</span> for oberving <span class="math notranslate nohighlight">\(y\)</span>.</p>
<p>The denominator <span class="math notranslate nohighlight">\(p(Y=y)\)</span> is called the <em>marginal likelhood</em> and serves as the normalizing factor so that the posterior probability sums or intergrates to one.</p>
<p>The marginal likelhood <span class="math notranslate nohighlight">\(p(Y=y)\)</span> is the probability of observing <span class="math notranslate nohighlight">\(y\)</span> by considering all the possible states <span class="math notranslate nohighlight">\(X\)</span>, and generally computed by marginalizing the joint probability</p>
<div class="math notranslate nohighlight">
\[  p(Y=y) = \sum_X p(Y=y|X=x)p(X=x) \]</div>
<p>or</p>
<div class="math notranslate nohighlight">
\[ p(Y=y) = \int_X p(Y=y|X=x)p(X=x) dx. \]</div>
<section id="example-mouse-in-a-bush">
<h3>Example: mouse in a bush<a class="headerlink" href="#example-mouse-in-a-bush" title="Link to this heading">#</a></h3>
<p>You are a cat chasing a mouse and saw it ran behind three bushes.</p>
<p>You know about half of the case a hiding mouse makes a sound, but about 10% of the time you hear sound just by the wind.
That gives you a sensory observation model:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p><span class="math notranslate nohighlight">\(p(Y \vert X)\)</span></p></th>
<th class="head text-center"><p>no mouse <span class="math notranslate nohighlight">\(X=0\)</span></p></th>
<th class="head text-center"><p>mouse hiding <span class="math notranslate nohighlight">\(X=1\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>no sound <span class="math notranslate nohighlight">\(Y=0\)</span></p></td>
<td class="text-center"><p>0.9</p></td>
<td class="text-center"><p>0.5</p></td>
</tr>
<tr class="row-odd"><td><p>sound <span class="math notranslate nohighlight">\(Y=1\)</span></p></td>
<td class="text-center"><p>0.1</p></td>
<td class="text-center"><p>0.5</p></td>
</tr>
</tbody>
</table>
</div>
<p>Then you heard a rustuling sound from one bush.
Then what is the probability of a mouse hiding behind the bush, <span class="math notranslate nohighlight">\(p(X=1|Y=1)\)</span>?
From the above table, 0.5?</p>
<p>No, actually. In the ‘heard’ row, 0.1 and 0.5 do not sum up to one. They are likelihoods <span class="math notranslate nohighlight">\(p(Y=1|X)\)</span>, but not probability distribution <span class="math notranslate nohighlight">\(p(X|Y=1)\)</span> for the mouse to be in the bush or not.</p>
<p>The mouse should be one of the three bushes, so you would assume that the prior probability of the mouse in this bush is 1/3:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-center"><p>prior probability</p></th>
<th class="head text-center"><p>no mouse <span class="math notranslate nohighlight">\(X=0\)</span></p></th>
<th class="head text-center"><p>mouse hiding <span class="math notranslate nohighlight">\(X=1\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p><span class="math notranslate nohighlight">\(p(X)\)</span></p></td>
<td class="text-center"><p>2/3</p></td>
<td class="text-center"><p>1/3</p></td>
</tr>
</tbody>
</table>
</div>
<p>By having this prior probability, we can use Bayes’ theorem:</p>
<div class="math notranslate nohighlight">
\[  p(X=1|Y=1) = \frac{p(Y=1|X=1)p(X=1)}{p(Y=1|X=0)p(X=0)+p(Y=1|X=1)p(X=1)}  \]</div>
<div class="math notranslate nohighlight">
\[  = \frac{0.5*1/3}{0.1*2/3+0.5*1/3}
    = \frac{5}{7} \simeq 0.71  \]</div>
</section>
</section>
<section id="iterative-bayesian-inference">
<h2>Iterative Bayesian Inference<a class="headerlink" href="#iterative-bayesian-inference" title="Link to this heading">#</a></h2>
<p>A useful property of Bayesian inference is that you can apply it iteratively to incoming data stream.</p>
<p>We denote the sequence of observations up to time <span class="math notranslate nohighlight">\(t\)</span> as</p>
<div class="math notranslate nohighlight">
\[  y_{1:t}=(y_1,...,y_t)  \]</div>
<p>and want to estimate the cause <span class="math notranslate nohighlight">\(x\)</span> of these observations</p>
<div class="math notranslate nohighlight">
\[  p(x|y_{1:t}) = \frac{p(y_{1:t}|x)\ p(x)}{p(y_{1:t})}  \]</div>
<p>If the observations are independent, their joint distribution is a product</p>
<div class="math notranslate nohighlight">
\[  p(y_{1:t}|x) = p(y_1|x)\cdots p(y_t|x)  \]</div>
<p>and thus the posterior can be decomposed as</p>
<div class="math notranslate nohighlight">
\[  p(x|y_{1:t}) = \frac{p(y_1|x)\cdots p(y_{t-1}|x)\ p(y_t|x)\ p(x)}{p(y_1)\cdots p(y_{t-1})\ p(y_t)}  \]</div>
<div class="math notranslate nohighlight">
\[  = \frac{p(y_t|x)}{p(y_t)}\ \frac{p(y_1|x)\cdots p(y_{t-1}|x)\ p(x)}{p(y_1)\cdots p(y_{t-1}) }  \]</div>
<div class="math notranslate nohighlight">
\[  = \frac{p(y_t|x)\ p(x|y_{1:t-1})}{p(y_t)}.  \]</div>
<p>This means that the posterior <span class="math notranslate nohighlight">\(p(x|y_{1:t-1})\)</span> that you computed by time <span class="math notranslate nohighlight">\(t-1\)</span> serves as the prior to be combined with the likelihood for the new coming data <span class="math notranslate nohighlight">\(p(y_t|x)\)</span> for computing the new posterior <span class="math notranslate nohighlight">\(p(x|y_{1:t})\)</span>.</p>
<p>This iterative update of the posterior is practically helpful in online inference utilizing whatever data available so far.</p>
<section id="coin-toss">
<h3>Coin toss<a class="headerlink" href="#coin-toss" title="Link to this heading">#</a></h3>
<p>Here is a simple example of estimating the parameter <span class="math notranslate nohighlight">\(\mu\)</span>, probability for a coin to land head up, during multiple tosses.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># take samples</span>
<span class="n">mu</span> <span class="o">=</span> <span class="mf">0.4</span>  <span class="c1"># probability of head</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">10</span>   <span class="c1"># number of samples</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="o">-</span><span class="n">mu</span><span class="p">,</span> <span class="n">mu</span><span class="p">])</span> <span class="c1"># binary observation sequence</span>
<span class="n">y</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([1, 1, 0, 0, 1, 0, 1, 0, 0, 0])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dx</span> <span class="o">=</span> <span class="mf">0.01</span>  <span class="c1"># plot step</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dx</span><span class="p">)</span>  <span class="c1"># range of the parameter</span>
<span class="n">prior</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>  <span class="c1"># Assume a uniform prior</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="n">N</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># a new figure</span>
    <span class="c1"># prior</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span> <span class="n">x</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">)</span>
    <span class="c1"># observation</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span> <span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">N</span><span class="p">,</span> <span class="s1">&#39;ko&#39;</span><span class="p">)</span>
    <span class="c1"># likelihood</span>
    <span class="n">likelihood</span> <span class="o">=</span> <span class="n">x</span><span class="o">*</span><span class="n">y</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">x</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">y</span><span class="p">[</span><span class="n">t</span><span class="p">])</span> <span class="c1"># theta if head, 1-theta if tail</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span> <span class="n">x</span><span class="p">,</span> <span class="n">likelihood</span><span class="p">,</span> <span class="s1">&#39;g&#39;</span><span class="p">)</span>
    <span class="c1"># product</span>
    <span class="n">prilik</span> <span class="o">=</span> <span class="n">prior</span><span class="o">*</span><span class="n">likelihood</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span> <span class="n">x</span><span class="p">,</span> <span class="n">prilik</span><span class="p">,</span><span class="s1">&#39;c&#39;</span><span class="p">)</span>
    <span class="c1"># posterior by normalization</span>
    <span class="n">marginal</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">prilik</span><span class="p">)</span><span class="o">*</span><span class="n">dx</span>    <span class="c1"># integrate over the parameter range</span>
    <span class="n">posterior</span> <span class="o">=</span> <span class="n">prilik</span><span class="o">/</span><span class="n">marginal</span>  <span class="c1"># normalize</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span> <span class="n">x</span><span class="p">,</span> <span class="n">posterior</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\mu$&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">t</span><span class="o">==</span><span class="mi">0</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">((</span><span class="s1">&#39;prior&#39;</span><span class="p">,</span> <span class="s1">&#39;observation&#39;</span><span class="p">,</span> <span class="s1">&#39;likelihood&#39;</span><span class="p">,</span> <span class="s1">&#39;prior*like.&#39;</span><span class="p">,</span> <span class="s1">&#39;posterior&#39;</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;t = </span><span class="si">{</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="c1"># posterior as a new prior</span>
    <span class="n">prior</span> <span class="o">=</span> <span class="n">posterior</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/804b2779ad987e740a825547efd84a52cda94fe163c42804f018ed022c92e2c3.png" src="_images/804b2779ad987e740a825547efd84a52cda94fe163c42804f018ed022c92e2c3.png" />
</div>
</div>
<p>As more data are collected, the posterior distribution of <span class="math notranslate nohighlight">\(\mu\)</span> becomes sharper and colser to the true value.</p>
<p>For binary observations <span class="math notranslate nohighlight">\(y_{1:n}=(y_1,...,y_n)\)</span>, under uniform prior probability, the posterior probability of the mean <span class="math notranslate nohighlight">\(mu\)</span> is</p>
<div class="math notranslate nohighlight">
\[ p(\mu|y_{1:n}) \propto \mu^{n_1} (1-\mu)^{n_0} \]</div>
<p>where <span class="math notranslate nohighlight">\(n_1\)</span> and <span class="math notranslate nohighlight">\(n_0\)</span> are the number of observations of <span class="math notranslate nohighlight">\(1\)</span> and <span class="math notranslate nohighlight">\(0\)</span>, respectively. This is an example of <em>beta distribution</em>:</p>
<div class="math notranslate nohighlight">
\[ p(x; \alpha, \beta) \propto x^{\alpha-1} (1-x)^{\beta-1} \]</div>
<p>with <span class="math notranslate nohighlight">\(\alpha=n_1+1\)</span> and <span class="math notranslate nohighlight">\(\beta=n_0+1\)</span>. A uniform prior distribution is represented by <span class="math notranslate nohighlight">\(\alpha = \beta = 1\)</span>.</p>
<p>If a prior and the posterior are represented by the same class of distribution, the prior is called the conjugate prior for the observation model. Beta distribution is the conjugate prior for the binary (Bernoulli) observation.</p>
</section>
</section>
<section id="gaussian-observations">
<h2>Gaussian observations<a class="headerlink" href="#gaussian-observations" title="Link to this heading">#</a></h2>
<p>Estimate the mean <span class="math notranslate nohighlight">\(\mu\)</span> and the standard deviationn <span class="math notranslate nohighlight">\(\sigma\)</span> from noisy observations.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Noisy observation: y = N(mu,sigma)</span>
<span class="n">mu</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">sigma</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">mu</span> <span class="o">+</span> <span class="n">sigma</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[-1.00406966 -0.36834093  1.04267151  2.74659508  1.49144458  0.8525643
  1.62484305 -1.76929775 -1.99285274  1.27830984]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Start from a uniform prior</span>
<span class="n">rmu</span> <span class="o">=</span> <span class="mi">5</span><span class="p">;</span> <span class="n">dmu</span> <span class="o">=</span> <span class="mf">0.2</span>   <span class="c1"># range and step of mu</span>
<span class="n">rsig</span> <span class="o">=</span> <span class="mi">5</span><span class="p">;</span>  <span class="n">dsig</span> <span class="o">=</span> <span class="mf">0.2</span> <span class="c1"># range and step of sigma</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="n">rmu</span><span class="p">,</span> <span class="n">rmu</span><span class="p">,</span> <span class="n">dmu</span><span class="p">)</span>
<span class="n">s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">rsig</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="n">dsig</span><span class="p">)</span>
<span class="n">M</span><span class="p">,</span> <span class="n">S</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>  <span class="c1"># grid of mu and sigma</span>
<span class="n">prior</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">M</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">rmu</span><span class="o">*</span><span class="n">rsig</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="c1"># observation</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span> <span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span> <span class="n">dsig</span><span class="o">+</span><span class="n">rsig</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">N</span><span class="p">,</span> <span class="s1">&#39;w+&#39;</span><span class="p">)</span>
    <span class="c1"># likelihood</span>
    <span class="n">likelihood</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">((</span><span class="n">y</span><span class="p">[</span><span class="n">n</span><span class="p">]</span><span class="o">-</span><span class="n">M</span><span class="p">)</span><span class="o">/</span><span class="n">S</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span><span class="o">*</span><span class="n">S</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">likelihood</span><span class="p">,</span> <span class="n">extent</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="n">rmu</span><span class="p">,</span><span class="n">rmu</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="n">rsig</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\mu$&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\sigma$&#39;</span><span class="p">);</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;likelihood </span><span class="si">{</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">shrink</span><span class="o">=</span><span class="mf">0.25</span><span class="p">)</span>
    <span class="c1"># posterior</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">prilik</span> <span class="o">=</span> <span class="n">prior</span><span class="o">*</span><span class="n">likelihood</span>
    <span class="c1">#plt.imshow(prilik, extent=(-rmu,rmu,0,rsig))</span>
    <span class="n">marginal</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">prilik</span><span class="p">)</span><span class="o">*</span><span class="n">dmu</span><span class="o">*</span><span class="n">dsig</span>
    <span class="n">posterior</span> <span class="o">=</span> <span class="n">prilik</span><span class="o">/</span><span class="n">marginal</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">posterior</span><span class="p">,</span> <span class="n">extent</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="n">rmu</span><span class="p">,</span><span class="n">rmu</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="n">rsig</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\mu$&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\sigma$&#39;</span><span class="p">);</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;posterior </span><span class="si">{</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">prior</span> <span class="o">=</span> <span class="n">posterior</span>  <span class="c1"># new prior</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">shrink</span><span class="o">=</span><span class="mf">0.25</span><span class="p">)</span>
    <span class="c1"># true value</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span> <span class="p">[</span><span class="n">mu</span><span class="p">],</span> <span class="p">[</span><span class="n">sigma</span><span class="p">],</span> <span class="s1">&#39;r*&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/c4988e52df67cd829ad6e7acd5ff8240fc098986720a30ba1d2f3c98a75a3840.png" src="_images/c4988e52df67cd829ad6e7acd5ff8240fc098986720a30ba1d2f3c98a75a3840.png" />
<img alt="_images/af0f409190af27644126792a37fb14e7b410cfa9b72ed05d60e65f707695efaa.png" src="_images/af0f409190af27644126792a37fb14e7b410cfa9b72ed05d60e65f707695efaa.png" />
<img alt="_images/7be080f9e8cbfd39bde44595dfd94adf58f45af14899af48e244c12dc222a0c5.png" src="_images/7be080f9e8cbfd39bde44595dfd94adf58f45af14899af48e244c12dc222a0c5.png" />
<img alt="_images/fa6e47f6dbb9fd2c1eae5e030e2858707109f9292bd6210f1cdf3ab46ab355a4.png" src="_images/fa6e47f6dbb9fd2c1eae5e030e2858707109f9292bd6210f1cdf3ab46ab355a4.png" />
<img alt="_images/9b5e15da4a3b652cb4ffcfc2396b8fc976e32b3e322c1229b8136698f5efdcaa.png" src="_images/9b5e15da4a3b652cb4ffcfc2396b8fc976e32b3e322c1229b8136698f5efdcaa.png" />
<img alt="_images/93925976bea46ebda555d157e0d1144122786532ea9f06d799a316dd4285e8ee.png" src="_images/93925976bea46ebda555d157e0d1144122786532ea9f06d799a316dd4285e8ee.png" />
<img alt="_images/2c1d92ccc5a4ca23d19448ea9b5edadc3f2e4c1a1a7271ffb509912e73d9d875.png" src="_images/2c1d92ccc5a4ca23d19448ea9b5edadc3f2e4c1a1a7271ffb509912e73d9d875.png" />
<img alt="_images/8efda1e396b878b63f39b1c48875c6a3eb26d2dcb456533474eeb6baaf037c07.png" src="_images/8efda1e396b878b63f39b1c48875c6a3eb26d2dcb456533474eeb6baaf037c07.png" />
<img alt="_images/00506f8ae54b3c4fa2b8333ab0c524571f41b9a2a5555d1a73ca94828bcf26ad.png" src="_images/00506f8ae54b3c4fa2b8333ab0c524571f41b9a2a5555d1a73ca94828bcf26ad.png" />
<img alt="_images/8eae433510b14bcd28cbb137191acc91bedeae61214f3f777a60547c4aae9fe8.png" src="_images/8eae433510b14bcd28cbb137191acc91bedeae61214f3f777a60547c4aae9fe8.png" />
</div>
</div>
<p>Here we consider a case where <span class="math notranslate nohighlight">\(y\)</span> is an observation of <span class="math notranslate nohighlight">\(x\)</span> with Gaussian noise</p>
<div class="math notranslate nohighlight">
\[ p(y|x) \propto e^{-\frac{(y-x)^2}{2\sigma_1^2}} \]</div>
<p>If we assume that the prior distribution of <span class="math notranslate nohighlight">\(x\)</span> is also Gaussian</p>
<div class="math notranslate nohighlight">
\[ p(x) \propto e^{-\frac{(x-\mu_0)^2}{2\sigma_0^2}} \]</div>
<p>then the posterior distribution also takes a Gaussian form:</p>
<div class="math notranslate nohighlight">
\[ p(x|y) \propto p(x)p(y|x)
    \propto e^{-\frac{(x-\mu_0)^2}{2\sigma_0^2}} e^{-\frac{(y-x))^2}{2\sigma_1(x)^2}} 
    = e^{-\frac{(x-\mu_0)^2}{2\sigma_0^2} - \frac{(x-y)^2}{2\sigma_1^2}} 
    \propto e^{-\frac{(x-\mu_2)^2}{2\sigma_2^2}} \]</div>
<p>By considering the coefficients of <span class="math notranslate nohighlight">\(x^2\)</span> and <span class="math notranslate nohighlight">\(x\)</span> of the exponent, we have</p>
<div class="math notranslate nohighlight">
\[ \frac{1}{\sigma_0^2}+\frac{1}{\sigma_1^2} = \frac{1}{\sigma_2^2} \]</div>
<div class="math notranslate nohighlight">
\[ \frac{\mu_0}{\sigma_0^2}+\frac{y}{\sigma_1^2} = \frac{\mu_2}{\sigma_2^2} \]</div>
<p>From these we find the mean and the variance of the posterior as</p>
<div class="math notranslate nohighlight">
\[ \sigma_2^2 = \frac{1}{\frac{1}{\sigma_0^2}+\frac{1}{\sigma_1^2}}
    = \frac{\sigma_0^2 \sigma_1^2}{\sigma_0^2+\sigma_1^2} \]</div>
<div class="math notranslate nohighlight">
\[ \mu_2 = \frac{\sigma_2^2}{\sigma_0^2}\mu_0+\frac{\sigma_2^2}{\sigma_1^2}y 
    = \frac{\sigma_1^2}{\sigma_0^2+\sigma_1^2}\mu_0+\frac{\sigma_0^2}{\sigma_0^2+\sigma_1^2}y \]</div>
<p>The mean of the posterior is a weighted average of the mean of the prior and the new observation; larger the weight for smaller the variance.</p>
<p>If there are multiple independent observations, such as by vision, audition, and haptics, the same weighting based on the ratio of the variance of sensory obervation is expected.</p>
<p>For a Gaussian likelihood function, the conjugate priors for the mean <span class="math notranslate nohighlight">\(\mu\)</span> is also a Gaussian.
As we saw many <span class="math notranslate nohighlight">\(\frac{1}{\sigma^2}\)</span> above, it is often more convenient to parameterize a Gaussian distribution by the inverse variance, or the precision <span class="math notranslate nohighlight">\(\lambda=\frac{1}{\sigma^2}\)</span>.
For the presicision, the conjugate prior is a Gamma distribution</p>
<div class="math notranslate nohighlight">
\[ p(\lambda) \propto \lambda^{\alpha-1}e^{-\beta\lambda}. \]</div>
</section>
<section id="bayesian-approaches-in-machine-learning">
<h2>Bayesian approaches in machine learning<a class="headerlink" href="#bayesian-approaches-in-machine-learning" title="Link to this heading">#</a></h2>
<p>“Bayesian” is quite popular in machine learning, but it is used for different meanings:</p>
<ul class="simple">
<li><p>To combine prior knowledge and the likelihood from observation</p></li>
<li><p>To assume a graphical model of data generation for estimation of the causes</p></li>
<li><p>To estimate the distribution of a variable, not a single point</p></li>
</ul>
<p>In supervised learning:</p>
<ul class="simple">
<li><p>avoid over fitting by introducing a prior distribution on the parameters</p></li>
<li><p>compare models by their probability of producing observed data</p></li>
</ul>
<p>In reinforcement learning:</p>
<ul class="simple">
<li><p>infer the environmental state from incomplete observation</p></li>
<li><p>estimate the distribution of reward, not just the expectation</p></li>
</ul>
<p>In unsupervised learning:</p>
<ul class="simple">
<li><p>infer hidden variables behind data</p>
<ul>
<li><p>e.g. responsibility in Mixtures of Gaussians</p></li>
</ul>
</li>
</ul>
</section>
<section id="bayesian-linear-regression">
<h2>Bayesian Linear Regression<a class="headerlink" href="#bayesian-linear-regression" title="Link to this heading">#</a></h2>
<p>The standard linear regression (Chapter 3) assumes a linear regression function with additive noise
$<span class="math notranslate nohighlight">\( t_n = \b{w}^T\b{x}_n + \epsilon \)</span><span class="math notranslate nohighlight">\(
where \)</span>p(\epsilon)=\mathcal{N}(0,\beta^{-1})$.</p>
<p>In Bayesian linear regression, we assume that the weights are sampled from a prior distribution <span class="math notranslate nohighlight">\(p(\b{w})=\mathcal{N}(\b{0},\alpha^{-1}I)\)</span>.</p>
<p>The likelihood of the parameter <span class="math notranslate nohighlight">\(\b{w}\)</span> for the target output <span class="math notranslate nohighlight">\(\b{t}\)</span> is</p>
<div class="math notranslate nohighlight">
\[ p(\b{t}|X, \b{w}, \beta) = \prod_{n=1}^N \mathcal{N}(t_n|\b{w}^T\b{x}_n,\beta^{-1}) \]</div>
<p>When both the prior and likelihood are Gaussian, the posterior will also be Gaussian and have the form:</p>
<div class="math notranslate nohighlight">
\[ p(\b{w}|\b{t}) = \mathcal{N}(\b{w}|\b{m},S) \]</div>
<p>where the mean of the posterior weights is given as</p>
<div class="math notranslate nohighlight">
\[ \b{m} = \beta S X^T \bf{t} \]</div>
<p>and the variance as</p>
<div class="math notranslate nohighlight">
\[ S = (\alpha I + \beta X^T X)^{-1} \]</div>
<p>If we let <span class="math notranslate nohighlight">\(\alpha=0\)</span>, i.e. infinitely large variance for the weight prior, this is equivalent to regular linear regression.</p>
<p>The log posterior probability of weights is given by</p>
<div class="math notranslate nohighlight">
\[ \log p(\b{w}|\b{t}) = - \frac{\beta}{2}\sum_{n=1}^N \{t_n - \b{w}^T \b{x}_n\}^2
- \frac{\alpha}{2} \b{w}^T\b{w} +\mbox{const.}\]</div>
<p>This presents a link with a common method of adding a penalty term for the size of the weights, or equivallently adding diagonal component in the data correlation matrix, know as <em>ridge regression</em>, which minimizes</p>
<div class="math notranslate nohighlight">
\[ E(\b{w}) = \frac{1}{2}\sum_{n=1}^N \{t_n - \b{w}^T \b{x}_n\}^2
+ \frac{\lambda}{2} \b{w}^T\b{w} \]</div>
<p>The Bayesian regression gives a probabilistic interpretation of the regularization parameter as <span class="math notranslate nohighlight">\(\lambda=\frac{\alpha}{\beta}\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># We will use this frequently</span>
<span class="k">def</span><span class="w"> </span><span class="nf">gauss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Gaussian distribution&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">((</span><span class="n">x</span><span class="o">-</span><span class="n">mu</span><span class="p">)</span><span class="o">/</span><span class="n">sigma</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span><span class="o">*</span><span class="n">sigma</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Distributions in the parameter and data spaces</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">1.</span>  <span class="c1"># inverse variance of weight prior</span>
<span class="n">beta</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># inverse variance of observation noise</span>
<span class="n">wt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>  <span class="c1"># &#39;true&#39; weights</span>
<span class="c1"># sample data</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">6</span>
<span class="n">xrange</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">xrange</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">xrange</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">size</span><span class="o">=</span><span class="n">N</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">N</span><span class="p">),</span> <span class="n">X</span><span class="p">]</span>  <span class="c1"># prepend 1 in the leftmost column</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">wt</span><span class="nd">@X</span><span class="o">.</span><span class="n">T</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">N</span><span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">beta</span><span class="p">)</span>  <span class="c1"># targets with noise</span>
<span class="c1"># for weight space visualization</span>
<span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">30</span><span class="p">)</span>
<span class="n">W0</span><span class="p">,</span> <span class="n">W1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span>
<span class="n">K</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># weight samples</span>
<span class="n">pw</span> <span class="o">=</span> <span class="n">gauss</span><span class="p">(</span><span class="n">W0</span><span class="p">)</span><span class="o">*</span><span class="n">gauss</span><span class="p">(</span><span class="n">W1</span><span class="p">)</span>
<span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
    <span class="c1"># likelihood</span>
    <span class="n">like</span> <span class="o">=</span> <span class="n">gauss</span><span class="p">(</span><span class="n">t</span><span class="p">[</span><span class="n">n</span><span class="p">]</span> <span class="o">-</span> <span class="p">(</span><span class="n">W0</span><span class="o">+</span><span class="n">W1</span><span class="o">*</span><span class="n">X</span><span class="p">[</span><span class="n">n</span><span class="p">,</span><span class="mi">1</span><span class="p">]),</span> <span class="n">sigma</span><span class="o">=</span><span class="n">beta</span><span class="o">**</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>   <span class="c1"># left</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">pcolormesh</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">like</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;square&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Likelihood&quot;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;w0&quot;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;w1&quot;</span><span class="p">);</span>
    <span class="c1"># new posterior</span>
    <span class="n">S</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">alpha</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">beta</span><span class="o">*</span><span class="n">X</span><span class="p">[:</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="nd">@X</span><span class="p">[:</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">beta</span><span class="o">*</span><span class="n">S</span><span class="nd">@X</span><span class="p">[:</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="nd">@t</span><span class="p">[:</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
    <span class="c1"># print(n, &#39;: m =&#39;, m, &#39;; S =&#39;, S)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">post</span> <span class="o">=</span> <span class="n">pw</span><span class="o">*</span><span class="n">like</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">pcolormesh</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">post</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;square&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Posterior&quot;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;w0&quot;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;w1&quot;</span><span class="p">);</span>
    <span class="c1"># sample weights</span>
    <span class="n">wpost</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">K</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">wpost</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">wpost</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;w+&quot;</span><span class="p">)</span>
    <span class="c1"># plot model samples</span>
    <span class="n">xrange</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">])</span>   <span class="c1"># range of input x</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">K</span><span class="p">):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xrange</span><span class="p">,</span> <span class="n">wpost</span><span class="p">[</span><span class="n">k</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">+</span><span class="n">wpost</span><span class="p">[</span><span class="n">k</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">xrange</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="c1"># true line</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xrange</span><span class="p">,</span> <span class="n">wt</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">+</span><span class="n">wt</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">xrange</span><span class="p">,</span> <span class="s2">&quot;k&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">[:</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="n">t</span><span class="p">[:</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;k+&quot;</span><span class="p">)</span>  <span class="c1"># training data</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span><span class="o">.</span><span class="n">set_box_aspect</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Model samples&quot;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;y&quot;</span><span class="p">);</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>  <span class="c1"># adjust subplot margins</span>
    <span class="n">pw</span> <span class="o">=</span> <span class="n">post</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/63ad0ea0b5eb2133eb4401977587c975484d98e6214009890392288e9a49f936.png" src="_images/63ad0ea0b5eb2133eb4401977587c975484d98e6214009890392288e9a49f936.png" />
<img alt="_images/879c16ca9c2748f446683f35e464fe606d354f08e59b27b75041542a94e36ca2.png" src="_images/879c16ca9c2748f446683f35e464fe606d354f08e59b27b75041542a94e36ca2.png" />
<img alt="_images/eba2ed01eb2793b20faa59cc3ef8b45eb2cc2e81d51570ac017fe7a72678540d.png" src="_images/eba2ed01eb2793b20faa59cc3ef8b45eb2cc2e81d51570ac017fe7a72678540d.png" />
<img alt="_images/23594a1e2d95700959c5dd51d4c3ff421221b07df7c60350b5b54577bb3037bd.png" src="_images/23594a1e2d95700959c5dd51d4c3ff421221b07df7c60350b5b54577bb3037bd.png" />
<img alt="_images/e057b5949c1d617b795dd85ea429524b88589b946b084d1b0a26b60abba7e18d.png" src="_images/e057b5949c1d617b795dd85ea429524b88589b946b084d1b0a26b60abba7e18d.png" />
<img alt="_images/c3d213bd7a3318b426c98e6d48d60645a6362d99535ae5b6a319fa71799a978e.png" src="_images/c3d213bd7a3318b426c98e6d48d60645a6362d99535ae5b6a319fa71799a978e.png" />
</div>
</div>
<section id="predictive-distribution">
<h3>Predictive distribution<a class="headerlink" href="#predictive-distribution" title="Link to this heading">#</a></h3>
<p>In Bayesian regression, the result is not one weight vector, but a distribution in the weight space. Then it is reasonable to consider the distribution of the output considering such uncertainty in the weigts.</p>
<p>The output <span class="math notranslate nohighlight">\(y\)</span> for a new input <span class="math notranslate nohighlight">\(\b{x}\)</span> should have the distribution</p>
<div class="math notranslate nohighlight">
\[ p(y|\b{x},\b{t},\alpha,\beta) 
 = \int p(y|\b{x},\b{w},\beta)  p(\b{w}|\b{t},\alpha,\beta) d\b{w} \]</div>
<div class="math notranslate nohighlight">
\[ = \mathcal{N}(t|\b{m}^T \b{x}, \sigma^2(\b{x})) \]</div>
<p>where the variance of the output is given by</p>
<div class="math notranslate nohighlight">
\[ \sigma^2(\b{x}) = \beta^{-1} + \b{x}^T S \b{x} \]</div>
<p>Let us see the example of approximating a sine function by Gaussian basis functions.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1D Gaussian basis functions </span>
<span class="k">def</span><span class="w"> </span><span class="nf">gbf1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">xrange</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mf">1.</span><span class="p">,</span><span class="mf">1.</span><span class="p">],</span> <span class="n">M</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Gaussian basis functions: x can be a 1D array&quot;&quot;&quot;</span>
    <span class="n">xc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">xrange</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">xrange</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">num</span><span class="o">=</span><span class="n">M</span><span class="p">)</span>  <span class="c1"># centers</span>
    <span class="n">xd</span> <span class="o">=</span> <span class="p">(</span><span class="n">xc</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="n">xc</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>  <span class="c1"># interval</span>
    <span class="c1"># x can be an array for N data points</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">x</span><span class="p">,[</span><span class="n">M</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">T</span> <span class="o">-</span> <span class="n">xc</span><span class="p">)</span><span class="o">/</span><span class="n">xd</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
<span class="c1"># example</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">gbf1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">M</span><span class="o">=</span><span class="mi">10</span><span class="p">));</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;phi&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/6502caeed6c5b16f748e795ba9f301cc7fc8619a4f7b1b1191eace8947dedaf9.png" src="_images/6502caeed6c5b16f748e795ba9f301cc7fc8619a4f7b1b1191eace8947dedaf9.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">blr</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">10.</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Bayesian linear regression</span>
<span class="sd">    alpha: inv. variance of weight prior </span>
<span class="sd">    beta: inv. variance of observation noise</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">N</span><span class="p">,</span> <span class="n">D</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">S</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">alpha</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">D</span><span class="p">)</span> <span class="o">+</span> <span class="n">beta</span><span class="o">*</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="nd">@X</span><span class="p">)</span> <span class="c1"># posterior covariance</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">beta</span><span class="o">*</span><span class="n">S</span><span class="nd">@X</span><span class="o">.</span><span class="n">T</span><span class="nd">@t</span>   <span class="c1"># posterior mean</span>
    <span class="k">return</span> <span class="n">m</span><span class="p">,</span> <span class="n">S</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">target</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Target function&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Training data</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">eps</span> <span class="o">=</span> <span class="mf">0.2</span>  <span class="c1"># noise size</span>
<span class="n">xr</span> <span class="o">=</span> <span class="mi">4</span>   <span class="c1"># range of x</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="n">xr</span><span class="p">,</span> <span class="n">xr</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">N</span><span class="p">)</span>  
<span class="n">f</span> <span class="o">=</span> <span class="n">target</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># target function</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">f</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">N</span><span class="p">)</span> <span class="c1"># with noise</span>
<span class="c1"># data for testing/plotting</span>
<span class="n">Np</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">xp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="n">xr</span><span class="p">,</span> <span class="n">xr</span><span class="p">,</span> <span class="n">Np</span><span class="p">)</span>
<span class="n">fp</span> <span class="o">=</span> <span class="n">target</span><span class="p">(</span><span class="n">xp</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xp</span><span class="p">,</span> <span class="n">fp</span><span class="p">,</span> <span class="s2">&quot;g&quot;</span><span class="p">)</span>  <span class="c1"># target function</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="s2">&quot;ro&quot;</span><span class="p">);</span>  <span class="c1"># training data</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;y&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/1078f4dc5976cf26f0f264eaae8259861a80d809583a2b30359555b14ea49fab.png" src="_images/1078f4dc5976cf26f0f264eaae8259861a80d809583a2b30359555b14ea49fab.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">M</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># number of basis functions</span>
<span class="n">Phi</span> <span class="o">=</span> <span class="n">gbf1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="n">xr</span><span class="p">,</span><span class="n">xr</span><span class="p">],</span> <span class="n">M</span><span class="p">)</span>  <span class="c1"># Gaussian basis functions</span>
<span class="n">m</span><span class="p">,</span> <span class="n">S</span> <span class="o">=</span> <span class="n">blr</span><span class="p">(</span><span class="n">Phi</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>  <span class="c1"># Bayesian linear regression</span>
<span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
<span class="c1"># test data</span>
<span class="n">Phip</span> <span class="o">=</span> <span class="n">gbf1</span><span class="p">(</span><span class="n">xp</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="n">xr</span><span class="p">,</span><span class="n">xr</span><span class="p">],</span> <span class="n">M</span><span class="p">)</span>
<span class="n">yp</span> <span class="o">=</span> <span class="n">Phip</span><span class="nd">@m</span><span class="o">.</span><span class="n">T</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xp</span><span class="p">,</span> <span class="n">fp</span><span class="p">,</span> <span class="s2">&quot;g&quot;</span><span class="p">)</span>  <span class="c1"># target function</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="s2">&quot;ro&quot;</span><span class="p">);</span>  <span class="c1"># training data</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xp</span><span class="p">,</span> <span class="n">yp</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">);</span>  <span class="c1"># MAP estimate</span>
<span class="c1"># predictive distribution</span>
<span class="n">sigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">beta</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">Phip</span><span class="nd">@S</span><span class="o">*</span><span class="n">Phip</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xp</span><span class="p">,</span> <span class="n">yp</span><span class="o">+</span><span class="n">sigma</span><span class="p">,</span> <span class="s2">&quot;c&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xp</span><span class="p">,</span> <span class="n">yp</span><span class="o">-</span><span class="n">sigma</span><span class="p">,</span> <span class="s2">&quot;c&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;y&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[-5.23005324e-04 -4.74518285e-02 -5.59935804e-01 -6.61877086e-01
 -4.88333024e-02 -8.25684349e-02  1.00440932e+00  2.43531724e-01
  2.57087316e-01 -5.65933469e-01]
</pre></div>
</div>
<img alt="_images/29d58e581c0bf30618768279d5f3991ea5bd5cd70fa11723116656482cd026c2.png" src="_images/29d58e581c0bf30618768279d5f3991ea5bd5cd70fa11723116656482cd026c2.png" />
</div>
</div>
<p>See how <span class="math notranslate nohighlight">\(N\)</span>, <span class="math notranslate nohighlight">\(M\)</span>, <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> affect the performance.</p>
</section>
</section>
<section id="bayesian-model-comparison">
<h2>Bayesian model comparison<a class="headerlink" href="#bayesian-model-comparison" title="Link to this heading">#</a></h2>
<p>We have so far considered Bayesian inference of the parameter <span class="math notranslate nohighlight">\(\b{w}\)</span> for a given model <span class="math notranslate nohighlight">\(\c{M}\)</span>, such as a regression model with some input variables, but we can also think of Bayesian inference of probability over models <span class="math notranslate nohighlight">\(\c{M}_i\)</span>, such as regression models with different choices of input variables, given data <span class="math notranslate nohighlight">\(\c{D}\)</span></p>
<div class="math notranslate nohighlight">
\[ p(\c{M}_i|\c{D}) \propto p(\c{M}_i) p(\c{D}|\c{M}_i). \]</div>
<p>Here <span class="math notranslate nohighlight">\(p(\c{D}|\c{M}_i)\)</span>, the likelihood of a model given data, is called the <em>evidence</em> of the model.</p>
<p>If we include a model explicitly in our Bayesian parameter estimation, we have</p>
<div class="math notranslate nohighlight">
\[ p(\b{w}|\c{D},\c{M}_i) = \frac{p(\c{D}|\b{w},\c{M}_i)p(\b{w}|\c{M}_i)}{p(\c{D}|\c{M}_i)}, \]</div>
<p>where we have the <em>evidence</em> as the normalizing denominator</p>
<div class="math notranslate nohighlight">
\[ p(\c{D}|\c{M}_i) = \int p(\c{D}|\b{w},\c{M}_i)p(\b{w}|\c{M}_i) d\b{w}. \]</div>
<p>This is also called <em>marginal likelihood</em> because it is the likelihood of the model with its parameters marginalized.</p>
<section id="computing-model-evidence">
<h3>Computing model evidence<a class="headerlink" href="#computing-model-evidence" title="Link to this heading">#</a></h3>
<p>In Bayesian linear regression, the model evidence with the <em>hyperparamters</em> <span class="math notranslate nohighlight">\(\alpha\)</span> (weight prior) and <span class="math notranslate nohighlight">\(\beta\)</span> (observation noise) is given by integration over all the range of the weight parameters <span class="math notranslate nohighlight">\(\b{w}\)</span>:</p>
<div class="math notranslate nohighlight">
\[ p(\b{t}|\alpha,\beta) = \int p(\b{t}|\b{w},\beta)p(\b{w}|\alpha) d\b{w}, \]</div>
<p>The log evidence is given as (Bishop, Chapter 3.5)</p>
<div class="math notranslate nohighlight">
\[ \log p(\b{t}|\alpha,\beta) 
    = -\frac{\beta}{2} ||\b{t}-X\b{m}||^2 - \frac{\alpha}{2} ||\b{m}||^2 \]</div>
<div class="math notranslate nohighlight">
\[  + \frac{1}{2}\log|S| + \frac{D}{2}\log\alpha + \frac{N}{2}(\log\beta - \log(2\pi)) \]</div>
<p>where <span class="math notranslate nohighlight">\(\b{m}\)</span> and <span class="math notranslate nohighlight">\(S\)</span> also depend on <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">logev</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;log evidence for Bayesian regression</span>
<span class="sd">    m: posterior mean</span>
<span class="sd">    S: posterior covariance</span>
<span class="sd">    alpha: inv. variance of weight prior </span>
<span class="sd">    beta: inv. variance of observation noise</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">N</span><span class="p">,</span> <span class="n">D</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
    <span class="c1">#S = np.linalg.inv(alpha*np.eye(D) + beta*X.T@X) # posterior covariance</span>
    <span class="c1">#m = beta*S@X.T@t   # posterior mean</span>
    <span class="n">em</span> <span class="o">=</span> <span class="n">t</span> <span class="o">-</span> <span class="n">X</span><span class="nd">@m</span><span class="o">.</span><span class="n">T</span>  <span class="c1"># error with MAP estimate</span>
    <span class="c1">#alpha = D/np.dot(m,m)</span>
    <span class="c1">#beta = N/np.dot(em,em)</span>
    <span class="c1"># log evidence</span>
    <span class="n">lev</span> <span class="o">=</span> <span class="o">-</span><span class="n">beta</span><span class="o">/</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">em</span><span class="p">,</span><span class="n">em</span><span class="p">)</span> <span class="o">-</span> <span class="n">alpha</span><span class="o">/</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">m</span><span class="p">,</span><span class="n">m</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">det</span><span class="p">(</span><span class="n">S</span><span class="p">)))</span><span class="o">/</span><span class="mi">2</span> <span class="o">+</span> <span class="n">D</span><span class="o">/</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span> <span class="o">+</span> <span class="n">N</span><span class="o">/</span><span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">beta</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">lev</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># compare different values of alpha</span>
<span class="n">M</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># number of basis functions</span>
<span class="n">alphas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">10.</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">)</span>
<span class="n">beta</span> <span class="o">=</span> <span class="mf">10.</span>
<span class="n">mse</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># mean square errors</span>
<span class="n">lev</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># log evidences</span>
<span class="k">for</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="n">alphas</span><span class="p">:</span>
    <span class="n">Phi</span> <span class="o">=</span> <span class="n">gbf1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="n">xr</span><span class="p">,</span><span class="n">xr</span><span class="p">],</span> <span class="n">M</span><span class="p">)</span>  <span class="c1"># Gaussian basis functions</span>
    <span class="n">m</span><span class="p">,</span> <span class="n">S</span> <span class="o">=</span> <span class="n">blr</span><span class="p">(</span><span class="n">Phi</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>
    <span class="n">lev</span><span class="o">.</span><span class="n">append</span><span class="p">(</span> <span class="n">logev</span><span class="p">(</span><span class="n">Phi</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">))</span>
    <span class="c1"># test data</span>
    <span class="n">Phip</span> <span class="o">=</span> <span class="n">gbf1</span><span class="p">(</span><span class="n">xp</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="n">xr</span><span class="p">,</span><span class="n">xr</span><span class="p">],</span> <span class="n">M</span><span class="p">)</span>  <span class="c1"># Gaussian basis functions</span>
    <span class="n">err</span> <span class="o">=</span> <span class="n">fp</span> <span class="o">-</span> <span class="n">Phip</span><span class="nd">@m</span><span class="o">.</span><span class="n">T</span>  <span class="c1"># validation error</span>
    <span class="n">mse</span><span class="o">.</span><span class="n">append</span><span class="p">(</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">err</span><span class="p">,</span><span class="n">err</span><span class="p">)</span><span class="o">/</span><span class="n">Np</span><span class="p">)</span>
    <span class="c1">#print(alpha, beta, lev, mse)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">alphas</span><span class="p">,</span> <span class="n">mse</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;test error&quot;</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">alphas</span><span class="p">,</span> <span class="n">lev</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;log evidence&quot;</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\alpha$&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/d16f802608fe3354cced7c48ac26b6f561a79ada41f473e6623899b3fc0a047a.png" src="_images/d16f802608fe3354cced7c48ac26b6f561a79ada41f473e6623899b3fc0a047a.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># compare different values of beta</span>
<span class="n">M</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># number of basis functions</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">3.</span> 
<span class="n">betas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">50.</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="n">mse</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># mean square errors</span>
<span class="n">lev</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># log evidences</span>
<span class="k">for</span> <span class="n">beta</span> <span class="ow">in</span> <span class="n">betas</span><span class="p">:</span>
    <span class="n">Phi</span> <span class="o">=</span> <span class="n">gbf1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="n">xr</span><span class="p">,</span><span class="n">xr</span><span class="p">],</span> <span class="n">M</span><span class="p">)</span>  <span class="c1"># Gaussian basis functions</span>
    <span class="n">m</span><span class="p">,</span> <span class="n">S</span> <span class="o">=</span> <span class="n">blr</span><span class="p">(</span><span class="n">Phi</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>
    <span class="n">lev</span><span class="o">.</span><span class="n">append</span><span class="p">(</span> <span class="n">logev</span><span class="p">(</span><span class="n">Phi</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">))</span>
    <span class="c1"># test data</span>
    <span class="n">Phip</span> <span class="o">=</span> <span class="n">gbf1</span><span class="p">(</span><span class="n">xp</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="n">xr</span><span class="p">,</span><span class="n">xr</span><span class="p">],</span> <span class="n">M</span><span class="p">)</span>  <span class="c1"># Gaussian basis functions</span>
    <span class="n">err</span> <span class="o">=</span> <span class="n">fp</span> <span class="o">-</span> <span class="n">Phip</span><span class="nd">@m</span><span class="o">.</span><span class="n">T</span>  <span class="c1"># validation error</span>
    <span class="n">mse</span><span class="o">.</span><span class="n">append</span><span class="p">(</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">err</span><span class="p">,</span><span class="n">err</span><span class="p">)</span><span class="o">/</span><span class="n">Np</span><span class="p">)</span>
    <span class="c1">#print(alpha, beta, lev, mse)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">betas</span><span class="p">,</span> <span class="n">mse</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;test error&quot;</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">betas</span><span class="p">,</span> <span class="n">lev</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;log evidence&quot;</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\beta$&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/fef806ffa1565c573c469d87fb68afca1b68284910b65c4eff1bd87067381b31.png" src="_images/fef806ffa1565c573c469d87fb68afca1b68284910b65c4eff1bd87067381b31.png" />
</div>
</div>
</section>
<section id="bic-and-aic">
<h3>BIC and AIC<a class="headerlink" href="#bic-and-aic" title="Link to this heading">#</a></h3>
<p>In addition to the parameters <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span>, we want to select the number <span class="math notranslate nohighlight">\(M\)</span> of the basis functions.</p>
<p>The evidence for the model with <span class="math notranslate nohighlight">\(M\)</span> parameters trained by <span class="math notranslate nohighlight">\(N\)</span> samples are approximated by</p>
<div class="math notranslate nohighlight">
\[ \log p(\b{t}|M) 
    \simeq -\frac{\beta}{2} ||\b{t}-X\b{m}||^2 - \frac{1}{2}M\log N \]</div>
<p>This is know as the Bayesian information criterion (BIC). See Bishop Chapter 4.4 for details.</p>
<p>Another popular tool for model selection is Akaike information criterion (AIC), which is given by</p>
<div class="math notranslate nohighlight">
\[ AIC = -\frac{\beta}{2} ||\b{t}-X\b{m}||^2 - M. \]</div>
<p>AIC is based on the KL divergence of the data distributions between the true model and learned model.</p>
<p>Let us compare the test error and BIC for different model complexity <span class="math notranslate nohighlight">\(M\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">bic</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">beta</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;BIC and AIC</span>
<span class="sd">    m: posterior mean</span>
<span class="sd">    beta: inv. variance of observation noise</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">N</span><span class="p">,</span> <span class="n">M</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">em</span> <span class="o">=</span> <span class="n">t</span> <span class="o">-</span> <span class="n">X</span><span class="nd">@m</span><span class="o">.</span><span class="n">T</span>  <span class="c1"># error with MAP estimate</span>
    <span class="c1"># log evidence</span>
    <span class="n">bic</span> <span class="o">=</span> <span class="o">-</span><span class="n">beta</span><span class="o">/</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">em</span><span class="p">,</span><span class="n">em</span><span class="p">)</span> <span class="o">-</span> <span class="n">M</span><span class="o">/</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
    <span class="n">aic</span> <span class="o">=</span> <span class="o">-</span><span class="n">beta</span><span class="o">/</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">em</span><span class="p">,</span><span class="n">em</span><span class="p">)</span> <span class="o">-</span> <span class="n">M</span>
    <span class="k">return</span> <span class="n">bic</span><span class="p">,</span> <span class="n">aic</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># compare different values of M</span>
<span class="n">Max</span> <span class="o">=</span> <span class="mi">15</span>  <span class="c1"># max number of basis functions</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">3.</span>
<span class="n">beta</span> <span class="o">=</span> <span class="mf">20.</span>
<span class="n">mse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">Max</span><span class="p">)</span>  <span class="c1"># mean square errors</span>
<span class="n">baic</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">Max</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>  <span class="c1"># log evidences</span>
<span class="k">for</span> <span class="n">M</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="n">Max</span><span class="p">):</span>
    <span class="n">Phi</span> <span class="o">=</span> <span class="n">gbf1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="n">xr</span><span class="p">,</span><span class="n">xr</span><span class="p">],</span> <span class="n">M</span><span class="p">)</span>  <span class="c1"># Gaussian basis functions</span>
    <span class="n">m</span><span class="p">,</span> <span class="n">S</span> <span class="o">=</span> <span class="n">blr</span><span class="p">(</span><span class="n">Phi</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>
    <span class="n">baic</span><span class="p">[</span><span class="n">M</span><span class="p">]</span> <span class="o">=</span> <span class="n">bic</span><span class="p">(</span><span class="n">Phi</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>
    <span class="c1"># test data</span>
    <span class="n">Phip</span> <span class="o">=</span> <span class="n">gbf1</span><span class="p">(</span><span class="n">xp</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="n">xr</span><span class="p">,</span><span class="n">xr</span><span class="p">],</span> <span class="n">M</span><span class="p">)</span>  <span class="c1"># Gaussian basis functions</span>
    <span class="n">err</span> <span class="o">=</span> <span class="n">fp</span> <span class="o">-</span> <span class="n">Phip</span><span class="nd">@m</span><span class="o">.</span><span class="n">T</span>  <span class="c1"># validation error</span>
    <span class="n">mse</span><span class="p">[</span><span class="n">M</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">err</span><span class="p">,</span><span class="n">err</span><span class="p">)</span><span class="o">/</span><span class="n">Np</span>
    <span class="c1"># print(M, alpha, beta, lev[M], mse[M])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="n">Max</span><span class="p">),</span> <span class="n">mse</span><span class="p">[</span><span class="mi">2</span><span class="p">:],</span> <span class="s2">&quot;r&quot;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;test error&quot;</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="n">Max</span><span class="p">),</span> <span class="n">baic</span><span class="p">[</span><span class="mi">2</span><span class="p">:]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;BIC, AIC&quot;</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">((</span><span class="s2">&quot;BIC&quot;</span><span class="p">,</span><span class="s2">&quot;AIC&quot;</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;M&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/c646e6a79180b3024adb8cfeb5d8ed7cf604bb5f671c82c197049442dce3bd4a.png" src="_images/c646e6a79180b3024adb8cfeb5d8ed7cf604bb5f671c82c197049442dce3bd4a.png" />
</div>
</div>
</section>
</section>
<section id="bayesian-networks">
<h2>Bayesian networks<a class="headerlink" href="#bayesian-networks" title="Link to this heading">#</a></h2>
<p>As we have seen in the example of Bayesian linear regression, statistical machine learning assumes a <em>generative model</em> of the observed data and infer the posterior probability of variables of your interest, after marginalizing other unobserved variables.</p>
<p>In doing so, representation of the relationships by graphs with random variables as nodes (or vertices) and joint or conditional probabilities as links (or edges or arcs) have turned out to be very useful. They are called <em>graphical models</em>.</p>
<p>Directed graphs representing conditional probabilities by arrows (directed edges) are called <em>Bayesian networks</em>.</p>
<p>For example, Bayesian linear regression can be represented as a Bayesian network as below.</p>
<blockquote>
<div><img src="figures/BN_blr.png" width="200px">
Graphical model for the Bayesian linear regression.
</div></blockquote>
<p>Inference in Bayesian network goes like this: as values of some variables are observed,</p>
<ul class="simple">
<li><p><em>clamp</em> the values of the nodes where observation was made.</p></li>
<li><p>compute the posterior distributions of the nodes along the graph by repeating Bayesian inference and marginalization</p></li>
</ul>
<p>In doing this, the <em>conditional independence</em> of the nodes allows efficient computation.</p>
</section>
<section id="inference-on-a-chain">
<h2>Inference on a chain<a class="headerlink" href="#inference-on-a-chain" title="Link to this heading">#</a></h2>
<p>Here we consider the simplest case of a chain of discrete random variables.</p>
<blockquote>
<div><p><img alt="Chain" src="_images/BN_chain.png" />
Graphical model of a chain of states.</p>
</div></blockquote>
<p>For each node, the variable takes an integer value <span class="math notranslate nohighlight">\(x_n \in \{1,...,K_n\}\)</span> and we consider the joint distribution over the entire nodes:</p>
<div class="math notranslate nohighlight">
\[  p(x_1,...,x_N) = p(x_1)p(x_2|x_1) \cdots p(x_{N-1}|x_{N-2})p(x_N|x_{N-1}). \]</div>
<p>When an observation <span class="math notranslate nohighlight">\(x_N=k\)</span> is made at the end node, we would consider the posterior distribution</p>
<div class="math notranslate nohighlight">
\[  p(x_1,...,x_{N-1}|x_N=k) \propto 
    p(x_1)p(x_2|x_1) \cdots p(x_{N-1}|x_{N-2})p(x_N=k|x_{N-1}).  \]</div>
<p>The posterior distribution of each node <span class="math notranslate nohighlight">\(x_n\)</span> is given by marginalization</p>
<div class="math notranslate nohighlight">
\[  p(x_n|x_N=k) \propto \sum_{x_1}\cdots\sum_{x_{n-1}}\ 
    \sum_{x_{n+1}}\cdots\sum_{x_N}p(x_1,...,x_{N-1}|x_N=k) \]</div>
<div class="math notranslate nohighlight">
\[  = \left\{\sum_{x_{n-1}}p(x_n|x_{n-1}) \cdots
    \sum_{x_1}p(x_2|x_1)p(x_1)\right\} \]</div>
<div class="math notranslate nohighlight">
\[  \times \left\{\sum_{x_{n+1}}p(x_{n+1}|x_n) \cdots 
    \sum_{x_{N-1}}p(x_{N-1}|x_{N-2}) \sum_{x_N}p(x_N=k|x_{N-1})\right\}  \]</div>
<p>This can be computed efficiently by passing two <em>messages</em>:</p>
<ul class="simple">
<li><p>Forward message <span class="math notranslate nohighlight">\(\alpha_n\)</span> of prior:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ \alpha_1 = p(x_1)\]</div>
<div class="math notranslate nohighlight">
\[ \alpha_n = \sum_{x_{n-1}} p(x_n|x_{n-1}) \alpha_{n-1} \]</div>
<ul class="simple">
<li><p>Backward message <span class="math notranslate nohighlight">\(\beta_n\)</span> of likelihood:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ \beta_N = (0,...,1,...0) \]</div>
<p>with <span class="math notranslate nohighlight">\(1\)</span> at <span class="math notranslate nohighlight">\(k\)</span>-th component and</p>
<div class="math notranslate nohighlight">
\[ \beta_n = \sum_{x_{n+1}} p(x_{n+1}|x_n) \beta_{n+1} \]</div>
<p>The posterior distribution for each node is then given by their product</p>
<div class="math notranslate nohighlight">
\[  p(x_n|x_N=k) \propto \alpha_n \beta_n. \]</div>
<p>This is called <em>forward-backward</em> algorithm.</p>
<p>This can be generalized to tree-like networks and the algorithm using forward and backward message passing is known as <em>belief propagation</em>.</p>
<section id="markov-chain">
<h3>Markov chain<a class="headerlink" href="#markov-chain" title="Link to this heading">#</a></h3>
<p>Here is an example of inference in a Markov chain.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Markov</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Class for a Markov chain&quot;&quot;&quot;</span>
    
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ptr</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Create a new environment&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ptr</span> <span class="o">=</span> <span class="n">ptr</span>  <span class="c1"># transition matrix p(x&#39;|x)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Ns</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">ptr</span><span class="p">)</span>  <span class="c1"># number of states</span>
        
    <span class="k">def</span><span class="w"> </span><span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x0</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;generate a sample sequence from x0&quot;&quot;&quot;</span>
        <span class="n">seq</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">step</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span> <span class="c1"># sequence buffer</span>
        <span class="n">seq</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">x0</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">step</span><span class="p">):</span>
            <span class="n">pt1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ptr</span><span class="p">[:,</span> <span class="n">seq</span><span class="p">[</span><span class="n">t</span><span class="p">]]</span> <span class="c1"># prob. of new states</span>
            <span class="n">seq</span><span class="p">[</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Ns</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">pt1</span><span class="p">)</span> <span class="c1"># sample </span>
        <span class="k">return</span> <span class="n">seq</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p0</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;forward message from initial distribution p0&quot;&quot;&quot;</span>
        <span class="n">alpha</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">step</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Ns</span><span class="p">))</span> <span class="c1"># priors</span>
        <span class="n">alpha</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">p0</span>  <span class="c1"># initial distribution</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">step</span><span class="p">):</span>
            <span class="n">alpha</span><span class="p">[</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ptr</span> <span class="o">@</span> <span class="n">alpha</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> 
        <span class="k">return</span> <span class="n">alpha</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">obs</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;backward message from terminal observaion&quot;&quot;&quot;</span>
        <span class="n">beta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">step</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Ns</span><span class="p">))</span> <span class="c1"># likelihoods</span>
        <span class="n">beta</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">obs</span>  <span class="c1"># observation</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">step</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span> <span class="c1"># toward 0</span>
            <span class="n">beta</span><span class="p">[</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">beta</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">ptr</span>
        <span class="k">return</span> <span class="n">beta</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">posterior</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p0</span><span class="p">,</span> <span class="n">obs</span><span class="p">,</span> <span class="n">step</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;forward-backward algorithm&quot;&quot;&quot;</span>
        <span class="n">alpha</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">p0</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span>
        <span class="n">beta</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">obs</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span>
        <span class="n">post</span> <span class="o">=</span> <span class="n">alpha</span><span class="o">*</span><span class="n">beta</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">step</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
            <span class="n">post</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">post</span><span class="p">[</span><span class="n">t</span><span class="p">]</span><span class="o">/</span><span class="nb">sum</span><span class="p">(</span><span class="n">post</span><span class="p">[</span><span class="n">t</span><span class="p">])</span>  <span class="c1"># normalize        </span>
        <span class="k">return</span> <span class="n">post</span>
</pre></div>
</div>
</div>
</div>
<p>Here is an example of directed random walk on a ring.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># stochastic cycling on a ring</span>
<span class="n">ns</span> <span class="o">=</span> <span class="mi">6</span>   <span class="c1"># ring size</span>
<span class="n">ps</span> <span class="o">=</span> <span class="mf">0.3</span>  <span class="c1"># shift probability</span>
<span class="n">Ptr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">ns</span><span class="p">,</span> <span class="n">ns</span><span class="p">))</span>  <span class="c1"># transition matrix</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ns</span><span class="p">):</span>
    <span class="n">Ptr</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">ps</span>
    <span class="n">Ptr</span><span class="p">[(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">%</span><span class="k">ns</span>, i] = ps
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">Ptr</span><span class="p">)</span>
<span class="c1"># create a Markov chain</span>
<span class="n">ring</span> <span class="o">=</span> <span class="n">Markov</span><span class="p">(</span><span class="n">Ptr</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/111a66b931483a9120cb7cf5f88d52b0b9eb535625963aeff88ba2ee52e0fa91.png" src="_images/111a66b931483a9120cb7cf5f88d52b0b9eb535625963aeff88ba2ee52e0fa91.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># a sample trajectory</span>
<span class="n">T</span> <span class="o">=</span> <span class="mi">15</span>
<span class="n">ring</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([1, 2, 2, 2, 2, 3, 4, 4, 5, 0, 0, 1, 2, 3, 4, 5])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># forward message passing</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="n">ring</span><span class="o">.</span><span class="n">forward</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="n">T</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">alpha</span><span class="o">.</span><span class="n">T</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/8e088fe7cf1dd80705a6186bdaf9fd64e5a814b6e91a04f541a13fada799b933.png" src="_images/8e088fe7cf1dd80705a6186bdaf9fd64e5a814b6e91a04f541a13fada799b933.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># backward message passing</span>
<span class="n">beta</span> <span class="o">=</span> <span class="n">ring</span><span class="o">.</span><span class="n">backward</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="n">T</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">beta</span><span class="o">.</span><span class="n">T</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/ac0488e366875ac339def9ec78a1a45545bf95ffcbd6d03ff6ddba7665afe8ec.png" src="_images/ac0488e366875ac339def9ec78a1a45545bf95ffcbd6d03ff6ddba7665afe8ec.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># posterior by their products</span>
<span class="n">post</span> <span class="o">=</span> <span class="n">ring</span><span class="o">.</span><span class="n">posterior</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="n">T</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">post</span><span class="o">.</span><span class="n">T</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/7935061d131cce2876c40ef17ebb6e2e6ffa26b4edb358b34de183e668dfcb33.png" src="_images/7935061d131cce2876c40ef17ebb6e2e6ffa26b4edb358b34de183e668dfcb33.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># a little shifted observation</span>
<span class="n">post</span> <span class="o">=</span> <span class="n">ring</span><span class="o">.</span><span class="n">posterior</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="n">T</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">post</span><span class="o">.</span><span class="n">T</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/45a7d79066c7237ce7484a277640a9f360fbea0f59acee8b89fc231ee31c5445.png" src="_images/45a7d79066c7237ce7484a277640a9f360fbea0f59acee8b89fc231ee31c5445.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># longer sequence</span>
<span class="n">post</span> <span class="o">=</span> <span class="n">ring</span><span class="o">.</span><span class="n">posterior</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="mi">3</span><span class="o">*</span><span class="n">T</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">post</span><span class="o">.</span><span class="n">T</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/4d10a1f21f9753b8424dee45ae9d1fc7d9a015717adc492abd6a0c0760a96450.png" src="_images/4d10a1f21f9753b8424dee45ae9d1fc7d9a015717adc492abd6a0c0760a96450.png" />
</div>
</div>
</section>
</section>
<section id="dynamic-bayesian-inference">
<h2>Dynamic Bayesian Inference<a class="headerlink" href="#dynamic-bayesian-inference" title="Link to this heading">#</a></h2>
<p>Iterative Bayesian inference can be generalized to the case when the hidden variable <span class="math notranslate nohighlight">\(x\)</span> changes dynamically.</p>
<p>We denote the sequence of observation as</p>
<div class="math notranslate nohighlight">
\[  y_{1:t}=(y_1,..,y_t) \]</div>
<p>and the history of underlying state variable as</p>
<div class="math notranslate nohighlight">
\[  x_{1:t}=(x_1,..,x_t). \]</div>
<p>We assume two conditional probability distributions:</p>
<ul class="simple">
<li><p>Dynamics model: <span class="math notranslate nohighlight">\(p(x’|x)\)</span></p></li>
<li><p>Observation model: <span class="math notranslate nohighlight">\(p(y|x)\)</span></p></li>
</ul>
<p>Using the posterior <span class="math notranslate nohighlight">\(p(x_t|y_{1:t})\)</span> computed from the data up to time <span class="math notranslate nohighlight">\(t\)</span>, we use the dymamics model to compute the <em>predictive prior</em>:</p>
<div class="math notranslate nohighlight">
\[  p(x_{t+1}|y_{1:t}) = \int p(x_{t+1}|x_t) p(x_t|y_{1:t}) dx_t \]</div>
<p>by integrating or summing over the possible range of <span class="math notranslate nohighlight">\(x\)</span>.</p>
<p>We can combine this prior with the new coming data <span class="math notranslate nohighlight">\(y_{t+1}\)</span> to update the posterior as:</p>
<div class="math notranslate nohighlight">
\[  p(x_{t+1}|y_{1:t+1}) 
 = \frac{p(y_{t+1}|x_{t+1}) p(x_{t+1}|y_{1:t})}{ p(y_{1:t+1})}. \]</div>
<p>This is called <em>dynamic Bayesian inference</em> and allows real-time tracking of hidden variables from noisy observations.</p>
<p>When <span class="math notranslate nohighlight">\(x\)</span> is discrete, the process is called <em>hidden Markov model (HMM)</em>, which has been used extensively speech processing.</p>
<p>Another example is <em>Kalman filter</em>, in which <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> are continuous and the dynamics and observation models are linear mapping with Gaussian noise.</p>
<section id="hidden-markov-model">
<h3>Hidden Markov model<a class="headerlink" href="#hidden-markov-model" title="Link to this heading">#</a></h3>
<p>Here is a simple implementation of HMM based on the Markov chain above.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">HMM</span><span class="p">(</span><span class="n">Markov</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Hidden Markov model&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ptr</span><span class="p">,</span> <span class="n">pobs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Create HMM with transition and observation models&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">ptr</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pobs</span> <span class="o">=</span> <span class="n">pobs</span>  <span class="c1"># observation model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">No</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pobs</span><span class="p">)</span>  <span class="c1"># number of observations</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pst</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Ns</span><span class="p">)</span><span class="o">/</span><span class="bp">self</span><span class="o">.</span><span class="n">Ns</span>  <span class="c1"># state distribution</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Ns</span><span class="p">)</span>  <span class="c1"># predictive distribution</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x0</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;generate a sample sequence from x0&quot;&quot;&quot;</span>
        <span class="n">xt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">step</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span> <span class="c1"># state sequence</span>
        <span class="n">yt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">step</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span> <span class="c1"># observation sequence</span>
        <span class="n">xt</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">x0</span>
        <span class="n">po</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pobs</span><span class="p">[:,</span> <span class="n">x0</span><span class="p">]</span> <span class="c1"># prob. of observation</span>
        <span class="n">yt</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">No</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">po</span><span class="p">)</span> <span class="c1"># observe</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">step</span><span class="p">):</span>
            <span class="n">ps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ptr</span><span class="p">[:,</span> <span class="n">xt</span><span class="p">[</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span>  <span class="c1"># prob. of new states</span>
            <span class="n">xt</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Ns</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">ps</span><span class="p">)</span> <span class="c1"># transit </span>
            <span class="n">po</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pobs</span><span class="p">[:,</span> <span class="n">xt</span><span class="p">[</span><span class="n">t</span><span class="p">]]</span>   <span class="c1"># prob. of observation</span>
            <span class="n">yt</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">No</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">po</span><span class="p">)</span> <span class="c1"># observe </span>
        <span class="k">return</span> <span class="n">xt</span><span class="p">,</span> <span class="n">yt</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;predictive prior by transition model&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ptr</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">pst</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">obs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;update posterior by observation&quot;&quot;&quot;</span>
        <span class="n">prl</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pobs</span><span class="p">[</span><span class="n">obs</span><span class="p">]</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">pred</span> <span class="c1"># likelihood*prior</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pst</span> <span class="o">=</span> <span class="n">prl</span><span class="o">/</span><span class="nb">sum</span><span class="p">(</span><span class="n">prl</span><span class="p">)</span>  <span class="c1">#normalize</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;reset state probability&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pst</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Ns</span><span class="p">)</span><span class="o">/</span><span class="bp">self</span><span class="o">.</span><span class="n">Ns</span>  <span class="c1"># uniform</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">obs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;one step of dynamic bayesian inference&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">pst</span>  <span class="c1"># new prior</span>
</pre></div>
</div>
</div>
</div>
<p>Here is an example of directed random walk on a ring, like a mouse walking on a circular track.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># random walk on a ring</span>
<span class="n">ns</span> <span class="o">=</span> <span class="mi">6</span>   <span class="c1"># ring size</span>
<span class="n">ps</span> <span class="o">=</span> <span class="mf">0.3</span>  <span class="c1"># shift probability</span>
<span class="n">Ptr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">ns</span><span class="p">,</span> <span class="n">ns</span><span class="p">))</span>  <span class="c1"># transition matrix</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ns</span><span class="p">):</span>
    <span class="n">Ptr</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">ps</span>
    <span class="n">Ptr</span><span class="p">[(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">%</span><span class="k">ns</span>, i] = ps
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">Ptr</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;state&quot;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;next state&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/0f12002896f1979b7222832735a44c6012a71a31536c6bf7a564a599b00e6ec7.png" src="_images/0f12002896f1979b7222832735a44c6012a71a31536c6bf7a564a599b00e6ec7.png" />
</div>
</div>
<p>Suppose we have three coarse position sensors, which send signal only intermittently, and we want to estimate where the mouse is.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Blurred intermittent observation model</span>
<span class="n">no</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">po</span> <span class="o">=</span> <span class="mf">0.3</span>
<span class="n">Pobs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">no</span><span class="p">,</span> <span class="n">ns</span><span class="p">))</span>  <span class="c1"># p(obs|state)</span>
<span class="n">Pobs</span><span class="p">[</span><span class="mi">0</span><span class="p">,:]</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">po</span>  <span class="c1"># no information</span>
<span class="n">Pobs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">Pobs</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="n">Pobs</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">]</span> <span class="o">=</span> <span class="n">po</span>
<span class="n">Pobs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">Pobs</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">Pobs</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">Pobs</span><span class="p">[</span><span class="mi">2</span><span class="p">:</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">]</span> <span class="o">=</span> <span class="n">po</span><span class="o">/</span><span class="mi">2</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">Pobs</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;state&quot;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;observation&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/ba7cfa8bc71cd102c039d7d7e9a50eb69b8a00cef9028a00ae41f02e7249ddc6.png" src="_images/ba7cfa8bc71cd102c039d7d7e9a50eb69b8a00cef9028a00ae41f02e7249ddc6.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># crate a HMM</span>
<span class="n">ring</span> <span class="o">=</span> <span class="n">HMM</span><span class="p">(</span><span class="n">Ptr</span><span class="p">,</span> <span class="n">Pobs</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># sample a state trajectory and observations</span>
<span class="n">T</span> <span class="o">=</span> <span class="mi">30</span>
<span class="n">xt</span><span class="p">,</span> <span class="n">yt</span><span class="p">,</span> <span class="o">=</span> <span class="n">ring</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xt</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">yt</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;ro&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/2ad7f207fbd2b40595607d301b2e888327955587fb05a8459fac9951e8ac9904.png" src="_images/2ad7f207fbd2b40595607d301b2e888327955587fb05a8459fac9951e8ac9904.png" />
</div>
</div>
<p>From such noisy intermittent observations, how can we estimate the state?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Dynamic Bayesian inference in HMM</span>
<span class="n">post</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">T</span><span class="p">,</span> <span class="n">ns</span><span class="p">))</span>  <span class="c1"># posterior trajectory</span>
<span class="n">ring</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">):</span>
    <span class="n">post</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">ring</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">yt</span><span class="p">[</span><span class="n">t</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">post</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">origin</span><span class="o">=</span><span class="s1">&#39;lower&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;t&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;state&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/b57107ea7a745daaade0afa5ffa6d97faac0df1a1ceeaaf7ea5ac77a59a63985.png" src="_images/b57107ea7a745daaade0afa5ffa6d97faac0df1a1ceeaaf7ea5ac77a59a63985.png" />
</div>
</div>
<p>Even when there is no useful sensory input, you can predict the state distribution by the dynamic model.
When a sensory input becomes available, prediction is corrected and sharpened.</p>
<p>After sensory input, you can also reflect back and consider which previous states were more likely using <em>forward-backward</em> algorithms.</p>
</section>
</section>
<section id="approximate-bayesian-inference">
<h2>Approximate Bayesian inference<a class="headerlink" href="#approximate-bayesian-inference" title="Link to this heading">#</a></h2>
<p>For discrete distributions and continuous distributions following Gaussians and others, computation of posterior distribution is not very difficult.</p>
<p>But when we deal with an arbitrary high-dimension continous distributions, computation of posterior distribution. Especially the computation of the normalizaing factor (marginal likelihood) can be quite hard for integration over multiple dimensions.</p>
<p>For such cases, there are approximate Bayesian inference methods, such as:</p>
<section id="variational-inference">
<h3>Variational inference<a class="headerlink" href="#variational-inference" title="Link to this heading">#</a></h3>
<p>In variational inference, we approximate the posterior distribution by a certain functional form <span class="math notranslate nohighlight">\(q(x)\)</span>, and update it to minimize the discrepancy from the posterior distribuion, usually measured by the <em>KL divergence</em></p>
<div class="math notranslate nohighlight">
\[ KL[q(x);p(x|y)] = \int q(x) \log\frac{q(x)}{p(x|y)} dx \]</div>
<p>A typical assumption is that the posterior distribution can be factorized, i.e. represented by a product of distributions of different groups</p>
<div class="math notranslate nohighlight">
\[ q(x) = \prod_i q_i(x_i). \]</div>
<p>This leads to an repeated alternating optimization similar to the EM algorithm, but by taking into account the distribution of each group of variables.</p>
<p>See Chapter 10 of Bishop (2006) for details.</p>
</section>
<section id="sampling-methods">
<h3>Sampling methods:<a class="headerlink" href="#sampling-methods" title="Link to this heading">#</a></h3>
<p>In the sampling methods, we approximate a distribution <span class="math notranslate nohighlight">\(p(x)\)</span> by a collection of points <span class="math notranslate nohighlight">\({x_1,...,x_n}\)</span>.</p>
<p>We are often interested in evaluating the expectation of a certain function over the posterior distribution</p>
<div class="math notranslate nohighlight">
\[ E_{p(x)}[f(x)] = \int f(x)p(x) dx \]</div>
<p>This can be approximated by the sum of the values at the sample points</p>
<p>$$ E_{p(x)}[f(x)] = \frac{1}{n} \sum_{i=1}^n f(x_i) dx</p>
<p>if the distribution of samples <span class="math notranslate nohighlight">\({x_i}\)</span> well approximates <span class="math notranslate nohighlight">\(p(x)\)</span>.</p>
<p>See Chapter 11 of Bishop (2006) for details.</p>
</section>
</section>
<section id="bayesian-sensorimotor-processing">
<h2>Bayesian sensorimotor processing<a class="headerlink" href="#bayesian-sensorimotor-processing" title="Link to this heading">#</a></h2>
<p>Our life is full of uncertainty. In sensory perception, we need to cope with noise, delay and occulusion and also overcome fundamental ill-posedness, such as to identify the 3D location of your target from 2D retinal images or sounds to two ears.</p>
<p>To find a practical solution to such ill-posed problems, we need to make use of some prior assumptions, such as the light usually comes from the top or objects don’t jump abruptly.</p>
<p>Bayesian inference provides a principled way for combining any prior knowledge with sensory evidence. Indeed there are several lines of psychological evidence suggesting that humans and animals integrate knowledge from prior experience or multi-modal sensory information as predicted by Bayesian inference (Knill &amp; Pouget 2004, Doya et al. 2007).</p>
<ul class="simple">
<li><p>Ernst &amp; Banks (2002) tested in a grasping task in a virtual reality setting how human subjects’ object size perception depends on the noise level in the visula feedback. They showed that as the viaual noise increases, subjects’ responses are closer to the size estimated by the haptic input, consistently with the ratio of the variances or visual and haptic perception as predicted by the Bayesian theory of multisensory integration.</p></li>
<li><p>Kording &amp; Wolpert (2004) tested in an arm reaching task with modified visual feedback how the prior expectation based on repeated trials is combined with visual feedback of different clarities upon each trial. They showed that the subjects’ performance was based more on the visual feedback for higher clarity, as expected from Bayesian integration of the prior and likehihood by their variances.</p></li>
</ul>
<blockquote>
<div><p><img alt="Hewitson2018_Fig1" src="_images/Hewitson2018_Fig1.jpg" />
<img alt="Hewitson2018_Fig3" src="_images/Hewitson2018_Fig3.jpg" /></p>
<p>This is a study by Hewitson et al. (2018) following the experiment by Wolpert &amp; Koerding (2004). As the subject tries to move the cursor to the target, a random shift is introduced to the hand-to-curs mapping. The subjects acquire a prior distribution of the cursor shift from experience, and combine that with sensory feedback with different clarities in the middle of reaching. They confirmed that the subjects’ performance followed the predicted by Bayesian inference and further showed that the performace generalize across the arm used (from Hewitson et al. 2018).</p>
</div></blockquote>
</section>
<section id="bayesian-computation-in-the-brain">
<h2>Bayesian computation in the brain<a class="headerlink" href="#bayesian-computation-in-the-brain" title="Link to this heading">#</a></h2>
<p>How such Bayesian computation realized in the brain? How does the brain represent and manipulate probability distributions?</p>
<p>One possibility is that the <em>receptive field</em> of a neuron represents a basis function in the sensory space and the activities of a population of neurons represent a probability distribution. This idea is called <em>probabilistic population code</em> (Zemel et al. 2004, Ma et al. 2006).</p>
<p>The cerebral cortex has a hierarchical organization and bi-directional connections between lower and higher areas originating from specific layers.
There have been serveral hypotheses about how such hierarchical recurrent network can realize Bayesian inference, such as by belif propagation (Lochmann &amp; Deneve 2011; George D, Hawkins J 2009).</p>
<blockquote>
<div><p><img alt="George2009_Fig9" src="_images/George2009_Fig9.png" /></p>
<p>A hypothetical diagram of how bottom-up (green) and top-down (red) messages for Bayesian inference are processed by neurons in different cortical layers. From George and Hawkins (2009).</p>
</div></blockquote>
<p>Karl Frisont considered variational approximation as a plausible mechanism of Bayesian inference in the brain and proposed the minimization of <em>variational free energy</em> as the basic operational principle of the brain (Friston 2005, 2010).
His group proposed how such operations can be implemented in the canonical cortical circuits (Bastos et al. 2012).</p>
<blockquote>
<div><p><img alt="Bogacz17" src="_images/Bogacz17.png" />
This tutorial illustrates how variational free-energy approximation of posterior probability works and how such mechanisms might be mapped onto the cortical circuit (from Bobacz 2017).</p>
</div></blockquote>
<p>There has been only scarse attempts at directly testing those hypotheses, but a recent two-photon imaging experiment showed the evidence for dynamic Bayesian inference by populations of neurons in the parietal cortex (Funamizu et al. 2016).</p>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Bishop CM (2006) Pattern Recognition and Machine Learning. Springer. <a class="reference external" href="https://www.microsoft.com/en-us/research/people/cmbishop/prml-book/">https://www.microsoft.com/en-us/research/people/cmbishop/prml-book/</a></p>
<ul>
<li><p>Chapter 3: Bayesian linear regression</p></li>
<li><p>Chapter 8: Graphical models</p></li>
<li><p>Chapter 10: Approximate Bayesian inference</p></li>
<li><p>Chapter 11: Sampling methods</p></li>
</ul>
</li>
</ul>
<section id="bayesian-sensorimotor-integration">
<h3>Bayesian sensorimotor integration<a class="headerlink" href="#bayesian-sensorimotor-integration" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Knill DC, Pouget A (2004) The Bayesian brain: the role of uncertainty in neural coding and computation. Trends in neurosciences 27:712-719. <a class="reference external" href="https://doi.org/10.1016/j.tins.2004.10.007">https://doi.org/10.1016/j.tins.2004.10.007</a></p></li>
<li><p>Doya K, Ishii S, Pouget A, Rao R (2007) Bayesian Brain: Probabilistic Approach to Neural Coding and Learning. MIT Press.</p></li>
<li><p>Ernst MO, Banks MS (2002). Humans integrate visual and haptic information in a statistically optimal fashion. Nature, 415, 429-433. <a class="reference external" href="https://doi.org/10.1038/415429a">https://doi.org/10.1038/415429a</a></p></li>
<li><p>Körding KP, Wolpert DM (2004) Bayesian integration in sensorimotor learning. Nature 427:244-247. <a class="reference external" href="https://doi.org/10.1038/nature02169">https://doi.org/10.1038/nature02169</a></p></li>
<li><p>Hewitson CL, Sowman PF, Kaplan DM (2018). Interlimb Generalization of Learned Bayesian Visuomotor Prior Occurs in Extrinsic Coordinates. eneuro, 10.1523/eneuro.0183-18.2018. <a class="reference external" href="https://doi.org/10.1523/eneuro.0183-18.2018">https://doi.org/10.1523/eneuro.0183-18.2018</a></p></li>
</ul>
</section>
<section id="probabilistic-population-codes">
<h3>Probabilistic population codes<a class="headerlink" href="#probabilistic-population-codes" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Zemel RS, Dayan P, Pouget A (1998) Probabilistic interpretation of population codes. Neural computation 10:403-430. <a class="reference external" href="https://doi.org/10.1162/089976698300017818">https://doi.org/10.1162/089976698300017818</a></p></li>
<li><p>Ma WJ, Beck JM, Latham PE, Pouget A (2006) Bayesian inference with probabilistic population codes. Nature neuroscience 9:1432-1438. <a class="reference external" href="https://doi.org/10.1038/nn1790">https://doi.org/10.1038/nn1790</a></p></li>
</ul>
</section>
<section id="baysian-inference-in-the-cortical-circuit">
<h3>Baysian inference in the cortical circuit<a class="headerlink" href="#baysian-inference-in-the-cortical-circuit" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>George D, Hawkins J (2009). Towards a mathematical theory of cortical micro-circuits. PLoS Comput Biol, 5, e1000532. <a class="reference external" href="https://doi.org/10.1371/journal.pcbi.1000532">https://doi.org/10.1371/journal.pcbi.1000532</a></p></li>
<li><p>Lochmann T, Deneve S (2011) Neural processing as causal inference. Current opinion in neurobiology 21:774-781. <a class="reference external" href="https://doi.org/10.1016/j.conb.2011.05.018">https://doi.org/10.1016/j.conb.2011.05.018</a></p></li>
<li><p>Friston K (2005). A theory of cortical responses. Philos Trans R Soc Lond B Biol Sci, 360, 815-36. <a class="reference external" href="http://doi.org/10.1098/rstb.2005.1622">http://doi.org/10.1098/rstb.2005.1622</a></p></li>
<li><p>Friston K (2010). The free-energy principle: a unified brain theory? Nat Rev Neurosci, 11, 127-38. <a class="reference external" href="http://doi.org/10.1038/nrn2787">http://doi.org/10.1038/nrn2787</a></p></li>
<li><p>Bastos AM, Usrey WM, Adams RA, Mangun GR, Fries P, Friston KJ (2012). Canonical microcircuits for predictive coding. Neuron, 76, 695-711. <a class="reference external" href="https://doi.org/10.1016/j.neuron.2012.10.038">https://doi.org/10.1016/j.neuron.2012.10.038</a></p></li>
<li><p>Bogacz R (2017) A tutorial on the free-energy framework for modelling perception and learning. Journal of Mathematical Psychology. 76, 198–211. <a class="reference external" href="https://doi.org/10.1016/j.jmp.2015.11.003">https://doi.org/10.1016/j.jmp.2015.11.003</a></p></li>
<li><p>Funamizu A, Kuhn B, Doya K (2016) Neural substrate of dynamic Bayesian inference in the cerebral cortex. Nature Neuroscience 19:1682-1689. <a class="reference external" href="https://doi.org/10.1038/nn.4390">https://doi.org/10.1038/nn.4390</a></p></li>
</ul>
</section>
</section>
<div class="toctree-wrapper compound">
</div>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="Unsupervised_Exercise.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Unsupervised Learning: Exercise</p>
      </div>
    </a>
    <a class="right-next"
       href="Bayesian_Exercise.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Bayesian Approaches: Exercise</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#contents">Contents</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayes-theorem">Bayes’ Theorem</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-mouse-in-a-bush">Example: mouse in a bush</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#iterative-bayesian-inference">Iterative Bayesian Inference</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#coin-toss">Coin toss</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gaussian-observations">Gaussian observations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-approaches-in-machine-learning">Bayesian approaches in machine learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-linear-regression">Bayesian Linear Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#predictive-distribution">Predictive distribution</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-model-comparison">Bayesian model comparison</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#computing-model-evidence">Computing model evidence</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bic-and-aic">BIC and AIC</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-networks">Bayesian networks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#inference-on-a-chain">Inference on a chain</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#markov-chain">Markov chain</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dynamic-bayesian-inference">Dynamic Bayesian Inference</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hidden-markov-model">Hidden Markov model</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#approximate-bayesian-inference">Approximate Bayesian inference</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#variational-inference">Variational inference</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sampling-methods">Sampling methods:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-sensorimotor-processing">Bayesian sensorimotor processing</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-computation-in-the-brain">Bayesian computation in the brain</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-sensorimotor-integration">Bayesian sensorimotor integration</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#probabilistic-population-codes">Probabilistic population codes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#baysian-inference-in-the-cortical-circuit">Baysian inference in the cortical circuit</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Kenji Doya
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>