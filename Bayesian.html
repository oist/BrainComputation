
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Chatper 6. Bayesian Approaches &#8212; Brain Computation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'Bayesian';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Bayesian Approaches: Exercise" href="Bayesian_Exercise.html" />
    <link rel="prev" title="Unsupervised Learning: Exercise" href="Unsupervised_Exercise.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="BrainComputation.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/BC_logo.png" class="logo__image only-light" alt="Brain Computation - Home"/>
    <script>document.write(`<img src="_static/BC_logo.png" class="logo__image only-dark" alt="Brain Computation - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="BrainComputation.html">
                    Brain Computation: A Hands-on Guidebook
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="Introduction.html">Chapter 1. Introduction</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="Neurons.html">Chapter 2. Neural Modeling and Analysis</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="Neurons_Exercise.html">Chapter 2. Neural Modeling and Analysis: Exercise</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="Supervised.html">Chapter 3: Supervised Learning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="Supervised_Exercise.html">Supervised Learning: Exercise</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="Reinforcement.html">Chapter 4. Reinforcement Learning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="Reinforcement_Exercise.html">Chapter 4. Reinforcement Learning: Exercise</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="Unsupervised.html">Chapter 5. Unsupervised Learning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="Unsupervised_Exercise.html">Unsupervised Learning: Exercise</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="current reference internal" href="#">Chatper 6. Bayesian Approaches</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="Bayesian_Exercise.html">Bayesian Approaches: Exercise</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="Deep.html">Chapter 7: Deep Learning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="Deep_Exercise.html">Chapter 7: Deep Learning: Exercise</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="Multiple.html">Chapter 8. Multiple Agents</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="Multiple_Exercise.html">Chapter 8. Multiple Agents: Exercise</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="Meta.html">Chapter 9. Meta-Learning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="Meta_Exercise.html">Chapter 9. Meta-Learning: Exercise</a></li>
</ul>
</details></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/Bayesian.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Chatper 6. Bayesian Approaches</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#contents">Contents</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayes-theorem">Bayes’ Theorem</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-mouse-in-a-bush">Example: mouse in a bush</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#iterative-bayesian-inference">Iterative Bayesian Inference</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#coin-toss">Coin toss</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#noisy-observation">Noisy observation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-approaches-in-machine-learning">Bayesian approaches in machine learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-linear-regression">Bayesian Linear Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#predictive-distribution">Predictive distribution</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-model-comparison">Bayesian model comparison</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#computing-model-evidence">Computing model evidence</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-networks">Bayesian networks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#inference-on-a-chain">Inference on a chain</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#markov-chain">Markov chain</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dynamic-bayesian-inference">Dynamic Bayesian Inference</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hidden-markov-model">Hidden Markov model</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-sensorimotor-processing">Bayesian sensorimotor processing</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-computation-in-the-brain">Bayesian computation in the brain</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-sensorimotor-integration">Bayesian sensorimotor integration</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#probabilistic-population-codes">Probabilistic population codes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#baysian-inference-in-the-cortical-circuit">Baysian inference in the cortical circuit</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="chatper-6-bayesian-approaches">
<h1>Chatper 6. Bayesian Approaches<a class="headerlink" href="#chatper-6-bayesian-approaches" title="Link to this heading">#</a></h1>
<div class="math notranslate nohighlight">
\[ % Latex macros
\newcommand{\mat}[1]{\begin{pmatrix} #1 \end{pmatrix}}
\newcommand{\p}[2]{\frac{\partial #1}{\partial #2}}
\renewcommand{\b}[1]{\boldsymbol{#1}}
\newcommand{\c}[1]{\mathcal{#1}}
\]</div>
<section id="contents">
<h2>Contents<a class="headerlink" href="#contents" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Bayes’ theorem</p></li>
<li><p>Bayesian linear regression (Bishop, Chater 3)</p></li>
<li><p>Bayesian model comparison</p></li>
<li><p>Bayesian networks  (Bishop, Chater 8)</p></li>
<li><p>Dynamic Bayesian inference</p></li>
<li><p>Bayesian Brain</p>
<ul>
<li><p>Sensory psychophysics</p></li>
<li><p>Cortical circuit</p></li>
</ul>
</li>
</ul>
</section>
<section id="bayes-theorem">
<h2>Bayes’ Theorem<a class="headerlink" href="#bayes-theorem" title="Link to this heading">#</a></h2>
<p>Let us recall the two fundamental rules of probability regarding random variables <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>:</p>
<ul class="simple">
<li><p>Sum rule:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[  p(X) = \sum_Y p(X, Y) \]</div>
<ul class="simple">
<li><p>Product rule:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[  p(X,Y) = p(Y|X) p(X) \]</div>
<p>From the symmetry of joint probability <span class="math notranslate nohighlight">\(p(X,Y)=p(Y,X)\)</span>,<br />
we have the relationship <span class="math notranslate nohighlight">\(p(X|Y)p(Y) = p(Y|X)p(X)\)</span>,<br />
which brings us to Bayes’ theorem:</p>
<div class="math notranslate nohighlight">
\[  p(X|Y) = \frac{p(Y|X)p(X)}{p(Y)}. \]</div>
<p>Using Bayes’ theorem, we can convert one conditional probability to the other.
This simple formula has turned out to be very insightful in the context of sensory processing and learning.</p>
<p>Suppose <span class="math notranslate nohighlight">\(X\)</span> is the variable of your interest, such as the existence of your target, and <span class="math notranslate nohighlight">\(Y\)</span> is a noisey sensory input. What sensory input <span class="math notranslate nohighlight">\(Y\)</span> you would receive if your target exists or not is represented by a sensory model <span class="math notranslate nohighlight">\(p(Y|X)\)</span>.</p>
<ul class="simple">
<li><p>Your knowledge or assumption about existence of your target is represented by <span class="math notranslate nohighlight">\(p(X)\)</span>, called <em>prior probability</em>.</p></li>
<li><p>For a given sensory input <span class="math notranslate nohighlight">\(y\)</span>, <span class="math notranslate nohighlight">\(p(Y=y|X)\)</span> as a function of <span class="math notranslate nohighlight">\(X\)</span> is called the <em>likelihood</em> of the state <span class="math notranslate nohighlight">\(X\)</span>.</p></li>
<li><p>The probability of the target <span class="math notranslate nohighlight">\(X\)</span> existing after observing <span class="math notranslate nohighlight">\(y\)</span>, <span class="math notranslate nohighlight">\(p(X|Y=y)\)</span> is called the <em>posterior probability</em>.</p></li>
</ul>
<p>Bayes’ theorem gives a theoretical basis for the intuition that the posterior probability is proportional to the product of the prior prbability and the likelihood.</p>
<section id="example-mouse-in-a-bush">
<h3>Example: mouse in a bush<a class="headerlink" href="#example-mouse-in-a-bush" title="Link to this heading">#</a></h3>
<p>You are a cat chasing a mouse and heard a rustuling sound from a bush.
About half of the case a hiding mouse makes a sound, but about 10% of the time you hear rustling just by the wind.
How would you estimate the probability for a mouse hiding in the bush?</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p><span class="math notranslate nohighlight">\(p(Y \vert X)\)</span></p></th>
<th class="head text-center"><p>no mouse <span class="math notranslate nohighlight">\(X=0\)</span></p></th>
<th class="head text-center"><p>mouse hiding <span class="math notranslate nohighlight">\(X=1\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>no sound <span class="math notranslate nohighlight">\(Y=0\)</span></p></td>
<td class="text-center"><p>0.9</p></td>
<td class="text-center"><p>0.5</p></td>
</tr>
<tr class="row-odd"><td><p>sound <span class="math notranslate nohighlight">\(Y=1\)</span></p></td>
<td class="text-center"><p>0.1</p></td>
<td class="text-center"><p>0.5</p></td>
</tr>
</tbody>
</table>
</div>
<p>For example, if you heard a sound, <span class="math notranslate nohighlight">\(Y=1\)</span>, what is the probability of a mouse hiding behind the bush, <span class="math notranslate nohighlight">\(p(X=1|Y=1)\)</span>?
From the above table, 0.5?</p>
<p>No, actually. In the ‘heard’ row, 0.1 and 0.5 do not sum up to one. They are likelihoods <span class="math notranslate nohighlight">\(p(Y=1|X)\)</span>, but not probability distribution <span class="math notranslate nohighlight">\(p(X|Y=1)\)</span> for the mouse to be in the bush or not.</p>
<p>The mouse ran into other nearby bushes, so you assume that the prior probability of the mouse in this bush is 30%:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-center"><p>prior probability</p></th>
<th class="head text-center"><p>no mouse <span class="math notranslate nohighlight">\(X=0\)</span></p></th>
<th class="head text-center"><p>mouse hiding <span class="math notranslate nohighlight">\(X=1\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p><span class="math notranslate nohighlight">\(p(X)\)</span></p></td>
<td class="text-center"><p>0.7</p></td>
<td class="text-center"><p>0.3</p></td>
</tr>
</tbody>
</table>
</div>
<p>By having this prior probability, we can use Bayes’ theorem:</p>
<div class="math notranslate nohighlight">
\[  p(X=1|Y=1) = \frac{p(Y=1|X=1)p(X=1)}{p(Y=1|X=0)p(X=0)+p(Y=1|X=1)p(X=1)}  \]</div>
<div class="math notranslate nohighlight">
\[  = \frac{0.5*0.3}{0.1*0.7+0.5*0.3}
    = \frac{0.15}{0.07+0.15} 
    = \frac{0.15}{0.22} \simeq 0.68  \]</div>
</section>
</section>
<section id="iterative-bayesian-inference">
<h2>Iterative Bayesian Inference<a class="headerlink" href="#iterative-bayesian-inference" title="Link to this heading">#</a></h2>
<p>A useful property of Bayesian inference is that you can apply it iteratively to incoming data stream.</p>
<p>We denote the sequence of observations up to time <span class="math notranslate nohighlight">\(t\)</span> as</p>
<div class="math notranslate nohighlight">
\[  y_{1:t}=(y_1,...,y_t)  \]</div>
<p>and want to estimate the cause <span class="math notranslate nohighlight">\(x\)</span> of these observations</p>
<div class="math notranslate nohighlight">
\[  p(x|y_{1:t}) = \frac{p(y_{1:t}|x)\ p(x)}{p(y_{1:t})}  \]</div>
<p>If the observations are independent, their joint distribution is a product</p>
<div class="math notranslate nohighlight">
\[  p(y_{1:t}|x) = p(y_1|x)\cdots p(y_t|x)  \]</div>
<p>and thus the posterior can be decomposed as</p>
<div class="math notranslate nohighlight">
\[  p(x|y_{1:t}) = \frac{p(y_1|x)\cdots p(y_{t-1}|x)\ p(y_t|x)\ p(x)}{p(y_1)\cdots p(y_{t-1})\ p(y_t)}  \]</div>
<div class="math notranslate nohighlight">
\[  = \frac{p(y_1|x)\cdots p(y_{t-1}|x)\ p(x)}{p(y_1)\cdots p(y_{t-1}) }\ \frac{p(y_t|x)}{p(y_t)}  \]</div>
<div class="math notranslate nohighlight">
\[  = \frac{p(x|y_{1:t-1})\ p(y_t|x)}{p(y_t)}.  \]</div>
<p>This means that the posterior <span class="math notranslate nohighlight">\(p(x|y_{1:t-1})\)</span> that you computed by time <span class="math notranslate nohighlight">\(t-1\)</span> serves as the prior to be combined with the likelihood for the new coming data <span class="math notranslate nohighlight">\(p(y_t|x)\)</span> for computing the new posterior <span class="math notranslate nohighlight">\(p(x|y_{1:t})\)</span>.</p>
<p>This iterative update of the posterior is practically helpful in online inference utilizing whatever data available so far.</p>
<section id="coin-toss">
<h3>Coin toss<a class="headerlink" href="#coin-toss" title="Link to this heading">#</a></h3>
<p>Here is a simple example of estimating the parameter <span class="math notranslate nohighlight">\(\mu\)</span>, probability for a coin to land head up, during multiple tosses.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># take samples</span>
<span class="n">mu</span> <span class="o">=</span> <span class="mf">0.3</span>  <span class="c1"># probability of head</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">8</span>   <span class="c1"># number of samples</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="o">-</span><span class="n">mu</span><span class="p">,</span> <span class="n">mu</span><span class="p">])</span> <span class="c1"># binary observation sequence</span>
<span class="n">y</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([1, 0, 0, 0, 0, 0, 0, 1])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dx</span> <span class="o">=</span> <span class="mf">0.01</span>  <span class="c1"># plot step</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dx</span><span class="p">)</span>  <span class="c1"># range of the parameter</span>
<span class="n">prior</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>  <span class="c1"># Assume a uniform prior</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># a new figure</span>
    <span class="c1"># prior</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span> <span class="n">x</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">)</span>
    <span class="c1"># observation</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span> <span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">N</span><span class="p">,</span> <span class="s1">&#39;ko&#39;</span><span class="p">)</span>
    <span class="c1"># likelihood</span>
    <span class="n">likelihood</span> <span class="o">=</span> <span class="n">x</span><span class="o">*</span><span class="n">y</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">x</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">y</span><span class="p">[</span><span class="n">t</span><span class="p">])</span> <span class="c1"># theta if head, 1-theta if tail</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span> <span class="n">x</span><span class="p">,</span> <span class="n">likelihood</span><span class="p">,</span> <span class="s1">&#39;g&#39;</span><span class="p">)</span>
    <span class="c1"># product</span>
    <span class="n">prilik</span> <span class="o">=</span> <span class="n">prior</span><span class="o">*</span><span class="n">likelihood</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span> <span class="n">x</span><span class="p">,</span> <span class="n">prilik</span><span class="p">,</span><span class="s1">&#39;c&#39;</span><span class="p">)</span>
    <span class="c1"># posterior by normalization</span>
    <span class="n">marginal</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">prilik</span><span class="p">)</span><span class="o">*</span><span class="n">dx</span>    <span class="c1"># integrate over the parameter range</span>
    <span class="n">posterior</span> <span class="o">=</span> <span class="n">prilik</span><span class="o">/</span><span class="n">marginal</span>  <span class="c1"># normalize</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span> <span class="n">x</span><span class="p">,</span> <span class="n">posterior</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\mu$&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">t</span><span class="o">==</span><span class="mi">0</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">((</span><span class="s1">&#39;prior&#39;</span><span class="p">,</span> <span class="s1">&#39;observation&#39;</span><span class="p">,</span> <span class="s1">&#39;likelihood&#39;</span><span class="p">,</span> <span class="s1">&#39;prior*like.&#39;</span><span class="p">,</span> <span class="s1">&#39;posterior&#39;</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;t = </span><span class="si">{</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="c1"># posterior as a new prior</span>
    <span class="n">prior</span> <span class="o">=</span> <span class="n">posterior</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/645f3b57543db5c9506fd6f28dfec81eac278875ad4dafa862a7f835be7bc1b9.png" src="_images/645f3b57543db5c9506fd6f28dfec81eac278875ad4dafa862a7f835be7bc1b9.png" />
</div>
</div>
<p>As more data are collected, the posterior distribution of <span class="math notranslate nohighlight">\(\mu\)</span> becomes sharper and colser to the true value.</p>
</section>
</section>
<section id="noisy-observation">
<h2>Noisy observation<a class="headerlink" href="#noisy-observation" title="Link to this heading">#</a></h2>
<p>Estimate the mean <span class="math notranslate nohighlight">\(\mu\)</span> and the standard deviationn <span class="math notranslate nohighlight">\(\sigma\)</span> from noisy observations.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Noisy observation: y = N(mu,sigma)</span>
<span class="n">mu</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">sigma</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">12</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">mu</span> <span class="o">+</span> <span class="n">sigma</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[-0.29650755  0.37821215  1.84774002 -2.78765557  2.26589076  1.19551142
 -0.17336153  0.83397063 -0.07840148  0.19259326 -2.41061598  3.06239619]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Start from a uniform prior</span>
<span class="n">rmu</span> <span class="o">=</span> <span class="mi">5</span><span class="p">;</span> <span class="n">dmu</span> <span class="o">=</span> <span class="mf">0.2</span>   <span class="c1"># range and step of mu</span>
<span class="n">rsig</span> <span class="o">=</span> <span class="mi">5</span><span class="p">;</span>  <span class="n">dsig</span> <span class="o">=</span> <span class="mf">0.2</span> <span class="c1"># range and step of sigma</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="n">rmu</span><span class="p">,</span> <span class="n">rmu</span><span class="p">,</span> <span class="n">dmu</span><span class="p">)</span>
<span class="n">s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">rsig</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="n">dsig</span><span class="p">)</span>
<span class="n">M</span><span class="p">,</span> <span class="n">S</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>
<span class="n">prior</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">M</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">rmu</span><span class="o">*</span><span class="n">rsig</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="c1"># observation</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span> <span class="n">y</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">],</span> <span class="n">dsig</span><span class="o">+</span><span class="n">rsig</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="n">N</span><span class="p">,</span> <span class="s1">&#39;w+&#39;</span><span class="p">)</span>
    <span class="c1"># likelihood</span>
    <span class="n">likelihood</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">((</span><span class="n">y</span><span class="p">[</span><span class="n">n</span><span class="p">]</span><span class="o">-</span><span class="n">M</span><span class="p">)</span><span class="o">/</span><span class="n">S</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span><span class="o">*</span><span class="n">S</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">likelihood</span><span class="p">,</span> <span class="n">extent</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="n">rmu</span><span class="p">,</span><span class="n">rmu</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="n">rsig</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\mu$&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\sigma$&#39;</span><span class="p">);</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;likelihood </span><span class="si">{</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="c1"># posterior</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">prilik</span> <span class="o">=</span> <span class="n">prior</span><span class="o">*</span><span class="n">likelihood</span>
    <span class="c1">#plt.imshow(prilik, extent=(-rmu,rmu,0,rsig))</span>
    <span class="n">marginal</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">prilik</span><span class="p">)</span><span class="o">*</span><span class="n">dmu</span><span class="o">*</span><span class="n">dsig</span>
    <span class="n">posterior</span> <span class="o">=</span> <span class="n">prilik</span><span class="o">/</span><span class="n">marginal</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">posterior</span><span class="p">,</span> <span class="n">extent</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="n">rmu</span><span class="p">,</span><span class="n">rmu</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="n">rsig</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\mu$&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$\sigma$&#39;</span><span class="p">);</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;posterior </span><span class="si">{</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">prior</span> <span class="o">=</span> <span class="n">posterior</span>  <span class="c1"># new prior</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/606db10ff6635d996135980d377df77de2adb22f7404c95fddcc23dd7aeb9eb5.png" src="_images/606db10ff6635d996135980d377df77de2adb22f7404c95fddcc23dd7aeb9eb5.png" />
<img alt="_images/ecfecfdb4d70eb6e8c719ebd29a1eeaf87364bcd0281f7dfc796348ae279744f.png" src="_images/ecfecfdb4d70eb6e8c719ebd29a1eeaf87364bcd0281f7dfc796348ae279744f.png" />
<img alt="_images/c236c6d638a004f52b88ed12dc4f4ef1cfcfe794e7734f5a21f63bd48b630fd9.png" src="_images/c236c6d638a004f52b88ed12dc4f4ef1cfcfe794e7734f5a21f63bd48b630fd9.png" />
<img alt="_images/bb09ef5e1ce18794a809ded4dbd53b73e00430e0913866651c227394e3b1e992.png" src="_images/bb09ef5e1ce18794a809ded4dbd53b73e00430e0913866651c227394e3b1e992.png" />
<img alt="_images/2788a860225a80f287e2d980048c56be21cfa82c91d739746a2f3bbfc06796d9.png" src="_images/2788a860225a80f287e2d980048c56be21cfa82c91d739746a2f3bbfc06796d9.png" />
<img alt="_images/394c78cf3149856000831d243f13bd33d2e79547711108d6db8a4cf635b6a874.png" src="_images/394c78cf3149856000831d243f13bd33d2e79547711108d6db8a4cf635b6a874.png" />
<img alt="_images/8e54b8ca9c72264eaac2b03eea40aa3a1314b6b1ba1c2a766b9420a51ad4534a.png" src="_images/8e54b8ca9c72264eaac2b03eea40aa3a1314b6b1ba1c2a766b9420a51ad4534a.png" />
<img alt="_images/6a388e9dcf0ec15ce0dff7060e4a8ce453799128118b89d614d13438902a07f5.png" src="_images/6a388e9dcf0ec15ce0dff7060e4a8ce453799128118b89d614d13438902a07f5.png" />
<img alt="_images/6fc8c1197a4ac949746f7fd7202463c8b9b68a137f79b80a38b675e15f05cb38.png" src="_images/6fc8c1197a4ac949746f7fd7202463c8b9b68a137f79b80a38b675e15f05cb38.png" />
<img alt="_images/fd4674b28dac862d9e7d5db01b7aa983ef6b8cb463abfb29d3e2ee612483de94.png" src="_images/fd4674b28dac862d9e7d5db01b7aa983ef6b8cb463abfb29d3e2ee612483de94.png" />
<img alt="_images/ea6f61b3f6f23743bc151960eb31d6f4c140ef91c6e6fc9b940afca66a1c9cac.png" src="_images/ea6f61b3f6f23743bc151960eb31d6f4c140ef91c6e6fc9b940afca66a1c9cac.png" />
<img alt="_images/00ce48777aa87c5283eec59840533911582b8eef01cd59cad644531cf0e33e13.png" src="_images/00ce48777aa87c5283eec59840533911582b8eef01cd59cad644531cf0e33e13.png" />
</div>
</div>
</section>
<section id="bayesian-approaches-in-machine-learning">
<h2>Bayesian approaches in machine learning<a class="headerlink" href="#bayesian-approaches-in-machine-learning" title="Link to this heading">#</a></h2>
<p>“Bayesian” is quite popular in machine learning, but it is used for different meanings:</p>
<ul class="simple">
<li><p>To combine prior knowledge and the likelihood from observation</p></li>
<li><p>To assume a graphical model of data generation for estimation of the causes</p></li>
<li><p>To estimate the distribution of a variable, not a single point</p></li>
</ul>
<p>In supervised learning:</p>
<ul class="simple">
<li><p>avoid over fitting by introducing a prior distribution on the parameters</p></li>
<li><p>compare models by their probability of producing observed data</p></li>
</ul>
<p>In reinforcement learning:</p>
<ul class="simple">
<li><p>infer the environmental state from incomplete observation</p></li>
<li><p>estimate the distribution of reward, not just the expectation</p></li>
</ul>
<p>In unsupervised learning:</p>
<ul class="simple">
<li><p>infer hidden variables behind data</p>
<ul>
<li><p>e.g. responsibility in Mixtures of Gaussians</p></li>
</ul>
</li>
</ul>
</section>
<section id="bayesian-linear-regression">
<h2>Bayesian Linear Regression<a class="headerlink" href="#bayesian-linear-regression" title="Link to this heading">#</a></h2>
<p>The standard linear regression (Chapter 3) assumes a linear regression function with additive noise
$<span class="math notranslate nohighlight">\( t_n = \b{w}^T\b{x}_n + \epsilon \)</span><span class="math notranslate nohighlight">\(
where \)</span>p(\epsilon)=\mathcal{N}(0,\beta^{-1})$.</p>
<p>In Bayesian linear regression, we assume that the weights are sampled from a prior distribution <span class="math notranslate nohighlight">\(p(\b{w})=\mathcal{N}(\b{0},\alpha^{-1}I)\)</span>.</p>
<p>The likelihood of the parameter <span class="math notranslate nohighlight">\(\b{w}\)</span> for the target output <span class="math notranslate nohighlight">\(\b{t}\)</span> is</p>
<div class="math notranslate nohighlight">
\[ p(\b{t}|X, \b{w}, \beta) = \prod_{n=1}^N \mathcal{N}(t_n|\b{w}^T\b{x},\beta^{-1}) \]</div>
<p>When both the prior and likelihood are Gaussian, the posterior will also be Gaussian and have the form:</p>
<div class="math notranslate nohighlight">
\[ p(\b{w}|\b{t}) = \mathcal{N}(\b{w}|\b{m},S) \]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[ \b{m} = \beta S X^T \bf{t} \]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[ S = (\alpha I + \beta X^T X)^{-1} \]</div>
<p>If we let <span class="math notranslate nohighlight">\(\alpha=0\)</span>, i.e. infinitely large variance for the weight prior, this is equivalent to regular linear regression. Introducing a penalty term is commonly done in linear regression as</p>
<div class="math notranslate nohighlight">
\[ E(\b{w}) = \frac{\beta}{2}\sum_{n=1}^N \{t_n - \b{w}^T \b{x}_n\}^2
+ \frac{\alpha}{2} \b{w}^T\b{w} \]</div>
<div class="math notranslate nohighlight">
\[ = \frac{\beta}{2} ||\b{t} - X \b{w}||^2 + \frac{\alpha}{2} ||\b{w}||^2. \]</div>
<p>The Bayesian regression gives a probabilistic interpretation on the role of the regularization parameter <span class="math notranslate nohighlight">\(\alpha\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># We will use this frequently</span>
<span class="k">def</span><span class="w"> </span><span class="nf">gauss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Gaussian distribution&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">((</span><span class="n">x</span><span class="o">-</span><span class="n">mu</span><span class="p">)</span><span class="o">/</span><span class="n">sigma</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span><span class="o">*</span><span class="n">sigma</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Distributions in the parameter and data spaces</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">1.</span>  <span class="c1"># inverse variance of weight prior</span>
<span class="n">beta</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># inverse variance of observation noise</span>
<span class="c1"># prior distribution of weights</span>
<span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">30</span><span class="p">)</span>
<span class="n">W0</span><span class="p">,</span> <span class="n">W1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span>
<span class="n">pw</span> <span class="o">=</span> <span class="n">gauss</span><span class="p">(</span><span class="n">W0</span><span class="p">)</span><span class="o">*</span><span class="n">gauss</span><span class="p">(</span><span class="n">W1</span><span class="p">)</span>
<span class="c1"># plot the distribution</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">pcolormesh</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">pw</span><span class="p">)</span>  
<span class="c1"># sample weight from prior distribution</span>
<span class="n">K</span> <span class="o">=</span> <span class="mi">10</span>  
<span class="n">wpri</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="n">alpha</span><span class="o">**-</span><span class="mf">0.5</span><span class="p">,</span><span class="n">size</span><span class="o">=</span><span class="mi">2</span><span class="o">*</span><span class="n">K</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">K</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">wpri</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">wpri</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;r+&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;square&#39;</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Prior&quot;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;w0&quot;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;w1&quot;</span><span class="p">);</span>
<span class="c1"># plot model samples</span>
<span class="n">xrange</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">])</span>   <span class="c1"># range of input x</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">K</span><span class="p">):</span>
    <span class="c1">#wpri[1,k]*X</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xrange</span><span class="p">,</span> <span class="n">wpri</span><span class="p">[</span><span class="n">k</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">+</span><span class="n">wpri</span><span class="p">[</span><span class="n">k</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">xrange</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Model samples&quot;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;y&quot;</span><span class="p">);</span>    
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/f326422657d04df3be2618d722411019e01b3785a15968c66212dc2df220b790.png" src="_images/f326422657d04df3be2618d722411019e01b3785a15968c66212dc2df220b790.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># after observation of data</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">wt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>  <span class="c1"># &#39;true&#39; weights</span>
<span class="c1"># sample data</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">xrange</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">xrange</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">size</span><span class="o">=</span><span class="n">N</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">N</span><span class="p">),</span> <span class="n">X</span><span class="p">]</span>  <span class="c1"># prepend 1 in the leftmost column</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">wt</span><span class="nd">@X</span><span class="o">.</span><span class="n">T</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">N</span><span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">beta</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># start from top right</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xrange</span><span class="p">,</span> <span class="n">wt</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">+</span><span class="n">wt</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">xrange</span><span class="p">,</span> <span class="s2">&quot;g&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">t</span><span class="p">,</span> <span class="s2">&quot;r.&quot;</span><span class="p">)</span>  <span class="c1"># training data</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="n">xrange</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Data&quot;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;y&quot;</span><span class="p">);</span>
<span class="c1"># likelihood</span>
<span class="n">like</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
    <span class="n">like</span> <span class="o">=</span> <span class="n">like</span><span class="o">*</span><span class="n">gauss</span><span class="p">(</span><span class="n">t</span><span class="p">[</span><span class="n">n</span><span class="p">]</span> <span class="o">-</span> <span class="p">(</span><span class="n">W0</span><span class="o">+</span><span class="n">W1</span><span class="o">*</span><span class="n">X</span><span class="p">[</span><span class="n">n</span><span class="p">,</span><span class="mi">1</span><span class="p">]),</span> <span class="n">sigma</span><span class="o">=</span><span class="n">beta</span><span class="o">**</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>   <span class="c1"># top left</span>
<span class="n">plt</span><span class="o">.</span><span class="n">pcolormesh</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">like</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;square&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Likelihood&quot;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;w0&quot;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;w1&quot;</span><span class="p">);</span>
<span class="c1"># new posterior</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span>
<span class="n">post</span> <span class="o">=</span> <span class="n">pw</span><span class="o">*</span><span class="n">like</span>
<span class="n">plt</span><span class="o">.</span><span class="n">pcolormesh</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">post</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;square&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Posterior&quot;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;w0&quot;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;w1&quot;</span><span class="p">);</span>
<span class="n">S</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">alpha</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">beta</span><span class="o">*</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="nd">@X</span><span class="p">)</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">beta</span><span class="o">*</span><span class="n">S</span><span class="nd">@X</span><span class="o">.</span><span class="n">T</span><span class="nd">@t</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;m =&#39;</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;S =&#39;</span><span class="p">,</span> <span class="n">S</span><span class="p">)</span>
<span class="c1"># sample weights</span>
<span class="n">wpost</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">K</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">wpost</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">wpost</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;r+&quot;</span><span class="p">)</span>
<span class="c1"># plot model samples</span>
<span class="n">xrange</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">])</span>   <span class="c1"># range of input x</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">K</span><span class="p">):</span>
    <span class="c1">#wpri[1,k]*X</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xrange</span><span class="p">,</span> <span class="n">wpost</span><span class="p">[</span><span class="n">k</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">+</span><span class="n">wpost</span><span class="p">[</span><span class="n">k</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">xrange</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">t</span><span class="p">,</span> <span class="s2">&quot;r.&quot;</span><span class="p">)</span>  <span class="c1"># training data</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Model samples&quot;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;y&quot;</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>  <span class="c1"># adjust subplot margins</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>m = [-1.18109368  0.59550667]
S = [[0.2615659  0.372318  ]
 [0.372318   0.81227751]]
</pre></div>
</div>
<img alt="_images/1a0e89e672a6323441527c44f39325527b413eeec92d43460809c41296662eb8.png" src="_images/1a0e89e672a6323441527c44f39325527b413eeec92d43460809c41296662eb8.png" />
</div>
</div>
<section id="predictive-distribution">
<h3>Predictive distribution<a class="headerlink" href="#predictive-distribution" title="Link to this heading">#</a></h3>
<p>In Bayesian regression, the result is not one weight vector, but a distribution in the weight space. Then it is reasonable to consider the distribution of the output considering such uncertainty in the weigts.</p>
<p>The output <span class="math notranslate nohighlight">\(y\)</span> for a new input <span class="math notranslate nohighlight">\(\b{x}\)</span> should have the distribution</p>
<div class="math notranslate nohighlight">
\[ p(y|\b{x},\b{t},\alpha,\beta) 
 = \int p(y|\b{x},\b{w},\beta)  p(\b{w}|\b{t},\alpha,\beta) d\b{w} \]</div>
<div class="math notranslate nohighlight">
\[ = \mathcal{N}(t|\b{m}^T \b{x}, \sigma^2(\b{x})) \]</div>
<p>where the variance of the output is given by</p>
<div class="math notranslate nohighlight">
\[ \sigma^2(\b{x}) = \beta^{-1} + \b{x}^T S \b{x} \]</div>
<p>Let us see the example of approximating a sine function by Gaussian basis functions.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1D Gaussian basis functions </span>
<span class="k">def</span><span class="w"> </span><span class="nf">gbf1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">xrange</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mf">1.</span><span class="p">,</span><span class="mf">1.</span><span class="p">],</span> <span class="n">M</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Gaussian basis functions: x can be a 1D array&quot;&quot;&quot;</span>
    <span class="n">xc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">xrange</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">xrange</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">num</span><span class="o">=</span><span class="n">M</span><span class="p">)</span>  <span class="c1"># centers</span>
    <span class="n">xd</span> <span class="o">=</span> <span class="p">(</span><span class="n">xc</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="n">xc</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>  <span class="c1"># interval</span>
    <span class="c1"># x can be an array for N data points</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">x</span><span class="p">,[</span><span class="n">M</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">T</span> <span class="o">-</span> <span class="n">xc</span><span class="p">)</span><span class="o">/</span><span class="n">xd</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
<span class="c1"># example</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">gbf1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">M</span><span class="o">=</span><span class="mi">5</span><span class="p">));</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;phi&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/0fb23549a0591bd28502b2fe63b67fd5ce9922a36a0a2381b243fc0dd33093d4.png" src="_images/0fb23549a0591bd28502b2fe63b67fd5ce9922a36a0a2381b243fc0dd33093d4.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">blr</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">10.</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Bayesian linear regression</span>
<span class="sd">    alpha: inv. variance of weight prior </span>
<span class="sd">    beta: inv. variance of observation noise</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">N</span><span class="p">,</span> <span class="n">D</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">S</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">alpha</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">D</span><span class="p">)</span> <span class="o">+</span> <span class="n">beta</span><span class="o">*</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="nd">@X</span><span class="p">)</span> <span class="c1"># posterior covariance</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">beta</span><span class="o">*</span><span class="n">S</span><span class="nd">@X</span><span class="o">.</span><span class="n">T</span><span class="nd">@t</span>   <span class="c1"># posterior mean</span>
    <span class="k">return</span> <span class="n">m</span><span class="p">,</span> <span class="n">S</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">target</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Target function&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Training data</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">eps</span> <span class="o">=</span> <span class="mf">0.2</span>  <span class="c1"># noise size</span>
<span class="n">xr</span> <span class="o">=</span> <span class="mi">4</span>   <span class="c1"># range of x</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="n">xr</span><span class="p">,</span> <span class="n">xr</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">N</span><span class="p">)</span>  
<span class="n">f</span> <span class="o">=</span> <span class="n">target</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># target function</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">f</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">N</span><span class="p">)</span> <span class="c1"># with noise</span>
<span class="c1"># data for testing/plotting</span>
<span class="n">Np</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">xp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="n">xr</span><span class="p">,</span> <span class="n">xr</span><span class="p">,</span> <span class="n">Np</span><span class="p">)</span>
<span class="n">fp</span> <span class="o">=</span> <span class="n">target</span><span class="p">(</span><span class="n">xp</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xp</span><span class="p">,</span> <span class="n">fp</span><span class="p">,</span> <span class="s2">&quot;g&quot;</span><span class="p">)</span>  <span class="c1"># target function</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="s2">&quot;ro&quot;</span><span class="p">);</span>  <span class="c1"># training data</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;y&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/11a020337ae124e2a24c069e52bde8dbeb1e67ff73f06cffffcc591200b84674.png" src="_images/11a020337ae124e2a24c069e52bde8dbeb1e67ff73f06cffffcc591200b84674.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">M</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># number of basis functions</span>
<span class="n">Phi</span> <span class="o">=</span> <span class="n">gbf1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="n">xr</span><span class="p">,</span><span class="n">xr</span><span class="p">],</span> <span class="n">M</span><span class="p">)</span>  <span class="c1"># Gaussian basis functions</span>
<span class="n">m</span><span class="p">,</span> <span class="n">S</span> <span class="o">=</span> <span class="n">blr</span><span class="p">(</span><span class="n">Phi</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>  <span class="c1"># Bayesian linear regression</span>
<span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
<span class="c1"># test data</span>
<span class="n">Phip</span> <span class="o">=</span> <span class="n">gbf1</span><span class="p">(</span><span class="n">xp</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="n">xr</span><span class="p">,</span><span class="n">xr</span><span class="p">],</span> <span class="n">M</span><span class="p">)</span>
<span class="n">yp</span> <span class="o">=</span> <span class="n">Phip</span><span class="nd">@m</span><span class="o">.</span><span class="n">T</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xp</span><span class="p">,</span> <span class="n">fp</span><span class="p">,</span> <span class="s2">&quot;g&quot;</span><span class="p">)</span>  <span class="c1"># target function</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="s2">&quot;ro&quot;</span><span class="p">);</span>  <span class="c1"># training data</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xp</span><span class="p">,</span> <span class="n">yp</span><span class="p">,</span> <span class="s2">&quot;b&quot;</span><span class="p">);</span>  <span class="c1"># MAP estimate</span>
<span class="c1"># predictive distribution</span>
<span class="n">sigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">beta</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">Phip</span><span class="nd">@S</span><span class="o">*</span><span class="n">Phip</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xp</span><span class="p">,</span> <span class="n">yp</span><span class="o">+</span><span class="n">sigma</span><span class="p">,</span> <span class="s2">&quot;c&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xp</span><span class="p">,</span> <span class="n">yp</span><span class="o">-</span><span class="n">sigma</span><span class="p">,</span> <span class="s2">&quot;c&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;y&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[ 0.36391967  0.22252404 -1.04505018 -0.45994288 -0.02332867  0.07579673
  0.50329866  0.7265915  -0.21131898 -0.56739785]
</pre></div>
</div>
<img alt="_images/bacf35633111145317ff2698e99be08dfea38e4ee1847b8704244536305e21cf.png" src="_images/bacf35633111145317ff2698e99be08dfea38e4ee1847b8704244536305e21cf.png" />
</div>
</div>
<p>See how <span class="math notranslate nohighlight">\(N\)</span>, <span class="math notranslate nohighlight">\(M\)</span>, <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> affect the performance.</p>
</section>
</section>
<section id="bayesian-model-comparison">
<h2>Bayesian model comparison<a class="headerlink" href="#bayesian-model-comparison" title="Link to this heading">#</a></h2>
<p>We have so far considered Bayesian inference of the parameter <span class="math notranslate nohighlight">\(\b{w}\)</span> for a given model <span class="math notranslate nohighlight">\(\c{M}\)</span>, such as a regression model with some input variables, but we can also think of Bayesian inference of probability over models <span class="math notranslate nohighlight">\(\c{M}_i\)</span>, such as regression models with different choices of input variables, given data <span class="math notranslate nohighlight">\(\c{D}\)</span></p>
<div class="math notranslate nohighlight">
\[ p(\c{M}_i|\c{D}) \propto p(\c{M}_i) p(\c{D}|\c{M}_i). \]</div>
<p>Here <span class="math notranslate nohighlight">\(p(\c{D}|\c{M}_i)\)</span>, the likelihood of a model given data, is called the <em>evidence</em> of the model.</p>
<p>If we include a model explicitly in our Bayesian parameter estimation, we have</p>
<div class="math notranslate nohighlight">
\[ p(\b{w}|\c{D},\c{M}_i) = \frac{p(\c{D}|\b{w},\c{M}_i)p(\b{w}|\c{M}_i)}{p(\c{D}|\c{M}_i)}, \]</div>
<p>where we have the <em>evidence</em> as the normalizing denominator</p>
<div class="math notranslate nohighlight">
\[ p(\c{D}|\c{M}_i) = \int p(\c{D}|\b{w},\c{M}_i)p(\b{w}|\c{M}_i) d\b{w}. \]</div>
<p>This is also called <em>marginal likelihood</em> because it is the likelihood of the model with its parameters marginalized.</p>
<section id="computing-model-evidence">
<h3>Computing model evidence<a class="headerlink" href="#computing-model-evidence" title="Link to this heading">#</a></h3>
<p>In Bayesian linear regression, the model evidence with the <em>hyperparamters</em> <span class="math notranslate nohighlight">\(\alpha\)</span> (weight prior) and <span class="math notranslate nohighlight">\(\beta\)</span> (observation noise) is given by integration over all the range of the weight parameters <span class="math notranslate nohighlight">\(\b{w}\)</span>:</p>
<div class="math notranslate nohighlight">
\[ p(\b{t}|\alpha,\beta) = \int p(\b{t}|\b{w},\beta)p(\b{w}|\alpha) d\b{w}, \]</div>
<p>By further integrating this over the prior distribution of <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> we have the evidence for the full model.</p>
<p>A practical approximation is to find <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> that maximize <span class="math notranslate nohighlight">\(p(\b{t}|\alpha,\beta)\)</span>.</p>
<p>The log evidence is given as (Bishop, Chapter 3.5)</p>
<div class="math notranslate nohighlight">
\[ \log p(\b{t}|\alpha,\beta) 
    = -\frac{\beta}{2} ||\b{t}-X\b{m}||^2 - \frac{\alpha}{2} ||\b{m}||^2 \]</div>
<div class="math notranslate nohighlight">
\[  + \frac{1}{2}\log|S| + \frac{D}{2}\log\alpha + \frac{N}{2}(\log\beta - \log(2\pi)) \]</div>
<p>where <span class="math notranslate nohighlight">\(\b{m}\)</span> and <span class="math notranslate nohighlight">\(S\)</span> also depend on <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span>.</p>
<p>For <span class="math notranslate nohighlight">\(N&gt;&gt;D\)</span>, the evidence is maximized by</p>
<div class="math notranslate nohighlight">
\[ \alpha^{-1} = \frac{1}{D}||\b{m}||^2 \]</div>
<div class="math notranslate nohighlight">
\[ \beta^{-1} = \frac{1}{N}||\b{t} - X \b{w}||^2. \]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">logev</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">S</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;log evidence for Bayesian regression</span>
<span class="sd">    m: posterior mean</span>
<span class="sd">    S: posterior covariance</span>
<span class="sd">    alpha: inv. variance of weight prior </span>
<span class="sd">    beta: inv. variance of observation noise</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">N</span><span class="p">,</span> <span class="n">D</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
    <span class="c1">#S = np.linalg.inv(alpha*np.eye(D) + beta*X.T@X) # posterior covariance</span>
    <span class="c1">#m = beta*S@X.T@t   # posterior mean</span>
    <span class="n">em</span> <span class="o">=</span> <span class="n">t</span> <span class="o">-</span> <span class="n">X</span><span class="nd">@m</span><span class="o">.</span><span class="n">T</span>  <span class="c1"># error with MAP estimate</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">D</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">m</span><span class="p">,</span><span class="n">m</span><span class="p">)</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">N</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">em</span><span class="p">,</span><span class="n">em</span><span class="p">)</span>
    <span class="c1"># log evidence</span>
    <span class="n">lev</span> <span class="o">=</span> <span class="o">-</span><span class="n">beta</span><span class="o">/</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">em</span><span class="p">,</span><span class="n">em</span><span class="p">)</span> <span class="o">-</span> <span class="n">alpha</span><span class="o">/</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">m</span><span class="p">,</span><span class="n">m</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">det</span><span class="p">(</span><span class="n">S</span><span class="p">)))</span><span class="o">/</span><span class="mi">2</span> <span class="o">+</span> <span class="n">D</span><span class="o">/</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span> <span class="o">+</span> <span class="n">N</span><span class="o">/</span><span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">beta</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">lev</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span>
</pre></div>
</div>
</div>
</div>
<p>Try computing log evidence for different <span class="math notranslate nohighlight">\(M\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># try different values of M</span>
<span class="n">Max</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># max number of basis functions</span>
<span class="n">mse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">Max</span><span class="p">)</span>  <span class="c1"># mean square errors</span>
<span class="n">lev</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">Max</span><span class="p">)</span>  <span class="c1"># log evidences</span>
<span class="k">for</span> <span class="n">M</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="n">Max</span><span class="p">):</span>
    <span class="n">Phi</span> <span class="o">=</span> <span class="n">gbf1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="n">xr</span><span class="p">,</span><span class="n">xr</span><span class="p">],</span> <span class="n">M</span><span class="p">)</span>  <span class="c1"># Gaussian basis functions</span>
    <span class="n">m</span><span class="p">,</span> <span class="n">S</span> <span class="o">=</span> <span class="n">blr</span><span class="p">(</span><span class="n">Phi</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
    <span class="n">lev</span><span class="p">[</span><span class="n">M</span><span class="p">],</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span> <span class="o">=</span> <span class="n">logev</span><span class="p">(</span><span class="n">Phi</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">S</span><span class="p">)</span>
    <span class="c1"># test data</span>
    <span class="n">Phip</span> <span class="o">=</span> <span class="n">gbf1</span><span class="p">(</span><span class="n">xp</span><span class="p">,</span> <span class="p">[</span><span class="o">-</span><span class="n">xr</span><span class="p">,</span><span class="n">xr</span><span class="p">],</span> <span class="n">M</span><span class="p">)</span>  <span class="c1"># Gaussian basis functions</span>
    <span class="n">err</span> <span class="o">=</span> <span class="n">fp</span> <span class="o">-</span> <span class="n">Phip</span><span class="nd">@m</span><span class="o">.</span><span class="n">T</span>  <span class="c1"># validation error</span>
    <span class="n">mse</span><span class="p">[</span><span class="n">M</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">err</span><span class="p">,</span><span class="n">err</span><span class="p">)</span><span class="o">/</span><span class="n">Np</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">lev</span><span class="p">[</span><span class="n">M</span><span class="p">],</span> <span class="n">mse</span><span class="p">[</span><span class="n">M</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="n">Max</span><span class="p">),</span> <span class="n">mse</span><span class="p">[</span><span class="mi">2</span><span class="p">:],</span> <span class="s2">&quot;r&quot;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;mse&quot;</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="n">Max</span><span class="p">),</span> <span class="n">lev</span><span class="p">[</span><span class="mi">2</span><span class="p">:],</span> <span class="s2">&quot;b&quot;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;log evidence&quot;</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;M&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2 33.778293113514245 2.235354125870604 -23.004360177490817 0.4282154212895307
3 4.809068595449988 2.353969520142632 -25.904249731742564 0.4217001729584062
4 0.26086678175653677 29.39264202712211 -7.934591230390508 0.008300982317741172
5 0.7103208561540388 35.621867349160055 -6.189526459689123 0.00898405424985272
6 1.3424356751646311 36.3044791989696 -5.941403071117978 0.010284653624427542
7 1.951385311166175 39.245493383553836 -5.722673291650771 0.013605463974775873
8 2.471114863612759 44.50507806345025 -4.603746165185857 0.021325326728145622
9 3.0992991724890615 49.14605097768851 -3.649030413450628 0.03349519189754263
</pre></div>
</div>
<img alt="_images/59144ec30abf0653966a8bc4b0de4633af4a50211258a507cf2de737fdf06d25.png" src="_images/59144ec30abf0653966a8bc4b0de4633af4a50211258a507cf2de737fdf06d25.png" />
</div>
</div>
</section>
</section>
<section id="bayesian-networks">
<h2>Bayesian networks<a class="headerlink" href="#bayesian-networks" title="Link to this heading">#</a></h2>
<p>As we have seen in the example of Bayesian linear regression, statistical machine learning assumes a <em>generative model</em> of the observed data and infer the posterior probability of variables of your interest, after marginalizing other unobserved variables.</p>
<p>In doing so, representation of the relationships by graphs with random variables as nodes (or vertices) and joint or conditional probabilities as links (or edges or arcs) have turned out to be very useful. They are called <em>graphical models</em>.</p>
<p>Directed graphs representing conditional probabilities by arrows (directed edges) are called <em>Bayesian networks</em>.</p>
<p>For example, Bayesian linear regression can be represented as a Bayesian network as below.</p>
<blockquote>
<div><img src="figures/BN_blr.png" width="200px">
Graphical model for the Bayesian linear regression.
</div></blockquote>
<p>Inference in Bayesian network goes like this: as values of some variables are observed,</p>
<ul class="simple">
<li><p><em>clamp</em> the values of the nodes where observation was made.</p></li>
<li><p>compute the posterior distributions of the nodes along the graph by repeating Bayesian inference and marginalization</p></li>
</ul>
<p>In doing this, the <em>conditional independence</em> of the nodes allows efficient computation.</p>
</section>
<section id="inference-on-a-chain">
<h2>Inference on a chain<a class="headerlink" href="#inference-on-a-chain" title="Link to this heading">#</a></h2>
<p>Here we consider the simplest case of a chain of discrete random variables.</p>
<blockquote>
<div><p><img alt="Chain" src="_images/BN_chain.png" />
Graphical model of a chain of states.</p>
</div></blockquote>
<p>For each node, the variable takes an integer value <span class="math notranslate nohighlight">\(x_n \in \{1,...,K_n\}\)</span> and we consider the joint distribution over the entire nodes:</p>
<div class="math notranslate nohighlight">
\[  p(x_1,...,x_N) = p(x_1)p(x_2|x_1) \cdots p(x_{N-1}|x_{N-2})p(x_N|x_{N-1}). \]</div>
<p>When an observation <span class="math notranslate nohighlight">\(x_N=k\)</span> is made at the end node, we would consider the posterior distribution</p>
<div class="math notranslate nohighlight">
\[  p(x_1,...,x_{N-1}|x_N=k) \propto 
    p(x_1)p(x_2|x_1) \cdots p(x_{N-1}|x_{N-2})p(x_N=k|x_{N-1}).  \]</div>
<p>The posterior distribution of each node <span class="math notranslate nohighlight">\(x_n\)</span> is given by marginalization</p>
<div class="math notranslate nohighlight">
\[  p(x_n|x_N=k) \propto \sum_{x_1}\cdots\sum_{x_{n-1}}\ 
    \sum_{x_{n+1}}\cdots\sum_{x_N}p(x_1,...,x_{N-1}|x_N=k) \]</div>
<div class="math notranslate nohighlight">
\[  = \left\{\sum_{x_{n-1}}p(x_n|x_{n-1}) \cdots
    \sum_{x_1}p(x_2|x_1)p(x_1)\right\} \]</div>
<div class="math notranslate nohighlight">
\[  \times \left\{\sum_{x_{n+1}}p(x_{n+1}|x_n) \cdots 
    \sum_{x_{N-1}}p(x_{N-1}|x_{N-2}) \sum_{x_N}p(x_N=k|x_{N-1})\right\}  \]</div>
<p>This can be computed efficiently by passing two <em>messages</em>:</p>
<ul class="simple">
<li><p>Forward message <span class="math notranslate nohighlight">\(\alpha_n\)</span> of prior:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ \alpha_1 = p(x_1)\]</div>
<div class="math notranslate nohighlight">
\[ \alpha_n = \sum_{x_{n-1}} p(x_n|x_{n-1}) \alpha_{n-1} \]</div>
<ul class="simple">
<li><p>Backward message <span class="math notranslate nohighlight">\(\beta_n\)</span> of likelihood:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ \beta_N = (0,...,1,...0) \]</div>
<p>with <span class="math notranslate nohighlight">\(1\)</span> at <span class="math notranslate nohighlight">\(k\)</span>-th component and</p>
<div class="math notranslate nohighlight">
\[ \beta_n = \sum_{x_{n+1}} p(x_{n+1}|x_n) \beta_{n+1} \]</div>
<p>The posterior distribution for each node is then given by their product</p>
<div class="math notranslate nohighlight">
\[  p(x_n|x_N=k) \propto \alpha_n \beta_n. \]</div>
<p>This is called <em>forward-backward</em> algorithm.</p>
<p>This can be generalized to tree-like networks and the algorithm using forward and backward message passing is known as <em>belief propagation</em>.</p>
<section id="markov-chain">
<h3>Markov chain<a class="headerlink" href="#markov-chain" title="Link to this heading">#</a></h3>
<p>Here is an example of inference in a Markov chain.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Markov</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Class for a Markov chain&quot;&quot;&quot;</span>
    
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ptr</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Create a new environment&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ptr</span> <span class="o">=</span> <span class="n">ptr</span>  <span class="c1"># transition matrix p(x&#39;|x)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Ns</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">ptr</span><span class="p">)</span>  <span class="c1"># number of states</span>
        
    <span class="k">def</span><span class="w"> </span><span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x0</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;generate a sample sequence from x0&quot;&quot;&quot;</span>
        <span class="n">seq</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">step</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span> <span class="c1"># sequence buffer</span>
        <span class="n">seq</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">x0</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">step</span><span class="p">):</span>
            <span class="n">pt1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ptr</span><span class="p">[:,</span> <span class="n">seq</span><span class="p">[</span><span class="n">t</span><span class="p">]]</span> <span class="c1"># prob. of new states</span>
            <span class="n">seq</span><span class="p">[</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Ns</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">pt1</span><span class="p">)</span> <span class="c1"># sample </span>
        <span class="k">return</span> <span class="n">seq</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p0</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;forward message from initial distribution p0&quot;&quot;&quot;</span>
        <span class="n">alpha</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">step</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Ns</span><span class="p">))</span> <span class="c1"># priors</span>
        <span class="n">alpha</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">p0</span>  <span class="c1"># initial distribution</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">step</span><span class="p">):</span>
            <span class="n">alpha</span><span class="p">[</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ptr</span> <span class="o">@</span> <span class="n">alpha</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> 
        <span class="k">return</span> <span class="n">alpha</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">obs</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;backward message from terminal observaion&quot;&quot;&quot;</span>
        <span class="n">beta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">step</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Ns</span><span class="p">))</span> <span class="c1"># likelihoods</span>
        <span class="n">beta</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">obs</span>  <span class="c1"># observation</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">step</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span> <span class="c1"># toward 0</span>
            <span class="n">beta</span><span class="p">[</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">beta</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">ptr</span>
        <span class="k">return</span> <span class="n">beta</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">posterior</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p0</span><span class="p">,</span> <span class="n">obs</span><span class="p">,</span> <span class="n">step</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;forward-backward algorithm&quot;&quot;&quot;</span>
        <span class="n">alpha</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">p0</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span>
        <span class="n">beta</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">obs</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span>
        <span class="n">post</span> <span class="o">=</span> <span class="n">alpha</span><span class="o">*</span><span class="n">beta</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">step</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
            <span class="n">post</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">post</span><span class="p">[</span><span class="n">t</span><span class="p">]</span><span class="o">/</span><span class="nb">sum</span><span class="p">(</span><span class="n">post</span><span class="p">[</span><span class="n">t</span><span class="p">])</span>  <span class="c1"># normalize        </span>
        <span class="k">return</span> <span class="n">post</span>
</pre></div>
</div>
</div>
</div>
<p>Here is an example of directed random walk on a ring.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># stochastic cycling on a ring</span>
<span class="n">ns</span> <span class="o">=</span> <span class="mi">6</span>   <span class="c1"># ring size</span>
<span class="n">ps</span> <span class="o">=</span> <span class="mf">0.3</span>  <span class="c1"># shift probability</span>
<span class="n">Ptr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">ns</span><span class="p">,</span> <span class="n">ns</span><span class="p">))</span>  <span class="c1"># transition matrix</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ns</span><span class="p">):</span>
    <span class="n">Ptr</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">ps</span>
    <span class="n">Ptr</span><span class="p">[(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">%</span><span class="k">ns</span>, i] = ps
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">Ptr</span><span class="p">)</span>
<span class="c1"># create a Markov chain</span>
<span class="n">ring</span> <span class="o">=</span> <span class="n">Markov</span><span class="p">(</span><span class="n">Ptr</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/111a66b931483a9120cb7cf5f88d52b0b9eb535625963aeff88ba2ee52e0fa91.png" src="_images/111a66b931483a9120cb7cf5f88d52b0b9eb535625963aeff88ba2ee52e0fa91.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># a sample trajectory</span>
<span class="n">T</span> <span class="o">=</span> <span class="mi">15</span>
<span class="n">ring</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([1, 2, 2, 2, 2, 2, 3, 3, 3, 4, 5, 5, 0, 0, 0, 1])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># forward message passing</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="n">ring</span><span class="o">.</span><span class="n">forward</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="n">T</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">alpha</span><span class="o">.</span><span class="n">T</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/8e088fe7cf1dd80705a6186bdaf9fd64e5a814b6e91a04f541a13fada799b933.png" src="_images/8e088fe7cf1dd80705a6186bdaf9fd64e5a814b6e91a04f541a13fada799b933.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># backward message passing</span>
<span class="n">beta</span> <span class="o">=</span> <span class="n">ring</span><span class="o">.</span><span class="n">backward</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="n">T</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">beta</span><span class="o">.</span><span class="n">T</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/ac0488e366875ac339def9ec78a1a45545bf95ffcbd6d03ff6ddba7665afe8ec.png" src="_images/ac0488e366875ac339def9ec78a1a45545bf95ffcbd6d03ff6ddba7665afe8ec.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># posterior by their products</span>
<span class="n">post</span> <span class="o">=</span> <span class="n">ring</span><span class="o">.</span><span class="n">posterior</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="n">T</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">post</span><span class="o">.</span><span class="n">T</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/7935061d131cce2876c40ef17ebb6e2e6ffa26b4edb358b34de183e668dfcb33.png" src="_images/7935061d131cce2876c40ef17ebb6e2e6ffa26b4edb358b34de183e668dfcb33.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># a little shifted observation</span>
<span class="n">post</span> <span class="o">=</span> <span class="n">ring</span><span class="o">.</span><span class="n">posterior</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="n">T</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">post</span><span class="o">.</span><span class="n">T</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/45a7d79066c7237ce7484a277640a9f360fbea0f59acee8b89fc231ee31c5445.png" src="_images/45a7d79066c7237ce7484a277640a9f360fbea0f59acee8b89fc231ee31c5445.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># longer sequence</span>
<span class="n">post</span> <span class="o">=</span> <span class="n">ring</span><span class="o">.</span><span class="n">posterior</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="mi">3</span><span class="o">*</span><span class="n">T</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">post</span><span class="o">.</span><span class="n">T</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/4d10a1f21f9753b8424dee45ae9d1fc7d9a015717adc492abd6a0c0760a96450.png" src="_images/4d10a1f21f9753b8424dee45ae9d1fc7d9a015717adc492abd6a0c0760a96450.png" />
</div>
</div>
</section>
</section>
<section id="dynamic-bayesian-inference">
<h2>Dynamic Bayesian Inference<a class="headerlink" href="#dynamic-bayesian-inference" title="Link to this heading">#</a></h2>
<p>Iterative Bayesian inference can be generalized to the case when the hidden variable <span class="math notranslate nohighlight">\(x\)</span> changes dynamically.</p>
<p>We denote the sequence of observation as</p>
<div class="math notranslate nohighlight">
\[  y_{1:t}=(y_1,..,y_t) \]</div>
<p>and the history of underlying state variable as</p>
<div class="math notranslate nohighlight">
\[  x_{1:t}=(x_1,..,x_t). \]</div>
<p>We assume two conditional probability distributions:</p>
<ul class="simple">
<li><p>Dynamics model: <span class="math notranslate nohighlight">\(p(x’|x)\)</span></p></li>
<li><p>Observation model: <span class="math notranslate nohighlight">\(p(y|x)\)</span></p></li>
</ul>
<p>Using the posterior <span class="math notranslate nohighlight">\(p(x_t|y_{1:t})\)</span> computed from the data up to time <span class="math notranslate nohighlight">\(t\)</span>, we use the dymamics model to compute the <em>predictive prior</em>:</p>
<div class="math notranslate nohighlight">
\[  p(x_{t+1}|y_{1:t}) = \int p(x_{t+1}|x_t) p(x_t|y_{1:t}) dx_t \]</div>
<p>by integrating or summing over the possible range of <span class="math notranslate nohighlight">\(x\)</span>.</p>
<p>We can combine this prior with the new coming data <span class="math notranslate nohighlight">\(y_{t+1}\)</span> to update the posterior as:</p>
<div class="math notranslate nohighlight">
\[  p(x_{t+1}|y_{1:t+1}) 
 = \frac{p(y_{t+1}|x_{t+1}) p(x_{t+1}|y_{1:t})}{ p(y_{1:t+1})}. \]</div>
<p>This is called <em>dynamic Bayesian inference</em> and allows real-time tracking of hidden variables from noisy observations.</p>
<p>When <span class="math notranslate nohighlight">\(x\)</span> is discrete, the process is called <em>hidden Markov model (HMM)</em>, which has been used extensively speech processing.</p>
<p>Another example is <em>Kalman filter</em>, in which <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span> are continuous and the dynamics and observation models are linear mapping with Gaussian noise.</p>
<section id="hidden-markov-model">
<h3>Hidden Markov model<a class="headerlink" href="#hidden-markov-model" title="Link to this heading">#</a></h3>
<p>Here is a simple implementation of HMM based on the Markov chain above.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">HMM</span><span class="p">(</span><span class="n">Markov</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Hidden Markov model&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ptr</span><span class="p">,</span> <span class="n">pobs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Create HMM with transition and observation models&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">ptr</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pobs</span> <span class="o">=</span> <span class="n">pobs</span>  <span class="c1"># observation model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">No</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pobs</span><span class="p">)</span>  <span class="c1"># number of observations</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pst</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Ns</span><span class="p">)</span><span class="o">/</span><span class="bp">self</span><span class="o">.</span><span class="n">Ns</span>  <span class="c1"># state distribution</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Ns</span><span class="p">)</span>  <span class="c1"># predictive distribution</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x0</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;generate a sample sequence from x0&quot;&quot;&quot;</span>
        <span class="n">xt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">step</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span> <span class="c1"># state sequence</span>
        <span class="n">yt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">step</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span> <span class="c1"># observation sequence</span>
        <span class="n">xt</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">x0</span>
        <span class="n">po</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pobs</span><span class="p">[:,</span> <span class="n">x0</span><span class="p">]</span> <span class="c1"># prob. of observation</span>
        <span class="n">yt</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">No</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">po</span><span class="p">)</span> <span class="c1"># observe</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">step</span><span class="p">):</span>
            <span class="n">ps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ptr</span><span class="p">[:,</span> <span class="n">xt</span><span class="p">[</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span>  <span class="c1"># prob. of new states</span>
            <span class="n">xt</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Ns</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">ps</span><span class="p">)</span> <span class="c1"># transit </span>
            <span class="n">po</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pobs</span><span class="p">[:,</span> <span class="n">xt</span><span class="p">[</span><span class="n">t</span><span class="p">]]</span>   <span class="c1"># prob. of observation</span>
            <span class="n">yt</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">No</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">po</span><span class="p">)</span> <span class="c1"># observe </span>
        <span class="k">return</span> <span class="n">xt</span><span class="p">,</span> <span class="n">yt</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;predictive prior by transition model&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ptr</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">pst</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">obs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;update posterior by observation&quot;&quot;&quot;</span>
        <span class="n">prl</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pobs</span><span class="p">[</span><span class="n">obs</span><span class="p">]</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">pred</span> <span class="c1"># likelihood*prior</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pst</span> <span class="o">=</span> <span class="n">prl</span><span class="o">/</span><span class="nb">sum</span><span class="p">(</span><span class="n">prl</span><span class="p">)</span>  <span class="c1">#normalize</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;reset state probability&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pst</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Ns</span><span class="p">)</span><span class="o">/</span><span class="bp">self</span><span class="o">.</span><span class="n">Ns</span>  <span class="c1"># uniform</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">obs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;one step of dynamic bayesian inference&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">pst</span>  <span class="c1"># new prior</span>
</pre></div>
</div>
</div>
</div>
<p>Here is an example of directed random walk on a ring, like a mouse walking on a circular track.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># random walk on a ring</span>
<span class="n">ns</span> <span class="o">=</span> <span class="mi">6</span>   <span class="c1"># ring size</span>
<span class="n">ps</span> <span class="o">=</span> <span class="mf">0.3</span>  <span class="c1"># shift probability</span>
<span class="n">Ptr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">ns</span><span class="p">,</span> <span class="n">ns</span><span class="p">))</span>  <span class="c1"># transition matrix</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">ns</span><span class="p">):</span>
    <span class="n">Ptr</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">ps</span>
    <span class="n">Ptr</span><span class="p">[(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">%</span><span class="k">ns</span>, i] = ps
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">Ptr</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;state&quot;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;next state&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/0f12002896f1979b7222832735a44c6012a71a31536c6bf7a564a599b00e6ec7.png" src="_images/0f12002896f1979b7222832735a44c6012a71a31536c6bf7a564a599b00e6ec7.png" />
</div>
</div>
<p>Suppose we have three coarse position sensors, which send signal only intermittently, and we want to estimate where the mouse is.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Blurred intermittent observation model</span>
<span class="n">no</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">po</span> <span class="o">=</span> <span class="mf">0.3</span>
<span class="n">Pobs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">no</span><span class="p">,</span> <span class="n">ns</span><span class="p">))</span>  <span class="c1"># p(obs|state)</span>
<span class="n">Pobs</span><span class="p">[</span><span class="mi">0</span><span class="p">,:]</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">po</span>  <span class="c1"># no information</span>
<span class="n">Pobs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">Pobs</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="n">Pobs</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">]</span> <span class="o">=</span> <span class="n">po</span>
<span class="n">Pobs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">Pobs</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">Pobs</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">Pobs</span><span class="p">[</span><span class="mi">2</span><span class="p">:</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">]</span> <span class="o">=</span> <span class="n">po</span><span class="o">/</span><span class="mi">2</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">Pobs</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;state&quot;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;observation&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/ba7cfa8bc71cd102c039d7d7e9a50eb69b8a00cef9028a00ae41f02e7249ddc6.png" src="_images/ba7cfa8bc71cd102c039d7d7e9a50eb69b8a00cef9028a00ae41f02e7249ddc6.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># crate a HMM</span>
<span class="n">ring</span> <span class="o">=</span> <span class="n">HMM</span><span class="p">(</span><span class="n">Ptr</span><span class="p">,</span> <span class="n">Pobs</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># sample a state trajectory and observations</span>
<span class="n">T</span> <span class="o">=</span> <span class="mi">30</span>
<span class="n">xt</span><span class="p">,</span> <span class="n">yt</span><span class="p">,</span> <span class="o">=</span> <span class="n">ring</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xt</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">yt</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;ro&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/c99b99e2028655a63d162f18533508956b5e3a180765f92b9fca03b88b6cd301.png" src="_images/c99b99e2028655a63d162f18533508956b5e3a180765f92b9fca03b88b6cd301.png" />
</div>
</div>
<p>From such noisy intermittent observations, how can we estimate the state?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Dynamic Bayesian inference in HMM</span>
<span class="n">post</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">T</span><span class="p">,</span> <span class="n">ns</span><span class="p">))</span>  <span class="c1"># posterior trajectory</span>
<span class="n">ring</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">):</span>
    <span class="n">post</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">ring</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">yt</span><span class="p">[</span><span class="n">t</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">post</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">origin</span><span class="o">=</span><span class="s1">&#39;lower&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;t&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;state&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/e7b33517f36d64e5c266294417fa64d58afcc775ef903fe372316085eae86aa4.png" src="_images/e7b33517f36d64e5c266294417fa64d58afcc775ef903fe372316085eae86aa4.png" />
</div>
</div>
<p>Even when there is no useful sensory input, you can predict the state distribution by the dynamic model.
When a sensory input becomes available, prediction is corrected and sharpened.</p>
<p>After sensory input, you can also reflect back and consider which previous states were more likely using <em>forward-backward</em> algorithms.</p>
</section>
</section>
<section id="bayesian-sensorimotor-processing">
<h2>Bayesian sensorimotor processing<a class="headerlink" href="#bayesian-sensorimotor-processing" title="Link to this heading">#</a></h2>
<p>Our life is full of uncertainty. In sensory perception, we need to cope with noise, delay and occulusion and also overcome fundamental ill-posedness, such as to identify the 3D location of your target from 2D retinal images or sounds to two ears.</p>
<p>To find a practical solution to such ill-posed problems, we need to make use of some prior assumptions, such as the light usually comes from the top or objects don’t jump abruptly.</p>
<p>Bayesian inference provides a principled way for combining any prior knowledge with sensory evidence. Indeed there are several lines of psychological evidence suggesting that humans and animals integrate knowledge from prior experience or multi-modal sensory information as predicted by Bayesian inference (Knill &amp; Pouget 2004, Kording &amp; Wolpert 2004, Doya et al. 2007).</p>
</section>
<section id="bayesian-computation-in-the-brain">
<h2>Bayesian computation in the brain<a class="headerlink" href="#bayesian-computation-in-the-brain" title="Link to this heading">#</a></h2>
<p>How such Bayesian computation realized in the brain? How does the brain represent and manipulate probability distributions?</p>
<p>One possibility is that the <em>receptive field</em> of a neuron represents a basis function in the sensory space and the activities of a population of neurons represent a probability distribution. This idea is called <em>probabilistic population code</em> (Zemel et al. 2004, Ma et al. 2006).</p>
<p>The cerebral cortex has a hierarchical organization and bi-directional connections between lower and higher areas originating from specific layers.
There have been serveral hypotheses about how such hierarchical recurrent network can realize Bayesian inference, such as belif propagation (Lochmann &amp; Deneve 2011) and variational <em>free energy</em> approximation (Friston 2005, 2010; Bogacz 2017).</p>
<blockquote>
<div><p><img alt="Bogacz17" src="_images/Bogacz17.png" />
This tutorial illustrates how variational free-energy approximation of posterior probability works and how such mechanisms might be mapped onto the cortical circuit (from Bobacz 2017).</p>
</div></blockquote>
<p>There has been only scarse attempts at directly testing those hypotheses, but a recent two-photon imaging experiment showed the evidence for dynamic Bayesian inference by populations of neurons in the parietal cortex (Funamizu et al. 2016).</p>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Bishop CM (2006) Pattern Recognition and Machine Learning. Springer. <a class="reference external" href="https://www.microsoft.com/en-us/research/people/cmbishop/prml-book/">https://www.microsoft.com/en-us/research/people/cmbishop/prml-book/</a></p>
<ul>
<li><p>Chapter 3: Bayesian linear regression</p></li>
<li><p>Chapter 8: Graphical models</p></li>
</ul>
</li>
</ul>
<section id="bayesian-sensorimotor-integration">
<h3>Bayesian sensorimotor integration<a class="headerlink" href="#bayesian-sensorimotor-integration" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Knill DC, Pouget A (2004) The Bayesian brain: the role of uncertainty in neural coding and computation. Trends in neurosciences 27:712-719. <a class="reference external" href="https://doi.org/10.1016/j.tins.2004.10.007">https://doi.org/10.1016/j.tins.2004.10.007</a></p></li>
<li><p>Körding KP, Wolpert DM (2004) Bayesian integration in sensorimotor learning. Nature 427:244-247. <a class="reference external" href="https://doi.org/10.1038/nature02169">https://doi.org/10.1038/nature02169</a></p></li>
<li><p>Doya K, Ishii S, Pouget A, Rao R (2007) Bayesian Brain: Probabilistic Approach to Neural Coding and Learning. MIT Press.</p></li>
</ul>
</section>
<section id="probabilistic-population-codes">
<h3>Probabilistic population codes<a class="headerlink" href="#probabilistic-population-codes" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Zemel RS, Dayan P, Pouget A (1998) Probabilistic interpretation of population codes. Neural computation 10:403-430. <a class="reference external" href="https://doi.org/10.1162/089976698300017818">https://doi.org/10.1162/089976698300017818</a></p></li>
<li><p>Ma WJ, Beck JM, Latham PE, Pouget A (2006) Bayesian inference with probabilistic population codes. Nature neuroscience 9:1432-1438. <a class="reference external" href="https://doi.org/10.1038/nn1790">https://doi.org/10.1038/nn1790</a></p></li>
</ul>
</section>
<section id="baysian-inference-in-the-cortical-circuit">
<h3>Baysian inference in the cortical circuit<a class="headerlink" href="#baysian-inference-in-the-cortical-circuit" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Bogacz R (2017) A tutorial on the free-energy framework for modelling perception and learning. Journal of Mathematical Psychology. 76, 198–211. <a class="reference external" href="https://doi.org/10.1016/j.jmp.2015.11.003">https://doi.org/10.1016/j.jmp.2015.11.003</a></p></li>
<li><p>Friston K (2005). A theory of cortical responses. Philos Trans R Soc Lond B Biol Sci, 360, 815-36. <a class="reference external" href="http://doi.org/10.1098/rstb.2005.1622">http://doi.org/10.1098/rstb.2005.1622</a></p></li>
<li><p>Friston K (2010). The free-energy principle: a unified brain theory? Nat Rev Neurosci, 11, 127-38. <a class="reference external" href="http://doi.org/10.1038/nrn2787">http://doi.org/10.1038/nrn2787</a></p></li>
<li><p>Lochmann T, Deneve S (2011) Neural processing as causal inference. Current opinion in neurobiology 21:774-781. <a class="reference external" href="https://doi.org/10.1016/j.conb.2011.05.018">https://doi.org/10.1016/j.conb.2011.05.018</a></p></li>
<li><p>Funamizu A, Kuhn B, Doya K (2016) Neural substrate of dynamic Bayesian inference in the cerebral cortex. Nature Neuroscience 19:1682-1689. <a class="reference external" href="https://doi.org/10.1038/nn.4390">https://doi.org/10.1038/nn.4390</a></p></li>
</ul>
</section>
</section>
<div class="toctree-wrapper compound">
</div>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="Unsupervised_Exercise.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Unsupervised Learning: Exercise</p>
      </div>
    </a>
    <a class="right-next"
       href="Bayesian_Exercise.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Bayesian Approaches: Exercise</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#contents">Contents</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayes-theorem">Bayes’ Theorem</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-mouse-in-a-bush">Example: mouse in a bush</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#iterative-bayesian-inference">Iterative Bayesian Inference</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#coin-toss">Coin toss</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#noisy-observation">Noisy observation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-approaches-in-machine-learning">Bayesian approaches in machine learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-linear-regression">Bayesian Linear Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#predictive-distribution">Predictive distribution</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-model-comparison">Bayesian model comparison</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#computing-model-evidence">Computing model evidence</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-networks">Bayesian networks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#inference-on-a-chain">Inference on a chain</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#markov-chain">Markov chain</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dynamic-bayesian-inference">Dynamic Bayesian Inference</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hidden-markov-model">Hidden Markov model</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-sensorimotor-processing">Bayesian sensorimotor processing</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-computation-in-the-brain">Bayesian computation in the brain</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-sensorimotor-integration">Bayesian sensorimotor integration</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#probabilistic-population-codes">Probabilistic population codes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#baysian-inference-in-the-cortical-circuit">Baysian inference in the cortical circuit</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Kenji Doya
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>