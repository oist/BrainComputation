
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Chapter 9. Meta-Learning &#8212; Brain Computation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'Meta';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Exercise: Meta-Learning" href="Meta_Exercise.html" />
    <link rel="prev" title="Exercise: Multiple Agents" href="Multiple_Exercise.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="BrainComputation.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/BC_logo.png" class="logo__image only-light" alt="Brain Computation - Home"/>
    <script>document.write(`<img src="_static/BC_logo.png" class="logo__image only-dark" alt="Brain Computation - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="BrainComputation.html">
                    Brain Computation: A Hands-on Guidebook
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="Introduction.html">Chapter 1. Introduction</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="Neurons.html">Chapter 2. Neural Modeling and Analysis</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="Neurons_Exercise.html">Exercise: Neural Modeling and Analysis</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="Supervised.html">Chapter 3: Supervised Learning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="Supervised_Exercise.html">Exercise: Supervised Learning</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="Reinforcement.html">Chapter 4. Reinforcement Learning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="Gymnasium.html">Appendix: Gymnasium</a></li>
<li class="toctree-l2"><a class="reference internal" href="Reinforcement_Exercise.html">Exercise: Reinforcement Learning</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="Unsupervised.html">Chapter 5. Unsupervised Learning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="Unsupervised_Exercise.html">Exercise: Unsupervised Learning</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="Bayesian.html">Chatper 6. Bayesian Approaches</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="PyStan.html">Appendix: Bayesian Model Fitting by PyStan</a></li>
<li class="toctree-l2"><a class="reference internal" href="Bayesian_Exercise.html">Exercise: Bayesian Approaches</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="Deep.html">Chapter 7: Deep Learning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="Deep_Exercise.html">Exercise: Deep Learning</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="Multiple.html">Chapter 8. Multiple Agents</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="Multiple_Exercise.html">Exercise:  Multiple Agents</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="current reference internal" href="#">Chapter 9. Meta-Learning</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="Meta_Exercise.html">Exercise: Meta-Learning</a></li>
</ul>
</details></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://mybinder.org/v2/gh/oist/BrainComputation/master?urlpath=tree/Meta.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Binder"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Binder logo" src="_static/images/logo_binder.svg">
  </span>
<span class="btn__text-container">Binder</span>
</a>
</li>
      
      
      
      
      <li><a href="https://colab.research.google.com/github/oist/BrainComputation/blob/master/Meta.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Colab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Colab logo" src="_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>



<a href="https://github.com/oist/BrainComputation" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/Meta.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Chapter 9. Meta-Learning</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#contents">Contents</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tuning-meta-parameters">Tuning meta-parameters</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-rate">Learning Rate</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exploration-exploitation">Exploration-Exploitation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#temporal-discount-factor">Temporal discount factor</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#self-tuning-of-meta-parameters">Self-tuning of  meta-parameters</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neuromodulators">Neuromodulators</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#serotonin-and-delayed-reward">Serotonin and delayed reward</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#acetylcholine-noradrenaline-and-learning">Acetylcholine, noradrenaline and learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sensory-representation-learning">Sensory representation learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#models-and-latent-variables">Models and latent variables</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#modularity-and-compositionality">Modularity and compositionality</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pathway-gating">Pathway gating</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#meta-reinforcement-learning">Meta-reinforcement learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-rate-and-exploration">Learning rate and exploration</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#serotonin">Serotonin:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#acetylcholine-and-noradrenaline-norepinephrine">Acetylcholine and noradrenaline (norepinephrine)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#representation-learning">Representation learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-based-strategies">Model-based strategies</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Modularity and compositionality</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Pathway gating</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Meta-reinforcement learning</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="chapter-9-meta-learning">
<h1>Chapter 9. Meta-Learning<a class="headerlink" href="#chapter-9-meta-learning" title="Link to this heading">#</a></h1>
<div class="math notranslate nohighlight">
\[ % Latex macros
\newcommand{\mat}[1]{\begin{pmatrix} #1 \end{pmatrix}}
\newcommand{\p}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\b}[1]{\boldsymbol{#1}}
\newcommand{\c}[1]{\mathcal{#1}}
\newcommand{\w}{\boldsymbol{w}}
\newcommand{\x}{\boldsymbol{x}}
\]</div>
<section id="contents">
<h2>Contents<a class="headerlink" href="#contents" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Tuning meta-parameters</p></li>
<li><p>Reuse of representations</p></li>
<li><p>Reuse of models</p></li>
<li><p>Modularity and compositionality</p></li>
</ul>
<p>We learn to understand and control many things in our life and learning one task often makes learning another task easier. This observation has been a target of study under various keywords, such as</p>
<ul class="simple">
<li><p>Lifelong learning (Thrun 1996)</p></li>
<li><p>Meta learning (Doya 2002)</p></li>
<li><p>Transfer learning (Taylor &amp; Stone 2009)</p></li>
</ul>
</section>
<section id="tuning-meta-parameters">
<h2>Tuning meta-parameters<a class="headerlink" href="#tuning-meta-parameters" title="Link to this heading">#</a></h2>
<p>Learning algorithms change the parameters of the model, such as the weights of neural networks, but most algorithms have higher-level parameters that control how learning goes on.
They are called <em>hyperparamters</em> or <em>metaparameters</em> and control the balance of various trade-offs.
Those meta parameteres are often tuned by machine learning engineers based on domain knowledge or past experience, but making the parameter tuning automatic or unnecessary is an important research topic.</p>
<p>How that is done in our brain is also an important question in neuroscience.</p>
<section id="learning-rate">
<h3>Learning Rate<a class="headerlink" href="#learning-rate" title="Link to this heading">#</a></h3>
<p>The learning rate parameter deals with the trade-off of quick learning and stable memory.
Stochastic gradient decent and other online learning algorithms take the form</p>
<div class="math notranslate nohighlight">
\[ w(t+1) = w(t) + \alpha(u(t) - w(t)) \]</div>
<p>where <span class="math notranslate nohighlight">\(u(t)\)</span> is the input for the parameter update.</p>
<p>This can be re-formulated as</p>
<div class="math notranslate nohighlight">
\[ w(t+1) = \sum_{s=1}^t \alpha(1-\alpha)^{t-s}u(s)\]</div>
<p>showing that <span class="math notranslate nohighlight">\(w\)</span> is an exponentially weighted average of past inputs <span class="math notranslate nohighlight">\(u(s)\)</span> with the decaying factor <span class="math notranslate nohighlight">\(1-\alpha\)</span>.
With a large <span class="math notranslate nohighlight">\(\alpha\)</span> close to one, past experiences are quickly forgotten.</p>
<p>To effectively average over about <span class="math notranslate nohighlight">\(N\)</span> past samples, the learning rate has to be set at the scale of <span class="math notranslate nohighlight">\(\alpha\simeq\frac{1}{N}\)</span></p>
<p>In a stationary environment, a good strategy is to take an even average of past inputs</p>
<div class="math notranslate nohighlight">
\[ w(t+1) = \frac{1}{t}\sum_{s=1}^t u(s) \]</div>
<p>This can be realized by hyperbolically decaying learning rate
<span class="math notranslate nohighlight">\(\alpha=\frac{1}{t}\)</span> because</p>
<div class="math notranslate nohighlight">
\[ w(t+1) = \frac{1}{t}(\sum_{s=1}^{t-1}u(s) + u(t))
 = \frac{1}{t}((t-1)w(t) + u(t)) \]</div>
<div class="math notranslate nohighlight">
\[ = w(t) + \frac{1}{t}(u(t) - w(t)) \]</div>
</section>
<section id="exploration-exploitation">
<h3>Exploration-Exploitation<a class="headerlink" href="#exploration-exploitation" title="Link to this heading">#</a></h3>
<p>In reinforcement learning, the balance of exploration of novel actions and exploitation by an action that is known to be good is controlled by <span class="math notranslate nohighlight">\(\epsilon\)</span> in <span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy action selection and the <em>inverse temperature</em> <span class="math notranslate nohighlight">\(\beta\)</span> in the <em>softmax</em> or <em>Boltzmann</em> action selection</p>
<div class="math notranslate nohighlight">
\[ p(a|s) = \frac{e^{\beta Q(s,a)}}{\sum_{a'\in\c{A}}e^{\beta Q(s,a')}} \]</div>
<p>where <span class="math notranslate nohighlight">\(\c{A}\)</span> is the set of available actions.</p>
<p>This can be seen as maximization of the negative free energy</p>
<div class="math notranslate nohighlight">
\[ –F = E[ Q(s,a) - \frac{1}{\beta}\log p(a|s)] \]</div>
<p>which is a sum of expected action value and the entropy of action selection probability.</p>
<p>As learning goes on, to reduce exploration and to promote exploitation, the inverse temperature is gradually increased, or the temperature <span class="math notranslate nohighlight">\(\tau=\frac{1}{\beta}\)</span> is reduced.
This is called <em>annealing</em>.</p>
<p>Another ways to promote exploration in early stage of learning is to give an additional reward for the states or state-action pairs that have not been tried before.
This is known as <em>novelty bonus</em> (Kakade &amp; Dayan, 2002).
A similar effect can be realized by optimistic initial setting of the value functions.</p>
<p>Further sophistication is <em>Bayesian reinforcement learning</em> which tries to learn not just the expected reward but the distribution of the reward <span class="math notranslate nohighlight">\(P(r|s,a)\)</span>.
Starting from a flat prior distribution, as the reward distribution becomes sharper, there is less need for exploration.
Knowledge of the reward distribution further allows optimistic, risk-seeking action selection or conservative, risk-avoiding action selection.</p>
</section>
<section id="temporal-discount-factor">
<h3>Temporal discount factor<a class="headerlink" href="#temporal-discount-factor" title="Link to this heading">#</a></h3>
<p>The temporal discount factor <span class="math notranslate nohighlight">\(\gamma\)</span> defines the temporal scale of maximization of future rewards</p>
<div class="math notranslate nohighlight">
\[ E[ r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + ...] \]</div>
<p>A low setting of <span class="math notranslate nohighlight">\(\gamma\)</span> can result in a short-sighted or impulsive behaviors which neglect long-term consequences of an action.</p>
<p>Although a high setting of <span class="math notranslate nohighlight">\(\gamma\)</span> promotes a long-term optimal behaviors, setting <span class="math notranslate nohighlight">\(\gamma\)</span> very close to one can make prediction more demanding, thus takes long time to learn.</p>
<p>For an average reward <span class="math notranslate nohighlight">\(r_t\sim r_0\)</span>, the value function takes the order of</p>
<div class="math notranslate nohighlight">
\[ V \simeq \frac{r_0}{1-\gamma} \]</div>
<p>which grows very large as <span class="math notranslate nohighlight">\(\gamma\)</span> is set close to one.
Then the temporal difference error</p>
<div class="math notranslate nohighlight">
\[ \delta_t = r_{t+1} + \gamma V(s_{t+1}) - V(s_t) \]</div>
<p>is dominated by the temporal difference of value functions, responding less to the encountered reward <span class="math notranslate nohighlight">\(r_{t+1}\)</span>.</p>
<p>If it is known that approximately <span class="math notranslate nohighlight">\(n\)</span> steps are necessary from the initial state of the goal states, the temporal discounting should be set at the order of <span class="math notranslate nohighlight">\(\frac{1}{1-\gamma}=n\)</span>, i.e., <span class="math notranslate nohighlight">\(\gamma=1-\frac{1}{n}\)</span>.</p>
</section>
</section>
<section id="self-tuning-of-meta-parameters">
<h2>Self-tuning of  meta-parameters<a class="headerlink" href="#self-tuning-of-meta-parameters" title="Link to this heading">#</a></h2>
<p>Can we use reinforcement learning or evolutionary algorithms to find good  meta-parameteres for a given range of tasks or environments?</p>
<p>That idea had been tested with reinforcement learning (Schweighofer, Doya 2003) and evolutionary algorithm (Elfwing et al. 2011).</p>
<p>More recently, the idea of automatizing parameter tuning and model selection has been addressed in the project of <em>AutoML</em></p>
<ul class="simple">
<li><p><a class="reference external" href="http://automl.chalearn.org">http://automl.chalearn.org</a></p></li>
<li><p><a class="reference external" href="https://cloud.google.com/automl/">https://cloud.google.com/automl/</a></p></li>
</ul>
</section>
<section id="neuromodulators">
<h2>Neuromodulators<a class="headerlink" href="#neuromodulators" title="Link to this heading">#</a></h2>
<p><em>Neuromodulators</em> are a subset of neurotransmitters that are not simply excitatory or inhibitory, but have complex, sometimes long lasting effects depending on the receptors.
The most representative neuromodulators are <em>dopamine (DA)</em>, <em>serotonin (5HT)</em>, <em>noradrenaline (NA)</em> (also called <em>norepinephrine (NE)</em>), and <em>acetylcholine (ACh)</em>.</p>
<p>The neurons that synthesize those neuromodulators are located in specific areas in the brain stem and their axons project broadly to the cerebral cortex and other brain areas.</p>
<p>From these features, neuromodulators are proposed to broadcast some signals and to regulate some global parameters of brain function.</p>
<blockquote>
<div><p>{figure-md} Fig_Modulators
<img alt="Neuromodulators" src="_images/Modulators.png" />
Major neuromodulators and their projections.
Red: <em>dopamine (DA)</em> from  <em>ventral tegmental area (VTA)</em> and the <em>substantia nigra pars compacta (SNc)</em>.
Green: <em>serotonin (5HT)</em> from the <em>dorsal raphe (DR)</em> and <em>median raphe (MR)</em> nuclei.
Blue: <em>noradrenaline (NA)</em>, or <em>norepinephrine (NE)</em>, from the <em>locus ceorelues (LC)</em>.
Magenta: <em>acetylcholine (ACh)</em> from the <em>septum (S)</em>, <em>mynert (M)</em> nucleus, and the <em>pedunculo pontine nucleus (PPTN)</em>.
(from Doya, 2002).</p>
</div></blockquote>
<p>Dopamine neurons in the <em>ventral tegmental area (VTA)</em> and the <em>substantia nigra pars compacta (SNc)</em> have been shown to represent the reward prediction error signal, the most important global learing signal <span class="math notranslate nohighlight">\(\delta\)</span> in reinforcement learning (Schultz 1998).</p>
<p>Building up on this notion, Doya (2002) proposed other major neuromodulators also encode and regulate global signals in reinforcement learning, namely, serotonin for the temporal discount factor <span class="math notranslate nohighlight">\(\gamma\)</span>, noradrenaline for the inverse temperature <span class="math notranslate nohighlight">\(\beta\)</span>, and acetylcholine for the learning rate <span class="math notranslate nohighlight">\(\alpha\)</span>.</p>
</section>
<section id="serotonin-and-delayed-reward">
<h2>Serotonin and delayed reward<a class="headerlink" href="#serotonin-and-delayed-reward" title="Link to this heading">#</a></h2>
<p>Motivated by the hypothesis, there has been a series of studies assessing the role of serotonin in regulating the behaviors for delayed rewards.</p>
<p>For example, Miyazaki et al. (2011) showed that serotonergic neurons in the dorsal raphe nucleus increase firing when rats kept waiting for a food pellet or water spout to come out.</p>
<p>More recently, it was shown in multiple laboratories that optogenetic activation of dorsal raphe serotoin neurons promote behaviors for delayed rewards (Miyazaki et al. 2014, 2018; Lottem et al. 2018).</p>
<p>However, the dorsal raphe serotonin neurons have been implicated in other behaviors as well, such as flexible switching of choice in the reversal task (Matias et al. 2017).</p>
<p>Grossman et al. (2021) proposed a meta-learning model in which the learning rate is increased with an increase in the reward prediction error, and suggested that serotonin is involved in meta-learning by monitoring recent reward prediction errors.</p>
</section>
<section id="acetylcholine-noradrenaline-and-learning">
<h2>Acetylcholine, noradrenaline and learning<a class="headerlink" href="#acetylcholine-noradrenaline-and-learning" title="Link to this heading">#</a></h2>
<p>Acetylcholine have been shown to facilitate learning from new sensory inputs (Hasselmo &amp; Sarter 2010).</p>
<p>Noradrenalinergic neurons in the locus coeruleus has been shown to have phasic and tonic modes of operation. While phasic activation was suggested to promote selection of the optimal response for exploitation (Usher et al. 1999), tonic activities have been suggested to promote exploration of suboptimal actions (Aston-Jones &amp; Cohen 2005).</p>
<p>Based on a Bayesian framework of learning, Yu and Dayan (2005) proposed a theory of differential roles of acetylcholine and noradrenaline; acetylcholine for expected uncertainty and noradrenaline for unexpected uncertainty.</p>
</section>
<section id="sensory-representation-learning">
<h2>Sensory representation learning<a class="headerlink" href="#sensory-representation-learning" title="Link to this heading">#</a></h2>
<p>When an animal learn a behavior, such as classical conditioning, a big challenge is to figure out which of the sensory inputs, such as vision, audition and odor, is relevant for any responses.
Once the animal realizes which sensory input is relevant in the task, such as sound, subsequent learning can be much faster, for example, linking other sounds to action or reward.</p>
<p>Courville and colleageus derived a Bayesian framework of how an animal infers the hiddden cause of stimuli and reward (Courville et al. 2005).</p>
<p>Having appropriate set of sensory features is critical in pattern recognition and other tasks (Bengio et al. 2012).
A common practice in visual object recognition is to re-use the features learned in the hidden layers of a deep neural network trained by a big dataset by re-training only the weights in the upper layers for a new but similar task.</p>
</section>
<section id="models-and-latent-variables">
<h2>Models and latent variables<a class="headerlink" href="#models-and-latent-variables" title="Link to this heading">#</a></h2>
<p>In reinforcement learning, an agent learns a policy given the environmental dynamics and reward setting. In the <em>model-free</em> approach, an agent learns  a policy for each setting of the environment.</p>
<p>In the <em>model-based</em> approach, an agent learns internal models of the state dynamics and reward function, and use internal computation to infer what is the best action. This indirectness gives computational burden for real-time execution, but may provide a benefit in adaptation, such as in the case where only the goal or the reward setting is changed but the state dynamics stays the same.</p>
<p>The behavioral benefit and neural mechanisms for model-based reinforcement learning were demonstrated by Glascher et al. (2010) by asking Caltech students to first learn a tree-like state transition, then disclosing the rewards at leaf nodes and finally letting them to find the right action sequence.</p>
<p>Daw et al. (2011) took a similar two-step task and analyzed how subjects respond to a large reward following a rare transition in the first step.</p>
</section>
<section id="modularity-and-compositionality">
<h2>Modularity and compositionality<a class="headerlink" href="#modularity-and-compositionality" title="Link to this heading">#</a></h2>
<p>In learning the model of the environment, rather than to learn a monolithic model to predict everything, it is more practical to learn multiple models for different situations or aspects of the environment and select or combine them as required.</p>
<p>Learning and use of modular internal models have been demonstrated, for example, in motor control tasks (Ghaharamani &amp; Wolpert 1997).
Such notions promoted computational architectures for modular learning and control, such as the MOSAIC architecture (Wolpert et al. 2003).</p>
<p>In reinforcement learning and optimal control theory, how to design controllers that can be efficiently combined is an area of active research (Todorov 2009).</p>
<p>Yang et al. (2018) trained a single recurrent neural network to perform 20 different cognitive tasks and analyzed how the hidden neurons are used in different tasks.
They found different sets of hidden neurons develop into multiple clusters specialized for different cognitive processes, allowing compositional reuse of learned modules.</p>
<p>In cognitive science, compositionality of models and skills is also regarded as a major component in human intelligence (Lake et al. 2017).</p>
</section>
<section id="pathway-gating">
<h2>Pathway gating<a class="headerlink" href="#pathway-gating" title="Link to this heading">#</a></h2>
<p>Most studies of functional MRI assume that, when a subject is asked to perform a certain task, computational modules requires for that are activated and identify brain areas specialized in particular computations.
However, it is unknown how those required modules are activated and connected appropriately.
This poses us interesting problems both at computational and biophysical implementation levels.</p>
<p>Wang and Yang (2018) termed this as the <em>pathway gating</em> problem.
They considered possible biophysical mechanisms, focusing on the roles of different inhibitory neurons in the cortical circuit.</p>
<p>Fernando et al. (2017) proposed <em>Pathnet</em> to use evolutionary optimization to find out which pathway in a deep neural network is to be used for each particular task.</p>
</section>
<section id="meta-reinforcement-learning">
<h2>Meta-reinforcement learning<a class="headerlink" href="#meta-reinforcement-learning" title="Link to this heading">#</a></h2>
<p>While reinforcement learning is based on the gradual learning of the policy, animals and humans some times change their policy quickly based on the recent experience of actions and rewards. A typical example is the “win-stay-lose-switch” strategy.</p>
<p>Ito &amp; Doya (2015) analyzed the choice sequences of rats in a binary choice task and showed that a finite-state model that assumes the agent transits between a number of discrete states based on the action-reward experience and takes a policy depending on the state can fit the animal behaviors better than reinforcement learning models.</p>
<p>Wang et al. (2018) trained a recurrent neural network to perform a reinforcement learning task with variable parameters, such as variable reward probabilities for left and right choices in a bandit task.
After sufficient training, they found that the network can adapt to new parameter setting even when their weights are fixed.
That was because the hidden units had learned to change their state</p>
<p>This was because the task-relevant features, such as the reward probability for different actions, are represented and update in the hidden state of the recurrent neural network, and the output was changed by the latent variables. They called this mechanism as “meta-reinforcement learning.”</p>
<p>In large language models (LLMs), the network produces a variety of output based on the text presented, some of them providing the context or task domain. The adaptive output by such preceding inputs is called “in-context learning.”</p>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Thrun S, Pratt L, eds. (1998) Learning to Learn: Springer.</p></li>
<li><p>Lake BM, Ullman TD, Tenenbaum JB, Gershman SJ (2017). Building machines that learn and think like people. Behav Brain Sci, 40, e253. <a class="reference external" href="http://doi.org/10.1017/S0140525X16001837">http://doi.org/10.1017/S0140525X16001837</a></p></li>
<li><p>Taylor ME, Stone P (2009) Transfer learning for reinforcement learning domains: A survey. Journal of Machine Learning Research, 10:1633-1685. <a class="reference external" href="https://www.jmlr.org/papers/v10/taylor09a.html">https://www.jmlr.org/papers/v10/taylor09a.html</a></p></li>
<li><p>Doya K (2002) Metalearning and Neuromodulation. Neural Networks, 15:495–506. <a class="reference external" href="https://doi.org/10.1016/S0893-6080(02)00044-8">https://doi.org/10.1016/S0893-6080(02)00044-8</a></p></li>
<li><p>Doya K (2008) Modulators of decision making. Nature Neuroscience, 11:410-416. <a class="reference external" href="https://doi.org/10.1038/nn2077">https://doi.org/10.1038/nn2077</a></p></li>
<li><p>Schweighofer N, Doya K (2003). Meta-learning in reinforcement learning. Neural Networks, 16, 5-9. <a class="reference external" href="https://doi.org/10.1016/s0893-6080(02)00228-9">https://doi.org/10.1016/s0893-6080(02)00228-9</a></p></li>
<li><p>Elfwing S, Uchibe E, Doya K, Christensen HI (2011) Darwinian embodied evolution of the learning ability for survival. Adaptive Behavior 19:101-120. <a class="reference external" href="https://doi.org/10.1177/1059712310397633">https://doi.org/10.1177/1059712310397633</a></p></li>
</ul>
<section id="learning-rate-and-exploration">
<h3>Learning rate and exploration<a class="headerlink" href="#learning-rate-and-exploration" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Kakade S, Dayan P (2002) Dopamine: generalization and bonuses. Neural Networks 15:549-559. <a class="reference external" href="https://doi.org/10.1016/s0893-6080(02)00048-5">https://doi.org/10.1016/s0893-6080(02)00048-5</a></p></li>
</ul>
</section>
<section id="serotonin">
<h3>Serotonin:<a class="headerlink" href="#serotonin" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Miyazaki K, Miyazaki KW, Doya K (2011) Activation of dorsal raphe serotonin neurons underlies waiting for delayed rewards. The Journal of Neuroscience, 31:469-479. <a class="reference external" href="https://doi.org/10.1523/JNEUROSCI.3714-10.2011">https://doi.org/10.1523/JNEUROSCI.3714-10.2011</a></p></li>
<li><p>Miyazaki KW, Miyazaki K, Tanaka KF, Yamanaka A, Takahashi A, Tabuchi S, Doya K (2014) Optogenetic activation of dorsal raphe serotonin neurons enhances patience for future rewards. Current Biology, 24:2033-2040. <a class="reference external" href="https://doi.org/10.1016/j.cub.2014.07.041">https://doi.org/10.1016/j.cub.2014.07.041</a></p></li>
<li><p>Miyazaki K, Miyazaki KW, Yamanaka A, Tokuda T, Tanaka KF, Doya K (2018) Reward probability and timing uncertainty alter the effect of dorsal raphe serotonin neurons on patience. Nature Communications, 9:2048. <a class="reference external" href="https://doi.org/10.1038/s41467-018-04496-y">https://doi.org/10.1038/s41467-018-04496-y</a></p></li>
<li><p>Lottem E, Banerjee D, Vertechi P, Sarra D, Lohuis MO, Mainen ZF (2018). Activation of serotonin neurons promotes active persistence in a probabilistic foraging task. Nat Commun, 9, 1000. <a class="reference external" href="https://doi.org/10.1038/s41467-018-03438-y">https://doi.org/10.1038/s41467-018-03438-y</a></p></li>
<li><p>Grossman CD, Bari BA, Cohen JY (2021). Serotonin neurons modulate learning rate through uncertainty. Curr Biol, 10.1016/j.cub.2021.12.006. <a class="reference external" href="https://doi.org/10.1016/j.cub.2021.12.006">https://doi.org/10.1016/j.cub.2021.12.006</a></p></li>
<li><p>Doya K, Miyazaki KW, Miyazaki K (2021). Serotonergic modulation of cognitive computations. Current Opinion in Behavioral Sciences, 38, 116-123. <a class="reference external" href="https://doi.org/10.1016/j.cobeha.2021.02.003">https://doi.org/10.1016/j.cobeha.2021.02.003</a></p></li>
</ul>
</section>
<section id="acetylcholine-and-noradrenaline-norepinephrine">
<h3>Acetylcholine and noradrenaline (norepinephrine)<a class="headerlink" href="#acetylcholine-and-noradrenaline-norepinephrine" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Hasselmo ME, Sarter M (2011). Modes and models of forebrain cholinergic neuromodulation of cognition. Neuropsychopharmacology, 36, 52-73. <a class="reference external" href="http://doi.org/10.1038/npp.2010.104">http://doi.org/10.1038/npp.2010.104</a></p></li>
<li><p>Usher M, Cohen JD, Servan-Schreiber D, Rajkowski J, Aston-Jones G (1999). The role of Locus Coeruleus in the regulation of cognitive performance. Science, 283, 549-554. <a class="reference external" href="http://doi.org/10.1126/science.283.5401.549">http://doi.org/10.1126/science.283.5401.549</a></p></li>
<li><p>Aston-Jones G, Cohen JD (2005). An integrative theory of locus coeruleus-norepinephrine function: adaptive gain and optimal performance. Annual Reviews in Neuroscience, 28, 403-50. <a class="reference external" href="http://doi.org/10.1146/annurev.neuro.28.061604.135709">http://doi.org/10.1146/annurev.neuro.28.061604.135709</a></p></li>
<li><p>Yu AJ, Dayan P (2005). Uncertainty, neuromodulation, and attention. Neuron, 46, 681-92. <a class="reference external" href="http://doi.org/10.1016/j.neuron.2005.04.026">http://doi.org/10.1016/j.neuron.2005.04.026</a></p></li>
</ul>
</section>
<section id="representation-learning">
<h3>Representation learning<a class="headerlink" href="#representation-learning" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Courville AC, Daw ND, Touretzky DS (2006). Bayesian theories of conditioning in a changing world. Trends Cogn Sci, 10, 294-300. <a class="reference external" href="https://doi.org/10.1016/j.tics.2006.05.004">https://doi.org/10.1016/j.tics.2006.05.004</a></p></li>
<li><p>Bengio Y, Courville A, Vincent P (2013). Representation learning: a review and new perspectives. IEEE Trans Pattern Anal Mach Intell, 35, 1798-828. <a class="reference external" href="https://doi.org/10.1109/TPAMI.2013.50">https://doi.org/10.1109/TPAMI.2013.50</a></p></li>
<li><p>Yang GR, Joglekar MR, Song HF, Newsome WT, Wang XJ (2019). Task representations in neural networks trained to perform many cognitive tasks. Nat Neurosci. <a class="reference external" href="https://doi.org/10.1038/s41593-018-0310-2">https://doi.org/10.1038/s41593-018-0310-2</a></p></li>
</ul>
</section>
<section id="model-based-strategies">
<h3>Model-based strategies<a class="headerlink" href="#model-based-strategies" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Glascher J, Daw N, Dayan P, O’Doherty JP (2010). States versus rewards: dissociable neural prediction error signals underlying model-based and model-free reinforcement learning. Neuron, 66, 585-95. <a class="reference external" href="http://doi.org/10.1016/j.neuron.2010.04.016">http://doi.org/10.1016/j.neuron.2010.04.016</a></p></li>
<li><p>Daw ND, Gershman SJ, Seymour B, Dayan P, Dolan RJ (2011). Model-based influences on humans’ choices and striatal prediction errors. Neuron, 69, 1204-15. <a class="reference external" href="https://doi.org/10.1016/j.neuron.2011.02.027">https://doi.org/10.1016/j.neuron.2011.02.027</a></p></li>
</ul>
</section>
<section id="id1">
<h3>Modularity and compositionality<a class="headerlink" href="#id1" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Ghahramani Z, Wolpert DM (1997). Modular decomposition in visuomotor learning. Nature, 386, 392-395. <a class="reference external" href="http://doi.org/10.1038/386392a0">http://doi.org/10.1038/386392a0</a></p></li>
<li><p>Wolpert DM, Doya K, Kawato M (2003). A unifying computational framework for motor control and social interaction. Philos Trans R Soc Lond B Biol Sci, 358, 593-602. <a class="reference external" href="http://doi.org/10.1098/rstb.2002.1238">http://doi.org/10.1098/rstb.2002.1238</a></p></li>
<li><p>Todorov E (2009). Efficient computation of optimal actions. Proc Natl Acad Sci U S A, 106, 11478-83. <a class="reference external" href="https://doi.org/10.1073/pnas.0710743106">https://doi.org/10.1073/pnas.0710743106</a></p></li>
</ul>
</section>
<section id="id2">
<h3>Pathway gating<a class="headerlink" href="#id2" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Wang XJ, Yang GR (2018). A disinhibitory circuit motif and flexible information routing in the brain. Curr Opin Neurobiol, 49, 75-83. <a class="reference external" href="https://doi.org/10.1016/j.conb.2018.01.002">https://doi.org/10.1016/j.conb.2018.01.002</a></p></li>
<li><p>Fernando C, Banarse D, Blundell C, Zwols Y, Ha D, Rusu AA, Pritzel A, Wierstra D (2017). Pathnet: Evolution channels gradient descent in super neural networks. arXiv:1701.08734.
<a class="reference external" href="https://doi.org/10.48550/arXiv.1701.08734">https://doi.org/10.48550/arXiv.1701.08734</a></p></li>
</ul>
</section>
<section id="id3">
<h3>Meta-reinforcement learning<a class="headerlink" href="#id3" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Ito M, Doya K (2015). Parallel representation of value-based and finite state-based strategies in the ventral and dorsal striatum. PLoS Comput Biol, 11, e1004540. <a class="reference external" href="https://doi.org/10.1371/journal.pcbi.1004540">https://doi.org/10.1371/journal.pcbi.1004540</a></p></li>
<li><p>Wang JX, Kurth-Nelson Z, Kumaran D, Tirumala D, Soyer H, Leibo JZ, Hassabis D, Botvinick M (2018). Prefrontal cortex as a meta-reinforcement learning system. Nat Neurosci, 21, 860-868. <a class="reference external" href="https://doi.org/10.1038/s41593-018-0147-8">https://doi.org/10.1038/s41593-018-0147-8</a></p></li>
</ul>
</section>
</section>
<div class="toctree-wrapper compound">
</div>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="Multiple_Exercise.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Exercise:  Multiple Agents</p>
      </div>
    </a>
    <a class="right-next"
       href="Meta_Exercise.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Exercise: Meta-Learning</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#contents">Contents</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tuning-meta-parameters">Tuning meta-parameters</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-rate">Learning Rate</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exploration-exploitation">Exploration-Exploitation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#temporal-discount-factor">Temporal discount factor</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#self-tuning-of-meta-parameters">Self-tuning of  meta-parameters</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neuromodulators">Neuromodulators</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#serotonin-and-delayed-reward">Serotonin and delayed reward</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#acetylcholine-noradrenaline-and-learning">Acetylcholine, noradrenaline and learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sensory-representation-learning">Sensory representation learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#models-and-latent-variables">Models and latent variables</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#modularity-and-compositionality">Modularity and compositionality</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pathway-gating">Pathway gating</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#meta-reinforcement-learning">Meta-reinforcement learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-rate-and-exploration">Learning rate and exploration</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#serotonin">Serotonin:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#acetylcholine-and-noradrenaline-norepinephrine">Acetylcholine and noradrenaline (norepinephrine)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#representation-learning">Representation learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-based-strategies">Model-based strategies</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Modularity and compositionality</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Pathway gating</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Meta-reinforcement learning</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Kenji Doya
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>