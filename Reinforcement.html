

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Chapter 4. Reinforcement Learning &#8212; Brain Computation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.4cbf315f70debaebd550c87a6162cf0f.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/sphinx_highlight.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'Reinforcement';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Chapter 4. Reinforcement Learning: Exercise" href="Reinforcement_Exercise.html" />
    <link rel="prev" title="Supervised Learning: Exercise" href="Supervised_Exercise.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="BrainComputation.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/BC_logo.png" class="logo__image only-light" alt="Brain Computation - Home"/>
    <script>document.write(`<img src="_static/BC_logo.png" class="logo__image only-dark" alt="Brain Computation - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="BrainComputation.html">
                    Brain Computation: A Hands-on Guidebook
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="Introduction.html">Chapter 1. Introduction</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="Neurons.html">Chapter 2. Neural Modeling and Analysis</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="Neurons_Exercise.html">Chapter 2. Neural Modeling and Analysis: Exercise</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="Supervised.html">Chapter 3: Supervised Learning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="Supervised_Exercise.html">Supervised Learning: Exercise</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="current reference internal" href="#">Chapter 4. Reinforcement Learning</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="Reinforcement_Exercise.html">Chapter 4. Reinforcement Learning: Exercise</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="Unsupervised.html">Chapter 5. Unsupervised Learning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="Unsupervised_Exercise.html">Unsupervised Learning: Exercise</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="Bayesian.html">Chatper 6. Bayesian Approaches</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="Bayesian_Exercise.html">Bayesian Approaches: Exercise</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="Deep.html">Chapter 7: Deep Learning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="Deep_Exercise.html">Chapter 7: Deep Learning: Exercise</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="Multiple.html">Chapter 8. Multiple Agents</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="Multiple_Exercise.html">Chapter 8. Multiple Agents: Exercise</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="Meta.html">Chapter 9. Meta-Learning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="Meta_Exercise.html">Chapter 9. Meta-Learning: Exercise</a></li>
</ul>
</details></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/Reinforcement.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Chapter 4. Reinforcement Learning</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#contents">Contents</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bandit-problem">Bandit problem</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#markov-decision-process-mdp">Markov decision process (MDP)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#value-functions">Value Functions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#solving-mdps">Solving MDPs</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-based-approach-dynamic-programming">Model-based approach: Dynamic programming</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-free-approach-reinforcement-learning">Model-free approach: Reinforcement learning</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sarsa-and-q-learning">SARSA and Q Learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#classes-for-minimum-environment-and-agent">Classes for minimum environment and agent</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-pain-gain-environment">Example: Pain-Gain environment</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#actor-critic">Actor-Critic</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dopamine-neurons">Dopamine neurons</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#basal-ganglia">Basal ganglia</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Dopamine neurons</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Basal ganglia</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="chapter-4-reinforcement-learning">
<h1>Chapter 4. Reinforcement Learning<a class="headerlink" href="#chapter-4-reinforcement-learning" title="Permalink to this heading">#</a></h1>
<div class="math notranslate nohighlight">
\[ % Latex macros
\newcommand{\mat}[1]{\begin{pmatrix} #1 \end{pmatrix}}
\newcommand{\p}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\b}[1]{\boldsymbol{#1}}
\newcommand{\w}{\boldsymbol{w}}
\newcommand{\x}{\boldsymbol{x}}
\]</div>
<section id="contents">
<h2>Contents<a class="headerlink" href="#contents" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>Bandit problem</p></li>
<li><p>Markov decision process (MDP)</p></li>
<li><p>Value functions</p></li>
<li><p>Dynamic programming (DP)</p></li>
<li><p>Q-learning and SARSA</p></li>
<li><p>Actor-Critic</p></li>
<li><p>Applications</p></li>
<li><p>Neural Mechanisms</p>
<ul>
<li><p>Dopamine neurons</p></li>
<li><p>Basal ganglia</p></li>
</ul>
</li>
</ul>
<p>In reinforcement learning, an <em>agent</em> interacts with the <em>environment</em> by observing its state, sending an action, and receiving some reward.</p>
<blockquote>
<div><p><img alt="ReinforcementLearning" src="_images/RL.jpg" />
Reinforcement learning in an agent-environment loop.</p>
</div></blockquote>
<p>The aim of reinforcemnet learning is to find a <em>policy</em>, a mapping <span class="math notranslate nohighlight">\(\pi\)</span> from any state <span class="math notranslate nohighlight">\(s\)</span> to an action <span class="math notranslate nohighlight">\(a\)</span>, that maximize the reward acquired.</p>
<ul class="simple">
<li><p>a deterministic policy <span class="math notranslate nohighlight">\(a=\pi(s)\)</span></p></li>
<li><p>a stochastic policy <span class="math notranslate nohighlight">\(\pi(s,a)=p(a|s)\)</span></p></li>
</ul>
</section>
<section id="bandit-problem">
<h2>Bandit problem<a class="headerlink" href="#bandit-problem" title="Permalink to this heading">#</a></h2>
<p>A simple case is that the state does not change, or change irrespective of the agent’s action.
In this case, the problem is simply to estimate the expected reward for each action and pick the one with highest expected reward.</p>
<div class="math notranslate nohighlight">
\[R(s,a) = E[r|s,a]\]</div>
<div class="math notranslate nohighlight">
\[\pi(s) = \arg\max_a R(s,a)\]</div>
</section>
<section id="markov-decision-process-mdp">
<h2>Markov decision process (MDP)<a class="headerlink" href="#markov-decision-process-mdp" title="Permalink to this heading">#</a></h2>
<p>A general setup for reinforcement learning is the Markov decision process (MDP), which is characterized by</p>
<ul class="simple">
<li><p>finite state <span class="math notranslate nohighlight">\(s \in S\)</span></p></li>
<li><p>finite actions <span class="math notranslate nohighlight">\(a\in A(s)\)</span></p></li>
<li><p>state transition rule <span class="math notranslate nohighlight">\(P(s'|s,a)\)</span></p></li>
<li><p>reward function <span class="math notranslate nohighlight">\(R(s,a,s') = E[r|s,a,s']\)</span></p></li>
</ul>
<p>There are two conventions for the time index of reward. The reward following <span class="math notranslate nohighlight">\(a_t\)</span> is denoted as <span class="math notranslate nohighlight">\(r_t\)</span> or <span class="math notranslate nohighlight">\(r_{t+1}\)</span> depending on the literature. Here we take the convention by Sutton and Barto (2018):
$<span class="math notranslate nohighlight">\((s_t,a_t)\rightarrow (r_{t+1},s_{t+1})\)</span>$</p>
<p>In a dynamical environment, an action at time <span class="math notranslate nohighlight">\(t\)</span> may affect the reward at later times <span class="math notranslate nohighlight">\(r_\tau\)</span>, <span class="math notranslate nohighlight">\(\tau &gt; t\)</span> through the state dynamics.<br />
From the other viewpoint, a reward at time <span class="math notranslate nohighlight">\(t\)</span> may depend on all the past states and actions at <span class="math notranslate nohighlight">\(\tau&lt;t\)</span>.</p>
<p>Thus in selecting an action, an agent needs to consider maximizing expected cumulative reward, called <em>return</em></p>
<ul class="simple">
<li><p>finite horizon till time <span class="math notranslate nohighlight">\(T\)</span>: <span class="math notranslate nohighlight">\(R_t = r_{t+1} + r_{t+2} + ... + r_T\)</span></p></li>
<li><p>infinite horizon: <span class="math notranslate nohighlight">\(R_t = r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3} + ...\)</span><br />
where <span class="math notranslate nohighlight">\(0\le\gamma&lt;1\)</span> is a <em>discount factor</em> to guarantee the sum to be finite.</p></li>
</ul>
</section>
<section id="value-functions">
<h2>Value Functions<a class="headerlink" href="#value-functions" title="Permalink to this heading">#</a></h2>
<p>To evaluate the goodness of states, actions, and policy, a critical tool is the <em>value function</em>. There are two types.</p>
<ul class="simple">
<li><p>State value function: expected return starting from a state <span class="math notranslate nohighlight">\(s\)</span> by following a policy <span class="math notranslate nohighlight">\(\pi\)</span></p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}V^\pi(s) \equiv E_\pi[R_t|s_t=s]\\ = E_\pi[\sum_k^\infty \gamma^k r_{t+k+1}|s_t=s]\end{split}\]</div>
<ul class="simple">
<li><p>(State-)Action value function: expected return by taking an action <span class="math notranslate nohighlight">\(a\)</span> at a state <span class="math notranslate nohighlight">\(s\)</span>, and then following a policy <span class="math notranslate nohighlight">\(\pi\)</span></p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}Q^\pi(s,a) \equiv E_\pi[R_t|s_t=s,a_t=a]\\ = E_\pi[\sum_k^\infty \gamma^k r_{t+k+1}|s_t=s,a_t=a]\end{split}\]</div>
<p>An optimal policy <span class="math notranslate nohighlight">\(\pi^*\)</span> for and MDP is defined as a policy that satisfies</p>
<div class="math notranslate nohighlight">
\[V^{\pi^*}(s) \ge V^{\pi}(s)\]</div>
<p>for any state <span class="math notranslate nohighlight">\(s\in S\)</span> and any policy <span class="math notranslate nohighlight">\(\pi\)</span>.</p>
<p>The value functions for an optimal policy is called <em>optimal value function</em> and denoted as <span class="math notranslate nohighlight">\(V^*(s)=V^{\pi^*}(s)\)</span>.  An MDP can have multiple optimal policies, but the optimal value function is unique.</p>
<p>An optimal state value function and action value function are related by</p>
<div class="math notranslate nohighlight">
\[V^*(s) = \max_aQ^*(s,a)\]</div>
</section>
<section id="solving-mdps">
<h2>Solving MDPs<a class="headerlink" href="#solving-mdps" title="Permalink to this heading">#</a></h2>
<p>There are two main approaches in solving MDP problems</p>
<section id="model-based-approach-dynamic-programming">
<h3>Model-based approach: Dynamic programming<a class="headerlink" href="#model-based-approach-dynamic-programming" title="Permalink to this heading">#</a></h3>
<p>When the state transition function <span class="math notranslate nohighlight">\(p(s'|s,a)\)</span> and the reward function <span class="math notranslate nohighlight">\(r(s,a)\)</span> are known or learned:</p>
<ul class="simple">
<li><p>Solve the <em>Bellman equation</em> for the optimal value function:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}V^*(s) = \max_a E[ r(s,a) + \gamma V^*(s')]\\
= \max_a [r(s,a) + \sum_{s'}p(s'|s,a)\gamma V^*(s')] \end{split}\]</div>
<ul class="simple">
<li><p>Use the optimal policy</p></li>
</ul>
<div class="math notranslate nohighlight">
\[a = \pi^*(s) = \arg\max_a E[ r(s,a) + \gamma V^*(s')]\]</div>
<ul class="simple">
<li><p>Representative algorithms:</p>
<ul>
<li><p>Policy iteration</p></li>
<li><p>Value iteration</p></li>
</ul>
</li>
</ul>
</section>
<section id="model-free-approach-reinforcement-learning">
<h3>Model-free approach: Reinforcement learning<a class="headerlink" href="#model-free-approach-reinforcement-learning" title="Permalink to this heading">#</a></h3>
<p>Do not assume that <span class="math notranslate nohighlight">\(p(s'|s,a)\)</span> and <span class="math notranslate nohighlight">\(r(s,a)\)</span> are unknown</p>
<ul class="simple">
<li><p>Learn from the sequence of experience:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[{s,a,r,s,a,r,…}\]</div>
<ul class="simple">
<li><p>Representative algorithms:</p>
<ul>
<li><p>SARSA</p></li>
<li><p>Q-learning</p></li>
<li><p>Actor-Critic</p></li>
</ul>
</li>
</ul>
</section>
</section>
<section id="sarsa-and-q-learning">
<h2>SARSA and Q Learning<a class="headerlink" href="#sarsa-and-q-learning" title="Permalink to this heading">#</a></h2>
<ul>
<li><p>Estimate the action value function <span class="math notranslate nohighlight">\(Q(s,a)\)</span> using a table or ANN.</p></li>
<li><p>Select an action using the action value function</p>
<ul class="simple">
<li><p>greedy: <span class="math notranslate nohighlight">\(a = \arg\max_a Q(s,a)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy: random action with prob. <span class="math notranslate nohighlight">\(\epsilon\)</span> and greedy with prob. <span class="math notranslate nohighlight">\(1-\epsilon\)</span></p></li>
<li><p>Boltzman:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[p(a|s) = \frac{e^{\beta Q(s,a)}}{\sum_{b\in A}e^{\beta Q(s,b)}}\]</div>
</li>
<li><p>Check the inconsistency of action value function estimates by the temporal difference (TD) error:</p></li>
</ul>
<p>SARSA (on-policy):</p>
<div class="math notranslate nohighlight">
\[\delta_t = r_{t+1} + \gamma Q(s_{t+1},a_{t+1}) - Q(s_t,a_t)\]</div>
<p>Q learning (off-policy): assuming greedy policy from the next state</p>
<div class="math notranslate nohighlight">
\[\delta_t = r_{t+1} + \gamma \max_{a'\in A}Q(s_{t+1},a') - Q(s_t,a_t)\]</div>
<ul class="simple">
<li><p>Update the action value function of the previous state and action in proportion to the TD error</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\Delta Q(s_t,a_t) = \alpha \delta_t\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
</pre></div>
</div>
</div>
</div>
<section id="classes-for-minimum-environment-and-agent">
<h3>Classes for minimum environment and agent<a class="headerlink" href="#classes-for-minimum-environment-and-agent" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Environment</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Class for a reinforcement learning environment&quot;&quot;&quot;</span>
    
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">nstate</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">naction</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Create a new environment&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Ns</span> <span class="o">=</span> <span class="n">nstate</span>   <span class="c1"># number of states</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Na</span> <span class="o">=</span> <span class="n">naction</span>  <span class="c1"># number of actions</span>
        
    <span class="k">def</span> <span class="nf">start</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;start an episode&quot;&quot;&quot;</span>
        <span class="c1"># randomly pick a state</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Ns</span><span class="p">)</span>
        <span class="k">return</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;step by an action&quot;&quot;&quot;</span>
        <span class="c1"># random reward</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reward</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span>  <span class="c1"># between 0 and 1</span>
        <span class="c1"># random next state</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">state</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Ns</span><span class="p">)</span>
        <span class="k">return</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reward</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Agent</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Class for a reinforcement learning agent&quot;&quot;&quot;</span>
    
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">nstate</span><span class="p">,</span> <span class="n">naction</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Create a new agent&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Ns</span> <span class="o">=</span> <span class="n">nstate</span>   <span class="c1"># number of states</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Na</span> <span class="o">=</span> <span class="n">naction</span>  <span class="c1"># number of actions</span>
        
    <span class="k">def</span> <span class="nf">start</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;first action, without reward feedback&quot;&quot;&quot;</span>
        <span class="c1"># randomly pick an action</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Na</span><span class="p">)</span>
        <span class="k">return</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">action</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;learn by reward and take an action&quot;&quot;&quot;</span>
        <span class="c1"># do nothing for reward</span>
        <span class="c1"># randomly pick an action</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Na</span><span class="p">)</span>
        <span class="k">return</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">action</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Reinforcement non-learning</span>
<span class="c1"># Create the environment and the agent</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">Environment</span><span class="p">()</span>
<span class="n">agent</span> <span class="o">=</span> <span class="n">Agent</span><span class="p">(</span><span class="n">env</span><span class="o">.</span><span class="n">Ns</span><span class="p">,</span> <span class="n">env</span><span class="o">.</span><span class="n">Na</span><span class="p">)</span>
<span class="c1"># First contact</span>
<span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
<span class="n">action</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">start</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
<span class="nb">print</span><span class="p">([</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">])</span>
<span class="c1"># Repeat interactoin</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">reward</span><span class="p">,</span> <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
    <span class="n">action</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">reward</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">([</span><span class="n">t</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[1, 1]
[0, 0.4987740172479077, 2, 0]
[1, 0.24912813083435448, 2, 1]
[2, 0.9231305950378759, 0, 0]
[3, 0.7201437011326701, 1, 1]
[4, 0.06128398569215643, 0, 0]
[5, 0.651349548691004, 0, 1]
[6, 0.4316443150360937, 1, 1]
[7, 0.039074398781138164, 2, 1]
[8, 0.3902166613375193, 1, 0]
[9, 0.4309098676708911, 2, 1]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">RL</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Reinforcement learning by interacton of Environment and Agent&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">environment</span><span class="p">,</span> <span class="n">agent</span><span class="p">,</span> <span class="n">nstate</span><span class="p">,</span> <span class="n">naction</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Create the environment and the agent&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">env</span> <span class="o">=</span> <span class="n">environment</span><span class="p">(</span><span class="n">nstate</span><span class="p">,</span> <span class="n">naction</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">agent</span> <span class="o">=</span> <span class="n">agent</span><span class="p">(</span><span class="n">nstate</span><span class="p">,</span> <span class="n">naction</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">episode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tmax</span><span class="o">=</span><span class="mi">50</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;One episode&quot;&quot;&quot;</span>
        <span class="c1"># First contact</span>
        <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
        <span class="n">action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="o">.</span><span class="n">start</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="c1"># Table of t, r, s, a</span>
        <span class="n">Trsa</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">tmax</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
        <span class="n">Trsa</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span>
        <span class="c1"># Repeat interactoin</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">tmax</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
            <span class="n">reward</span><span class="p">,</span> <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
            <span class="n">action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">reward</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span>
            <span class="n">Trsa</span><span class="p">[</span><span class="n">t</span><span class="p">,:]</span> <span class="o">=</span> <span class="p">[</span><span class="n">t</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span>
        <span class="k">return</span><span class="p">(</span><span class="n">Trsa</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">nrun</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">tmax</span><span class="o">=</span><span class="mi">50</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Multiple runs of episodes&quot;&quot;&quot;</span>
        <span class="n">Return</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">nrun</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nrun</span><span class="p">):</span>
            <span class="n">r</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">episode</span><span class="p">(</span><span class="n">tmax</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># reward sequence</span>
            <span class="n">Return</span><span class="p">[</span><span class="n">n</span><span class="p">]</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">r</span><span class="p">)</span>
        <span class="k">return</span><span class="p">(</span><span class="n">Return</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="example-pain-gain-environment">
<h3>Example: Pain-Gain environment<a class="headerlink" href="#example-pain-gain-environment" title="Permalink to this heading">#</a></h3>
<blockquote>
<div><p><img alt="PainGain" src="_images/PainGain.jpg" />
The Pain-Gain environment.</p>
</div></blockquote>
<p>This is a Markov Decision Process that was designed for a functional MRI experiment (Tanaka et al. 2004).
The environment has four states and two possible actions.
By taking action 1, the state shift to the left and a positive reward is given, except at the leftmost state where it a big negative reward is given.
By action 2, the state shift to the right and a negative reward is given, but at the rightmost state, a big positive reward is obtained.
What is the right policy?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">PainGain</span><span class="p">(</span><span class="n">Environment</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Pain-Gain environment &quot;&quot;&quot;</span>
    
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">nstate</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">naction</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">gain</span><span class="o">=</span><span class="mi">6</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">nstate</span><span class="p">,</span> <span class="n">naction</span><span class="p">)</span>
        <span class="c1"># setup the reward function as an array</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">R</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">Ns</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Na</span><span class="p">))</span> <span class="c1"># small gains</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">R</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>   <span class="c1"># small pains for 2nd action (a=1)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">R</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="n">gain</span>  <span class="c1"># large pain</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">R</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">gain</span>  <span class="c1"># large gain</span>
    
    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;step by an action&quot;&quot;&quot;</span>
        <span class="c1"># reward by the reward matrix</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reward</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">R</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">]</span>
        <span class="c1"># move left or right and circle</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">state</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">state</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">action</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">%</span><span class="k">self</span>.Ns  
        <span class="k">return</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">reward</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">QL</span><span class="p">(</span><span class="n">Agent</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Class for a Q-learning agent&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">nstate</span><span class="p">,</span> <span class="n">naction</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">nstate</span><span class="p">,</span> <span class="n">naction</span><span class="p">)</span>
        <span class="c1"># allocate Q table</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nstate</span><span class="p">,</span> <span class="n">naction</span><span class="p">))</span>
        <span class="c1"># default parameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.1</span>  <span class="c1"># learning rate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="mf">1.0</span>   <span class="c1"># inverse temperature</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.9</span>  <span class="c1"># discount factor</span>
    
    <span class="k">def</span> <span class="nf">boltzmann</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">q</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Boltzmann selection&quot;&quot;&quot;</span>
        <span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span><span class="o">*</span><span class="n">q</span><span class="p">)</span>   <span class="c1"># unnormalized probability</span>
        <span class="n">p</span> <span class="o">=</span> <span class="n">p</span><span class="o">/</span><span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>    <span class="c1"># probability</span>
        <span class="c1"># sample by multinoulli (categorical) distribution</span>
        <span class="n">s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multinomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">p</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="n">s</span><span class="p">)</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># find the index of 1</span>
        <span class="c1">#return(np.searchsorted( np.cumsum(p), np.random.random()))</span>

    <span class="k">def</span> <span class="nf">start</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;first action, without reward feedback&quot;&quot;&quot;</span>
        <span class="c1"># Boltzmann action selection</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">boltzmann</span><span class="p">(</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">[</span><span class="n">state</span><span class="p">,:])</span>
        <span class="c1"># remember the state</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">state</span> <span class="o">=</span> <span class="n">state</span>
        <span class="k">return</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">action</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;learn by reward and take an action&quot;&quot;&quot;</span>
        <span class="c1"># TD error: self.state/action retains the previous ones</span>
        <span class="n">delta</span> <span class="o">=</span> <span class="n">reward</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span><span class="o">*</span><span class="nb">max</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">[</span><span class="n">state</span><span class="p">,:])</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">action</span><span class="p">]</span>
        <span class="c1"># Update the value for previous state and action</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">action</span><span class="p">]</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="o">*</span><span class="n">delta</span>
        <span class="c1"># Boltzmann action selection</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">action</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">boltzmann</span><span class="p">(</span> <span class="bp">self</span><span class="o">.</span><span class="n">Q</span><span class="p">[</span><span class="n">state</span><span class="p">,:])</span>
        <span class="c1"># remember the state</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">state</span> <span class="o">=</span> <span class="n">state</span>
        <span class="k">return</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">action</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Pain-Gain environment and Q-learning agent</span>
<span class="n">pgq</span> <span class="o">=</span> <span class="n">RL</span><span class="p">(</span><span class="n">PainGain</span><span class="p">,</span> <span class="n">QL</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="c1"># customize parameters</span>
<span class="n">pgq</span><span class="o">.</span><span class="n">agent</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="mi">1</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pgq</span><span class="o">.</span><span class="n">agent</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.9</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># run an episode</span>
<span class="n">trsa</span> <span class="o">=</span> <span class="n">pgq</span><span class="o">.</span><span class="n">episode</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">trsa</span><span class="p">[:,</span><span class="mi">1</span><span class="p">:]);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">((</span><span class="s2">&quot;reward&quot;</span><span class="p">,</span> <span class="s2">&quot;state&quot;</span><span class="p">,</span> <span class="s2">&quot;action&quot;</span><span class="p">));</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/eeaf88830ca571204575eae42698919f9ed26adbcd078699ee0f397b9ed50ba0.png" src="_images/eeaf88830ca571204575eae42698919f9ed26adbcd078699ee0f397b9ed50ba0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visualize Q function</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span><span class="o">-</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">pgq</span><span class="o">.</span><span class="n">agent</span><span class="o">.</span><span class="n">Q</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="mf">0.4</span> <span class="p">)</span>  <span class="c1"># action0</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span><span class="o">+</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">pgq</span><span class="o">.</span><span class="n">agent</span><span class="o">.</span><span class="n">Q</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="mf">0.4</span> <span class="p">);</span>  <span class="c1"># action1</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/ec323a8db768ffbfce4c8e421900f999f81abf26f04ea0d35a160e04df985c95.png" src="_images/ec323a8db768ffbfce4c8e421900f999f81abf26f04ea0d35a160e04df985c95.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Repeat episodes for learning curve</span>
<span class="n">pgq</span> <span class="o">=</span> <span class="n">RL</span><span class="p">(</span><span class="n">PainGain</span><span class="p">,</span> <span class="n">QL</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">R</span> <span class="o">=</span> <span class="n">pgq</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">R</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/a5700fef9bea1e7db3e2a64e18fbc7ba482fe3b1a4a7a856f1041fc5742c4bf7.png" src="_images/a5700fef9bea1e7db3e2a64e18fbc7ba482fe3b1a4a7a856f1041fc5742c4bf7.png" />
</div>
</div>
</section>
</section>
<section id="actor-critic">
<h2>Actor-Critic<a class="headerlink" href="#actor-critic" title="Permalink to this heading">#</a></h2>
<p>Another popular class of RL is <em>Actor-Critic</em>.</p>
<p>Actor implements a policy <span class="math notranslate nohighlight">\(\pi_\w=p(a|s;\w)\)</span>, where <span class="math notranslate nohighlight">\(\w\)</span> is the parameter, such as the elements of a table or the weights of an ANN.</p>
<p>Critic learns the state value function of the actor’s policy <span class="math notranslate nohighlight">\(\pi_\w\)</span>:</p>
<div class="math notranslate nohighlight">
\[V^{\pi_\w}(s) = E_{\pi_\w}[ r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3} + ...|s_t=s]\]</div>
<p>using a table or an ANN.</p>
<p>Learning is based on the temporal difference (TD) error:</p>
<div class="math notranslate nohighlight">
\[\delta_t = r_{t+1} + \gamma V(s_{t+1}) - V(s_t)\]</div>
<p>The TD error is used for learning of both critic and actor, but in different ways:</p>
<p>Critic:</p>
<div class="math notranslate nohighlight">
\[\Delta V(s_t) = \alpha \delta_t\]</div>
<p>Actor:</p>
<div class="math notranslate nohighlight">
\[\Delta \w = \alpha_w \delta_t \p{p(a_t|s_t;\w)}{\w}\]</div>
<p>or</p>
<div class="math notranslate nohighlight">
\[\Delta \w = \alpha_w \delta_t \p{\log p(a_t|s_t;\w)}{\w}\]</div>
</section>
<section id="dopamine-neurons">
<h2>Dopamine neurons<a class="headerlink" href="#dopamine-neurons" title="Permalink to this heading">#</a></h2>
<p>Dopamine neurons in the midbrain appear to code TD error (Schultz et al. 1997).</p>
</section>
<section id="basal-ganglia">
<h2>Basal ganglia<a class="headerlink" href="#basal-ganglia" title="Permalink to this heading">#</a></h2>
<blockquote>
<div><p><img alt="BGforRL" src="_images/BGRL.jpg" />
A reinforcement learning model of the basal ganglia (Doya 2007).</p>
</div></blockquote>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>Sutton RS, Barto AG (2018). Reinforcement Learning: An Introduction, 2nd edition. MIT Press.<br />
(<a class="reference external" href="http://incompleteideas.net/book/the-book-2nd.html">http://incompleteideas.net/book/the-book-2nd.html</a>)</p></li>
<li><p>Barto AG, Sutton RS, Andersen CW (1983). Neuronlike adaptive elements that can solve difficult learning control problems. IEEE Transactions on Systems, Man, and Cybernetics, 13, 834-846.
<a class="reference external" href="http://doi.org/10.1109/TSMC.1983.6313077">http://doi.org/10.1109/TSMC.1983.6313077</a></p></li>
<li><p>Doya K (2007). Reinforcement learning: Computational theory and biological mechanisms. HFSP J, 1, 30-40.
<a class="reference external" href="http://doi.org/10.2976/1.2732246/10.2976/1">http://doi.org/10.2976/1.2732246/10.2976/1</a></p></li>
</ul>
<section id="id1">
<h3>Dopamine neurons<a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Schultz W, Dayan P, Montague PR (1997). A Neural Substrate of Prediction and Reward. Science, 275, 1593-1599.
<a class="reference external" href="http://doi.org/10.1126/science.275.5306.1593">http://doi.org/10.1126/science.275.5306.1593</a></p></li>
<li><p>Nomoto K, Schultz W, Watanabe T, Sakagami M (2010). Temporally extended dopamine responses to perceptually demanding reward-predictive stimuli. J Neurosci, 30, 10692-702.
<a class="reference external" href="http://doi.org/10.1523/JNEUROSCI.4828-09.2010">http://doi.org/10.1523/JNEUROSCI.4828-09.2010</a></p></li>
</ul>
</section>
<section id="id2">
<h3>Basal ganglia<a class="headerlink" href="#id2" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Samejima K, Ueda Y, Doya K, Kimura M (2005). Representation of action-specific reward values in the striatum. Science, 310, 1337-40.
<a class="reference external" href="http://doi.org/10.1126/science.1115270">http://doi.org/10.1126/science.1115270</a></p></li>
<li><p>Ito M, Doya K (2015). Distinct neural representation in the dorsolateral, dorsomedial, and ventral parts of the striatum during fixed- and free-choice tasks. Journal of Neuroscience, 35, 3499-3514. <a class="reference external" href="http://doi.org/10.1523/JNEUROSCI.1962-14.2015">http://doi.org/10.1523/JNEUROSCI.1962-14.2015</a></p></li>
</ul>
</section>
</section>
<div class="toctree-wrapper compound">
</div>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="Supervised_Exercise.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Supervised Learning: Exercise</p>
      </div>
    </a>
    <a class="right-next"
       href="Reinforcement_Exercise.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Chapter 4. Reinforcement Learning: Exercise</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#contents">Contents</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bandit-problem">Bandit problem</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#markov-decision-process-mdp">Markov decision process (MDP)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#value-functions">Value Functions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#solving-mdps">Solving MDPs</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-based-approach-dynamic-programming">Model-based approach: Dynamic programming</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-free-approach-reinforcement-learning">Model-free approach: Reinforcement learning</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sarsa-and-q-learning">SARSA and Q Learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#classes-for-minimum-environment-and-agent">Classes for minimum environment and agent</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-pain-gain-environment">Example: Pain-Gain environment</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#actor-critic">Actor-Critic</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dopamine-neurons">Dopamine neurons</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#basal-ganglia">Basal ganglia</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Dopamine neurons</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Basal ganglia</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Kenji Doya
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>