
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Chapter 3: Supervised Learning &#8212; Brain Computation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'Supervised';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Exercise: Supervised Learning" href="Supervised_Exercise.html" />
    <link rel="prev" title="Exercise: Neural Modeling and Analysis" href="Neurons_Exercise.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="BrainComputation.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/BC_logo.png" class="logo__image only-light" alt="Brain Computation - Home"/>
    <script>document.write(`<img src="_static/BC_logo.png" class="logo__image only-dark" alt="Brain Computation - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="BrainComputation.html">
                    Brain Computation: A Hands-on Guidebook
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="Introduction.html">Chapter 1. Introduction</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="Neurons.html">Chapter 2. Neural Modeling and Analysis</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="Neurons_Exercise.html">Exercise: Neural Modeling and Analysis</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="current reference internal" href="#">Chapter 3: Supervised Learning</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="Supervised_Exercise.html">Exercise: Supervised Learning</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="Reinforcement.html">Chapter 4. Reinforcement Learning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="Gymnasium.html">Appendix: Gymnasium</a></li>
<li class="toctree-l2"><a class="reference internal" href="Reinforcement_Exercise.html">Exercise: Reinforcement Learning</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="Unsupervised.html">Chapter 5. Unsupervised Learning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="Unsupervised_Exercise.html">Exercise: Unsupervised Learning</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="Bayesian.html">Chatper 6. Bayesian Approaches</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="PyStan.html">Appendix: Bayesian Model Fitting by PyStan</a></li>
<li class="toctree-l2"><a class="reference internal" href="Bayesian_Exercise.html">Exercise: Bayesian Approaches</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="Deep.html">Chapter 7: Deep Learning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="Deep_Exercise.html">Exercise: Deep Learning</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="Multiple.html">Chapter 8. Multiple Agents</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="Multiple_Exercise.html">Exercise:  Multiple Agents</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="Meta.html">Chapter 9. Meta-Learning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="Meta_Exercise.html">Exercise: Meta-Learning</a></li>
</ul>
</details></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://mybinder.org/v2/gh/oist/BrainComputation/master?urlpath=tree/Supervised.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Binder"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Binder logo" src="_static/images/logo_binder.svg">
  </span>
<span class="btn__text-container">Binder</span>
</a>
</li>
      
      
      
      
      <li><a href="https://colab.research.google.com/github/oist/BrainComputation/blob/master/Supervised.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Colab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Colab logo" src="_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>



<a href="https://github.com/oist/BrainComputation" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/Supervised.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Chapter 3: Supervised Learning</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#contents">Contents</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#function-approximation">3.1 Function Approximation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regression">Linear Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#least-square-solution">Least square solution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-likelihood-estimate">Maximum likelihood estimate</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-arm-dynamics">Example: Arm dynamics</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#online-learning">Online Learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#basis-functions">Basis Functions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#radial-basis-functions">Radial Basis Functions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pattern-recognition">3.2 Pattern Recognition</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-perceptron">The Perceptron</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#perceptron-learning-rule">Perceptron Learning Rule</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-perceptron-in-2d-feature-space">Example: Perceptron in 2D feature space</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#support-vector-machine">Support Vector Machine</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#online-svm">Online SVM</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression">Logistic Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-likelihood-by-iterative-least-squares">Maximum likelihood by iterative least squares</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#online-logistic-regression">Online logistic regression</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#classical-conditioning">3.3 Classical Conditioning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#eye-blink-conditioning">Eye Blink Conditioning</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-cerebellum">3.4 The Cerebellum</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cerebellar-perceptron-hyopthesis">Cerebellar Perceptron Hyopthesis</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cerebellar-internal-model-hyopthesis">Cerebellar Internal Model Hyopthesis</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#perceptron">Perceptron</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Classical conditioning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cerebellar-perceptron-hypothesis">Cerebellar perceptron hypothesis</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cerebellar-internal-model-hypothesis">Cerebellar internal model hypothesis</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="chapter-3-supervised-learning">
<h1>Chapter 3: Supervised Learning<a class="headerlink" href="#chapter-3-supervised-learning" title="Link to this heading">#</a></h1>
<div class="math notranslate nohighlight">
\[ % Latex macros
\newcommand{\mat}[1]{\begin{pmatrix} #1 \end{pmatrix}}
\newcommand{\p}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\b}[1]{\boldsymbol{#1}}
\newcommand{\w}{\boldsymbol{w}}
\newcommand{\x}{\boldsymbol{x}}
\newcommand{\y}{\boldsymbol{y}}
\]</div>
<section id="contents">
<h2>Contents<a class="headerlink" href="#contents" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>3.1 Function Approximation (Bishop 2006, Chapter 3)</p>
<ul>
<li><p>Linear regression</p></li>
<li><p>Least squares – Maximum likelihood</p></li>
<li><p>Online learning – Stochastic gradient descent</p></li>
</ul>
</li>
<li><p>3.2 Pattern Recognition (Bishop 2006, Chapter 4)</p>
<ul>
<li><p>Perceptron</p></li>
<li><p>Logistic regression</p></li>
<li><p>Support vector machine</p></li>
</ul>
</li>
<li><p>3.3 Classical Conditioning</p></li>
<li><p>3.4 Cerebellum</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">from</span><span class="w"> </span><span class="nn">mpl_toolkits.mplot3d</span><span class="w"> </span><span class="kn">import</span> <span class="n">Axes3D</span>
</pre></div>
</div>
</div>
</div>
<p>The aim of <em>supervised learning</em> is to construct an input-output mapping</p>
<div class="math notranslate nohighlight">
\[ \y = f(\x) \]</div>
<p>from pairs of samples <span class="math notranslate nohighlight">\((\x_1,\y_1),...,(\x_N,\y_N)\)</span>.</p>
<p>When <span class="math notranslate nohighlight">\(\y\)</span> is continuous, it is called <em>function approximation</em> or <em>regression</em>.</p>
<p>When <span class="math notranslate nohighlight">\(\y\)</span> is discrete, it is called <em>pattern recognition</em> or <em>classification</em>.</p>
<p>Here we focus on the case where the output is one dimension and computed from weighted sum of the inputs <span class="math notranslate nohighlight">\(x_1,...,x_D\)</span></p>
<div class="math notranslate nohighlight">
\[
    y = f(\x;\w) = g( w_0 + w_1 x_1 + ... + w_D x_D).
\]</div>
<p>This is considered as an artificial neuron unit with input synaptic weights <span class="math notranslate nohighlight">\(w_1,...,w_D\)</span>, bias <span class="math notranslate nohighlight">\(w_0\)</span> (or threshold <span class="math notranslate nohighlight">\(-w_0\)</span>), and the output function <span class="math notranslate nohighlight">\(g(\ )\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Illustration of a neuron unit</span>
<span class="n">D</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="sa">r</span><span class="s2">&quot;$x_1$&quot;</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;$x_2$&quot;</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;$\vdots$ &quot;</span><span class="p">,</span><span class="sa">r</span><span class="s2">&quot;$x_D$&quot;</span><span class="p">]</span>
<span class="n">w</span> <span class="o">=</span> <span class="p">[</span><span class="sa">r</span><span class="s2">&quot;$w_1$&quot;</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;$w_2$&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">,</span><span class="sa">r</span><span class="s2">&quot;$w_D$&quot;</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="o">-</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">1.5</span><span class="o">-</span><span class="n">i</span><span class="p">,</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>  <span class="c1"># inputs</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.5</span><span class="o">-</span><span class="n">i</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;k&quot;</span><span class="p">)</span>  <span class="c1"># inputs</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="p">(</span><span class="mf">1.5</span><span class="o">-</span><span class="n">i</span><span class="p">)</span><span class="o">*</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">w</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>  <span class="c1"># weights</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&quot;k&quot;</span><span class="p">)</span>   <span class="c1"># output</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;ko&quot;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">markerfacecolor</span><span class="o">=</span><span class="s2">&quot;w&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">1.6</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;$y$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>  <span class="c1"># output</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;off&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/7b57b4543f1b8e83118491dca3f0a653fc1a1076b148a533402d5eebaba8bfc4.png" src="_images/7b57b4543f1b8e83118491dca3f0a653fc1a1076b148a533402d5eebaba8bfc4.png" />
</div>
</div>
</section>
<section id="function-approximation">
<h2>3.1 Function Approximation<a class="headerlink" href="#function-approximation" title="Link to this heading">#</a></h2>
<p>A typical case of function approximation in the brain is learning motor-sensory mapping; how does your arm respond to the activation of the muscles. In this case <span class="math notranslate nohighlight">\(\x\)</span> is the activation pattern of the arm muscles and <span class="math notranslate nohighlight">\(\y\)</span> is the changes in the arm joint angles, for example. Such an <em>internal model</em> of the body dynamics is very much helpful in motor control. In this case, the supervisor is your musculoskeletal system and sensory neurons.</p>
</section>
<section id="linear-regression">
<h2>Linear Regression<a class="headerlink" href="#linear-regression" title="Link to this heading">#</a></h2>
<p>In the simplest case, the output function is identity <span class="math notranslate nohighlight">\(g(u)=u\)</span>. This is the case of <em>linear regression</em>.</p>
<p>For <span class="math notranslate nohighlight">\(D\)</span>-dimensional input</p>
<div class="math notranslate nohighlight">
\[\x = (x_1,...,x_D)^T,\]</div>
<p>we take a weight vector</p>
<div class="math notranslate nohighlight">
\[\w = (w_0,w_1,...,w_D)^T\]</div>
<p>and give a scalar output</p>
<div class="math notranslate nohighlight">
\[
y = f(\x;\w) = w_0 + w_1 x_1 + ... + w_D x_D.
\]</div>
<p>By considering a constant input <span class="math notranslate nohighlight">\(x_0=1\)</span> and redefining <span class="math notranslate nohighlight">\(\x\)</span> as</p>
<div class="math notranslate nohighlight">
\[\x = (1,x_1,...,x_D)^T,\]</div>
<p>we have a simple vector notation</p>
<div class="math notranslate nohighlight">
\[
y = f(\x;\w) = \w^T \x = \x^T \w.
\]</div>
<p>We represent a set of input data as a matrix</p>
<div class="math notranslate nohighlight">
\[\begin{split}X = \pmatrix{\x_1^T \\ \vdots \\ \x_N^T}
    = \pmatrix{1 &amp; x_{11} &amp; \cdots &amp; x_{1D}\\
    \vdots &amp;\vdots &amp; &amp; \vdots\\
    1 &amp; x_{N1} &amp; \cdots &amp; x_{ND}} 
\end{split}\]</div>
<p>and the set of outputs is given in a vector form as</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\y = \pmatrix{y_1\\ \vdots \\y_N} = X \w.
\end{split}\]</div>
<p>When a set of target outputs</p>
<div class="math notranslate nohighlight">
\[\begin{split} \y^* = \pmatrix{y^*_1\\ \vdots \\y^*_N}\end{split}\]</div>
<p>is given, how can we find the appropriate weight vector <span class="math notranslate nohighlight">\(\w\)</span> so that <span class="math notranslate nohighlight">\(\y\)</span> becomes close to <span class="math notranslate nohighlight">\(\y^*\)</span>?</p>
<section id="least-square-solution">
<h3>Least square solution<a class="headerlink" href="#least-square-solution" title="Link to this heading">#</a></h3>
<p>A basic solution is to minimize the squared error between the target output and the model output</p>
<div class="math notranslate nohighlight">
\[ E(\w) = \frac{1}{2}\sum_{n=1}^N (\w^T \x_n - y^*_n)^2 
 = \frac{1}{2} ||X\w - \y^*||^2. \]</div>
<p>To minimize this, we differentiate this error function with each weight parameter and set it equal to zero:</p>
<div class="math notranslate nohighlight">
\[
    \p{E(\w)}{w_i} = \sum_{n=1}^N (\w^T \x_n - y^*_n)x_{ni} = 0.
\]</div>
<p>These are represented in a vector form as</p>
<div class="math notranslate nohighlight">
\[
    \p{E(\w)}{\w} = X^T(X\w - \y^*) = (X^T X)\w - X^T\y^* = \b{0}.
\]</div>
<p>Thus the solution to this minimization problem is given by</p>
<div class="math notranslate nohighlight">
\[
    \b{\hat{w}} = (X^T X)^{-1} X^T \y^*.
\]</div>
</section>
<section id="maximum-likelihood-estimate">
<h3>Maximum likelihood estimate<a class="headerlink" href="#maximum-likelihood-estimate" title="Link to this heading">#</a></h3>
<p>A statistician would assume that the target output is generated by a linear model with additional noise</p>
<div class="math notranslate nohighlight">
\[
    y^*_n = \w^T\x_n + \epsilon_n
\]</div>
<p>where <span class="math notranslate nohighlight">\(\epsilon_n\)</span> is assumed to follow a Gaussian distribution <span class="math notranslate nohighlight">\(\mathcal{N}(0,\sigma^2)\)</span>.
This is also re-written as</p>
<div class="math notranslate nohighlight">
\[
    y^*_n \sim \mathcal{N}(\w^T\x_n,\sigma^2)
\]</div>
<blockquote>
<div><p>The Gaussian distribution is defined as:</p>
<div class="math notranslate nohighlight">
\[
    p(x;\mu,\sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}
\]</div>
</div></blockquote>
<p>In this setup, the standard way of selecting the parameter is to find the one with the <em>maximum likelihood</em>, i.e. the probability of producing the data.</p>
<p>The likelihood of weights <span class="math notranslate nohighlight">\(\w\)</span> for the set of observed data is the product of the likelihood for each data:</p>
<div class="math notranslate nohighlight">
\[
    L(\w) = p(\y^*|X,\w,\sigma^2)
    = \prod_{n=1}^N (2\pi\sigma^2)^{-\frac{1}{2}}
    e^{-\frac{(y^*_n-\w^T\x_n)^2}{2\sigma^2}}.
\]</div>
<p>In maximizing the likelihood, it is mathematically and computationally more convenient to take its logarithm</p>
<div class="math notranslate nohighlight">
\[
    l(\w) = \log p(\y^*|X,\w,\sigma^2)
    = \sum_{n=1}^N -\frac{1}{2}\log(2\pi\sigma^2)
    - \frac{(y^*_n-\w^T\x_n)^2}{2\sigma^2}
\]</div>
<div class="math notranslate nohighlight">
\[
    = - \frac{N}{2}\log(2\pi) - N\log\sigma  - \frac{1}{\sigma^2} E(\w).
\]</div>
<p>In the above, only the last term depends on the weights <span class="math notranslate nohighlight">\(\w\)</span>.
Thus the maximizing the likelihood is equivalent to minimizing the sum of squared errors <span class="math notranslate nohighlight">\(E(\w)\)</span>.</p>
<p>This link between minimal error and maximum likelihood is helpful when we consider regularization of parameters and Bayesian perspective in Chapter 6.</p>
</section>
<section id="example-arm-dynamics">
<h3>Example: Arm dynamics<a class="headerlink" href="#example-arm-dynamics" title="Link to this heading">#</a></h3>
<p>Let us simulate simple second-order dynamics of an arm hanging down and take the data.
Then we will make a linear regression model to predict the angular acceleration from the angle and angular velocity.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">scipy.integrate</span><span class="w"> </span><span class="kn">import</span> <span class="n">odeint</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Illustration of an arm hanging down</span>
<span class="n">l</span> <span class="o">=</span> <span class="mf">0.3</span>  <span class="c1"># arm length: m</span>
<span class="n">th</span> <span class="o">=</span> <span class="mf">0.3</span> <span class="c1"># angle: rad</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="n">l</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">th</span><span class="p">)],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="n">l</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">th</span><span class="p">)],</span> <span class="n">lw</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="n">l</span><span class="p">],</span> <span class="s2">&quot;k-&quot;</span><span class="p">)</span>  <span class="c1"># downward line</span>
<span class="n">tex</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.02</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="o">*</span><span class="n">l</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;$\theta$&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;equal&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/5533a6d9daadd478f7ac2cd799a7e38c3161f48be2d4f45485ce7ebee91b3ea1.png" src="_images/5533a6d9daadd478f7ac2cd799a7e38c3161f48be2d4f45485ce7ebee91b3ea1.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Dynamics of the arm</span>
<span class="n">m</span> <span class="o">=</span> <span class="mi">5</span>     <span class="c1"># arm mass: kg</span>
<span class="n">mu</span> <span class="o">=</span> <span class="mf">0.1</span>  <span class="c1"># damping: Nm/(rad/s)</span>
<span class="n">g</span> <span class="o">=</span> <span class="mf">9.8</span>   <span class="c1"># gravity: N/s^2</span>
<span class="n">I</span> <span class="o">=</span> <span class="p">(</span><span class="n">m</span><span class="o">*</span><span class="n">l</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="mi">3</span> <span class="c1"># inertia for a rod around an end: kg m^2</span>
<span class="k">def</span><span class="w"> </span><span class="nf">arm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;arm dynamics for odeint: x=[th,om]&quot;&quot;&quot;</span>
    <span class="n">th</span><span class="p">,</span> <span class="n">om</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># for readability</span>
    <span class="c1"># angular acceleration</span>
    <span class="n">aa</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="n">m</span><span class="o">*</span><span class="n">g</span><span class="o">*</span><span class="n">l</span><span class="o">/</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">th</span><span class="p">)</span> <span class="o">-</span> <span class="n">mu</span><span class="o">*</span><span class="n">om</span><span class="p">)</span><span class="o">/</span><span class="n">I</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">om</span><span class="p">,</span> <span class="n">aa</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Simulate for 10 sec.</span>
<span class="n">dt</span> <span class="o">=</span> <span class="mf">0.1</span>  <span class="c1"># time step</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">dt</span><span class="p">)</span>  <span class="c1"># time points</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">odeint</span><span class="p">(</span><span class="n">arm</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">t</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;time&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">((</span><span class="s2">&quot;theta&quot;</span><span class="p">,</span><span class="s2">&quot;omega&quot;</span><span class="p">));</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/15a0f7ec9e4851d508515b852dc72d4d70de80131091326cf88c1373c69a0616.png" src="_images/15a0f7ec9e4851d508515b852dc72d4d70de80131091326cf88c1373c69a0616.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Acceleration by differentiation</span>
<span class="n">Y</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="mi">1</span><span class="p">:,</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">X</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">])</span><span class="o">/</span><span class="n">dt</span>  <span class="c1"># temporal difference</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">,:]</span>    <span class="c1"># omit the last point</span>
<span class="n">N</span><span class="p">,</span> <span class="n">D</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>  <span class="c1"># data count and dimension</span>
<span class="c1"># add observation noise</span>
<span class="n">sig</span> <span class="o">=</span> <span class="mf">1.0</span>   <span class="c1"># noise size</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">Y</span> <span class="o">+</span> <span class="n">sig</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># You may want to use interactive mode for rotating 3D plots</span>
<span class="c1"># for Jupyter Lab or Notebook version&gt;=7 with ipympl</span>
<span class="c1">#%matplotlib widget</span>
<span class="c1"># for Jupyter Notebook before version&lt;7</span>
<span class="c1">#%matplotlib notebook</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>
<span class="c1"># Scatter plot in 3D</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">Y</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">Y</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;theta&quot;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;omega&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/daf69d2d01f6a32020c8d533121262541eb01ef47e9a251522b1324ab646ab16.png" src="_images/daf69d2d01f6a32020c8d533121262541eb01ef47e9a251522b1324ab646ab16.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Prepare data matrix</span>
<span class="n">X1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">N</span><span class="p">),</span> <span class="n">X</span><span class="p">]</span> <span class="c1">#  add a column of 1&#39;s</span>
<span class="c1"># Compute the weights: W = (X^T X)^(-1) X^T Y</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">X1</span><span class="o">.</span><span class="n">T</span><span class="nd">@X1</span><span class="p">)</span> <span class="o">@</span> <span class="n">X1</span><span class="o">.</span><span class="n">T</span><span class="nd">@Y</span>
<span class="c1">#w = np.linalg.solve(X1.T@X1, X1.T@Y)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;w =&quot;</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
<span class="nb">print</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="n">m</span><span class="o">*</span><span class="n">g</span><span class="o">*</span><span class="n">l</span><span class="o">/</span><span class="mi">2</span><span class="o">/</span><span class="n">I</span><span class="p">,</span> <span class="o">-</span><span class="n">mu</span><span class="o">/</span><span class="n">I</span><span class="p">])</span>  <span class="c1"># analytic values</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>w = [-1.14416797e-02 -3.87376810e+01 -2.82612361e+00]
[0, -49.0, -0.6666666666666667]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Squared error</span>
<span class="n">mse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">X1</span><span class="nd">@w</span> <span class="o">-</span> <span class="n">Y</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="n">N</span>  <span class="c1"># mean squared error</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;mse =&quot;</span><span class="p">,</span> <span class="n">mse</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>mse = 1.2546640217689426
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Show regression surface</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>
<span class="c1">#  make a grid</span>
<span class="n">X2</span><span class="p">,</span> <span class="n">Y2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span><span class="mf">0.1</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="c1"># show wireframe</span>
<span class="n">Z2</span> <span class="o">=</span> <span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">X2</span> <span class="o">+</span> <span class="n">w</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">*</span><span class="n">Y2</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot_wireframe</span><span class="p">(</span><span class="n">X2</span><span class="p">,</span> <span class="n">Y2</span><span class="p">,</span> <span class="n">Z2</span><span class="p">);</span>
<span class="c1"># show the data</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">Y</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">Y</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;theta&quot;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;omega&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/e4d44e2af9b4b92042f4b1711829dc00f695ef3ab0cc769d7db9d8d791a60542.png" src="_images/e4d44e2af9b4b92042f4b1711829dc00f695ef3ab0cc769d7db9d8d791a60542.png" />
</div>
</div>
</section>
</section>
<section id="online-learning">
<h2>Online Learning<a class="headerlink" href="#online-learning" title="Link to this heading">#</a></h2>
<p>In the above, we assumed that all the data are available at once. However, data come in sequence and we would rather learn from the data as they come in.</p>
<p>The basic way of online learning is to minimize the output error for each input</p>
<div class="math notranslate nohighlight">
\[
    E_n(\w) = \frac{1}{2}(y^*_n - \w^T \x_n)^2
\]</div>
<p>and move <span class="math notranslate nohighlight">\(\w\)</span> down to its gradient</p>
<div class="math notranslate nohighlight">
\[
    \Delta \w = - \alpha \p{E_n(\w)}{\w} = \alpha(y^*_n - \w^T \x_n)\x_n 
\]</div>
<p>where <span class="math notranslate nohighlight">\(\alpha&gt;0\)</span> is a learning rate parameter.</p>
<p>This is called <em>stochastic gradient descent (SGD)</em>, by assuming that <span class="math notranslate nohighlight">\(x_n\)</span> are stochastic samples from the input data space.</p>
<p>Online learning has several advantages over <em>batch</em> algorithms like linear regression that processes all the data at once.
Online learning does not require matrix inversion, which is computationally expensive when dealing with high dimensional data (large <span class="math notranslate nohighlight">\(D\)</span>).
For a very large dataset (large <span class="math notranslate nohighlight">\(N\)</span>), simply storing a huge <span class="math notranslate nohighlight">\(N\times D\)</span> data matrix <span class="math notranslate nohighlight">\(X\)</span> in the memory can be costly.</p>
</section>
<section id="basis-functions">
<h2>Basis Functions<a class="headerlink" href="#basis-functions" title="Link to this heading">#</a></h2>
<p>For approximating a nonlinear function of <span class="math notranslate nohighlight">\(\x\)</span>, a standard way is to prepare a set of nonlinear functions</p>
<div class="math notranslate nohighlight">
\[
    \b{\phi}(\x) = (\phi_1(\x),...,\phi_M(\x))^T,
\]</div>
<p>called basis functions, and represent the target function by</p>
<div class="math notranslate nohighlight">
\[
    f(\x;\w) = \sum_{j=1}^M w_j \phi_j(\x) = \w^T\b{\phi}(\x).
\]</div>
<p>Classically, polynomials <span class="math notranslate nohighlight">\((1, x, x^2, x^3,...)\)</span> or sinusoids <span class="math notranslate nohighlight">\((1, \sin x, \cos x, \sin 2x, \cos 2x,...)\)</span> were often used as basis functions, motivated by polynomial expansion or Fourier expansion.</p>
<p>However, these functions can grow so large or become so steep when we include higher order terms, which can make learned function changing wildly.</p>
<section id="radial-basis-functions">
<h3>Radial Basis Functions<a class="headerlink" href="#radial-basis-functions" title="Link to this heading">#</a></h3>
<p>In sensory nervous systems, neurons tend to respond to signals in a limited range, called <em>receptive field</em>. For example, each visual neuron responds to light stimuli on a small area in the retina. Each auditory neurons respond to a limited range of frequency.</p>
<p>Partly inspired by such local receptive field properties, a popular class of basis functions is called <em>radial basis function (RBF)</em>.
An RBF is give by a decreasing function of the distance from a center point:</p>
<div class="math notranslate nohighlight">
\[
    \phi_j(\x) = g(||\x-\x_j||)
\]</div>
<p>A typical example is Gaussian basis functions.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Gaussian basis functions in 1D</span>
<span class="k">def</span><span class="w"> </span><span class="nf">phi</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Gaussian centered at x=c with scale s&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">((</span><span class="n">x</span> <span class="o">-</span> <span class="n">c</span><span class="p">)</span><span class="o">/</span><span class="n">s</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
<span class="n">M</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># x range</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">M</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">phi</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;$</span><span class="se">\\</span><span class="s2">phi(x)$&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/74ce8176d7d6fd31a6a38aeb53c53e910f43afb859ccc66221e152022238e7af.png" src="_images/74ce8176d7d6fd31a6a38aeb53c53e910f43afb859ccc66221e152022238e7af.png" />
</div>
</div>
<p>Here you can generate random samples of functions using RBF.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">M</span><span class="p">)</span>  <span class="c1"># random weights</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">*</span><span class="mi">0</span>  <span class="c1"># output to cover the same range as input</span>
<span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">M</span><span class="p">):</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">y</span> <span class="o">+</span> <span class="n">w</span><span class="p">[</span><span class="n">c</span><span class="p">]</span><span class="o">*</span><span class="n">phi</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># try chaning the scale</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/8d955d6f8b26d9125374b7ed3048903d9c7f239e5fcba3207742cc9b65e67cfb.png" src="_images/8d955d6f8b26d9125374b7ed3048903d9c7f239e5fcba3207742cc9b65e67cfb.png" />
</div>
</div>
<p>Here are RBFs in 2D input space on a grid.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># 2D Gaussian basis functions</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">,</span> <span class="n">projection</span><span class="o">=</span><span class="s1">&#39;3d&#39;</span><span class="p">)</span>
<span class="c1"># prepare a surface grid</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">15</span><span class="p">)</span>
<span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">phi</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">phi</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="c1"># centers on a grid</span>
<span class="k">for</span> <span class="n">cx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">cy</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">):</span>
        <span class="c1">#Z = phi(X, cx, 1)*phi(Y, cy, 1)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">plot_surface</span><span class="p">(</span><span class="n">cx</span><span class="o">+</span><span class="n">X</span><span class="p">,</span> <span class="n">cy</span><span class="o">+</span><span class="n">Y</span><span class="p">,</span> <span class="n">Z</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;x1&quot;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;x2&quot;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\phi(x)$&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/0bdbe377f762473f6ff79ded0fec149cb64c3c07ee16ef9c3fa0bb39f6e1d52e.png" src="_images/0bdbe377f762473f6ff79ded0fec149cb64c3c07ee16ef9c3fa0bb39f6e1d52e.png" />
</div>
</div>
</section>
</section>
<section id="pattern-recognition">
<h2>3.2 Pattern Recognition<a class="headerlink" href="#pattern-recognition" title="Link to this heading">#</a></h2>
<p>Here we consider a problem of classifying input vectors <span class="math notranslate nohighlight">\(\x\)</span> into <span class="math notranslate nohighlight">\(K\)</span> discrete classes <span class="math notranslate nohighlight">\(C_k\)</span>.</p>
<p>There are three major approaches in pattern classification.</p>
<ul class="simple">
<li><p>Learn a discriminant function: <span class="math notranslate nohighlight">\(\x \rightarrow C_k\)</span></p></li>
<li><p>Learn a conditional probability: <span class="math notranslate nohighlight">\(p(C_k|\x)\)</span></p></li>
<li><p>Learn a generative model <span class="math notranslate nohighlight">\(p(\x|C_k)\)</span> and then use Bayes’ theorem:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[ p(C_k|\x) = \frac{p(\x|C_k)p(C_k)}{p(\x)} \]</div>
<p>Supervised pattern recognition has been highly successful in applications of image and speech recognition.
However, it is a question whether supervised learning is relevant for our vision and speech, because we do not usually receive labels for objects or words when we learn to see or hear as infants.
<em>Unsupervised learning</em>, covered in Chapter 5, might be a more plausible way of human sensory learning.</p>
</section>
<section id="the-perceptron">
<h2>The Perceptron<a class="headerlink" href="#the-perceptron" title="Link to this heading">#</a></h2>
<p>The first pattern classification learning machine was called <em>Perceptron</em> (Rosenblatt 1958, 1962).</p>
<blockquote>
<div><p><img alt="Perceptron" src="_images/Rosenblatt1958.png" /></p>
<p>The structure of a perceptron (from Rosenblatt F (1958). The design of an intelligent automaton. Research Reviews, US Office of Naval Research, 6, 5-13).</p>
</div></blockquote>
<p>The original Perceptron consisted of three layers of binary units: S(sensory)-units, A(associative)-units connected randomly with S-units, and R(response)-units.</p>
<p>Here we formulate the Perceptron in a generalized form.
The input vector <span class="math notranslate nohighlight">\(\x\)</span> is converted by a set of basis functions <span class="math notranslate nohighlight">\(\phi_i(\x)\)</span> into a feature vector</p>
<div class="math notranslate nohighlight">
\[
    \b{\phi}(\x)=(\phi_1(\x),...,\phi_M(\x))^T.
\]</div>
<p>In the simplest case, the feature vector is the same as the input vector <span class="math notranslate nohighlight">\(\b{\phi}(\x)=\x\)</span>, or just augmented by <span class="math notranslate nohighlight">\(1\)</span> to represent bias as <span class="math notranslate nohighlight">\(\b{\phi}(\x)=\pmatrix{1 \\ \x}\)</span>.
This is called <em>linear Perceptron</em>.</p>
<p>The output is given by</p>
<div class="math notranslate nohighlight">
\[
    y = f( \sum_{i=1}^M w_{i} \phi_i(\x)) = f( \w^T \b{\phi}(\x))
\]</div>
<p>where <span class="math notranslate nohighlight">\(\w=(w_1,...,w_M)^T\)</span> is the output connection weights.</p>
<p>The output function takes +1 or -1:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    f(u) = \begin{cases} 1 &amp; \mbox{if } u \ge 0 \\ -1 &amp; \mbox{if } u&lt;0. \end{cases}
\end{split}\]</div>
<p>For each input <span class="math notranslate nohighlight">\(\x_n\)</span>, the target output <span class="math notranslate nohighlight">\(y^*_n \in \{+1,-1\}\)</span> is given.</p>
<section id="perceptron-learning-rule">
<h3>Perceptron Learning Rule<a class="headerlink" href="#perceptron-learning-rule" title="Link to this heading">#</a></h3>
<p>Learning of Perceptron is based on the error function</p>
<div class="math notranslate nohighlight">
\[
    E(\w) = \sum_{y_n \ne y^*_n} -y^*_n\w^T\b{\phi}(\x_n)
\]</div>
<p>which takes a positive value for each missclassified output <span class="math notranslate nohighlight">\(y_n \ne y^*_n\)</span>.</p>
<p>The perceptron learning rule is the stochastic gradient</p>
<div class="math notranslate nohighlight">
\[
    \Delta \w = -\alpha\p{E(\w)}{\w}
\]</div>
<div class="math notranslate nohighlight">
\[
    = \alpha y^*_n\b{\phi}(\x_n) \ \mbox{ if } y_n\ne y^*_n
\]</div>
<div class="math notranslate nohighlight">
\[
    = \frac{\alpha}{2}(y^*_n-y_n)\b{\phi}(\x_n)
\]</div>
<p>which is a product of the output error and the input to each weight.</p>
<p>When two classes are <em>linearly separable</em>, the <em>Preceptron convergence theorem</em> assures that learning converges to find a proper hyperplane to separate two classes.</p>
</section>
<section id="example-perceptron-in-2d-feature-space">
<h3>Example: Perceptron in 2D feature space<a class="headerlink" href="#example-perceptron-in-2d-feature-space" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Perceptron</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Linear perceptron: phi(x)=[1,x]&quot;&quot;&quot;</span>
    
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">D</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Create a new perceptron&quot;&quot;&quot;</span>
        <span class="c1"># self.w = np.random.randn(D+1)  # output weight</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">D</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>  
        <span class="bp">self</span><span class="o">.</span><span class="n">vis</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">output</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;predict an output from input x&quot;&quot;&quot;</span>
        <span class="n">u</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="nd">@np</span><span class="o">.</span><span class="n">r_</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="n">x</span><span class="p">]</span>
        <span class="n">y</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">u</span><span class="o">&gt;</span><span class="mi">0</span> <span class="k">else</span> <span class="o">-</span><span class="mi">1</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">vis</span><span class="p">:</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;yo&quot;</span> <span class="k">if</span> <span class="n">y</span><span class="o">&gt;</span><span class="mi">0</span> <span class="k">else</span> <span class="s2">&quot;bo&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">y</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">learn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">yt</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;learn from (x, yt) pair&quot;&quot;&quot;</span>
        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">error</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">yt</span>
        <span class="k">if</span> <span class="n">error</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">+=</span> <span class="n">alpha</span><span class="o">*</span><span class="n">yt</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">r_</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="n">x</span><span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">vis</span><span class="p">:</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;y*&quot;</span> <span class="k">if</span> <span class="n">yt</span><span class="o">&gt;</span><span class="mi">0</span> <span class="k">else</span> <span class="s2">&quot;b*&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">plot_boundary</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">error</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">plot_boundary</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">u</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">,</span> <span class="n">r</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;plot decision boundary with shift u, range r</span>
<span class="sd">        w0 + w1*x + w2*y = u</span>
<span class="sd">        y = (u - w0 - w1*x)/w2</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># show weight vector</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">[</span><span class="mi">2</span><span class="p">]],</span> <span class="s2">&quot;r&quot;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="s2">&quot;r*&quot;</span><span class="p">)</span>  <span class="c1"># arrowhead</span>
        <span class="c1"># decision boundary</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="n">r</span><span class="p">,</span> <span class="n">r</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="n">u</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">x</span><span class="p">)</span><span class="o">/</span><span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">col</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;x1&quot;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;x2&quot;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;square&quot;</span><span class="p">);</span> 
        <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="n">r</span><span class="p">,</span> <span class="n">r</span><span class="p">]);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="n">r</span><span class="p">,</span> <span class="n">r</span><span class="p">]);</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Test by 2D gaussian data</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">10</span>  <span class="c1"># sample size</span>
<span class="n">D</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">Xp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">D</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">])</span>  <span class="c1"># positive data</span>
<span class="n">Xn</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span><span class="n">D</span><span class="p">)</span>   <span class="c1"># negative data</span>
<span class="c1"># concatenate positive/negative data</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">r_</span><span class="p">[</span><span class="n">Xp</span><span class="p">,</span><span class="n">Xn</span><span class="p">]</span>
<span class="n">Yt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">r_</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">N</span><span class="p">),</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">N</span><span class="p">)]</span>  <span class="c1"># target</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">N</span>
<span class="c1"># plot the data</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">Yt</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;x1&quot;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;x2&quot;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s2">&quot;equal&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/e01c4632c7edca3679e61797595091f9cb1766bcb44bfecaa397b86854ed8b81.png" src="_images/e01c4632c7edca3679e61797595091f9cb1766bcb44bfecaa397b86854ed8b81.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create a Perceptron</span>
<span class="n">perc</span> <span class="o">=</span> <span class="n">Perceptron</span><span class="p">(</span><span class="n">D</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># perceptron with 2D input</span>
<span class="n">perc</span><span class="o">.</span><span class="n">vis</span> <span class="o">=</span> <span class="kc">True</span>  <span class="c1"># turn on visualization</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Learning: run this cell several times</span>
<span class="n">err</span> <span class="o">=</span> <span class="mi">0</span>
<span class="c1"># shuffle data order</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
    <span class="n">err</span> <span class="o">+=</span> <span class="n">perc</span><span class="o">.</span><span class="n">learn</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">Yt</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;mse =&quot;</span><span class="p">,</span> <span class="n">err</span><span class="o">/</span><span class="n">N</span><span class="p">,</span> <span class="s2">&quot;; w = &quot;</span><span class="p">,</span> <span class="n">perc</span><span class="o">.</span><span class="n">w</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>mse = 0.6 ; w =  [-0.1         0.22415189 -0.03006994]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/var/folders/6k/5jhd3ffj5h796v6ts63gjksh0000gn/T/ipykernel_48582/519920391.py:39: RuntimeWarning: invalid value encountered in divide
  y = (u-self.w[0] - self.w[1]*x)/self.w[2]
</pre></div>
</div>
<img alt="_images/fa42cedf1c6c467dfa51292f42f1adcdb1a9c3bd765873aadf3652568614e31e.png" src="_images/fa42cedf1c6c467dfa51292f42f1adcdb1a9c3bd765873aadf3652568614e31e.png" />
</div>
</div>
</section>
</section>
<section id="support-vector-machine">
<h2>Support Vector Machine<a class="headerlink" href="#support-vector-machine" title="Link to this heading">#</a></h2>
<p>With Perceptron learning rule, the line separating two classes can end up in any configuration so that two classes are separated, which may not be good for <em>generalization</em>, i.e., classification of new data that were not used for training.</p>
<p>The <em>support vector machine (SVM)</em> (Bishop 2006, section 7.1) tries to find a separation line that has the largest margin to the positive and negative data by minimizing the <em>hinge</em> objective function</p>
<div class="math notranslate nohighlight">
\[
    E(\w) = \sum_n \max(0, 1-y^*_n\w^T\b{\phi}(\x_n)) + \lambda||\w||.
\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">25</span><span class="p">)</span>
<span class="n">E1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">E0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">E1</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">E0</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span> <span class="p">(</span><span class="s1">&#39;y*=1&#39;</span><span class="p">,</span> <span class="s1">&#39;y*=-1&#39;</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$w^T \phi(x)$&#39;</span><span class="p">);</span> <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;E&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/452367877501922afafe9aef3d3d50cdd56fc0efb2f8bc5cbb94f078eeeb415a.png" src="_images/452367877501922afafe9aef3d3d50cdd56fc0efb2f8bc5cbb94f078eeeb415a.png" />
</div>
</div>
<p>This objective function penalizes the weighted input sum <span class="math notranslate nohighlight">\(\w^T\b{\phi}(\x_n)\)</span> close to the decision boundary even if that is on the correct side, while keeping <span class="math notranslate nohighlight">\(\w\)</span> small.
This produces a large margin between the lines for <span class="math notranslate nohighlight">\(\w^T\b{\phi}(\x_n))=1\)</span> and <span class="math notranslate nohighlight">\(\w^T\b{\phi}(\x_n))=-1\)</span>.</p>
<p>A standard way for solving this optimization problem is to use a batch optimization method called <em>quadratic programming</em>.</p>
<p>It is often the case that the basis functions are allocated around each data point <span class="math notranslate nohighlight">\(\x_n\)</span> using so-called <em>kernel</em> function</p>
<div class="math notranslate nohighlight">
\[
    \phi_i(\x) = K(\x,\x_i).
\]</div>
<section id="online-svm">
<h3>Online SVM<a class="headerlink" href="#online-svm" title="Link to this heading">#</a></h3>
<p>Here we consider an online version of SVM called <em>Pagasos</em> (Shalev-Shwartz et al. 2010) that has a closer link with Perceptron learning.</p>
<p>The weights are updated by</p>
<div class="math notranslate nohighlight">
\[
    \Delta\w = \alpha\{1[1-y^*_n\w^T\b{\phi}(\x_n)] y^*_n\b{\phi}(\x_n) - \lambda\w\}
\]</div>
<p>where <span class="math notranslate nohighlight">\(1[\ ]\)</span> represents an indicator function; <span class="math notranslate nohighlight">\(1[u]=1\)</span> if <span class="math notranslate nohighlight">\(u&gt;0\)</span> and <span class="math notranslate nohighlight">\(1[u]=0\)</span> otherwise.</p>
<p>The learning rate is gradually decreased as <span class="math notranslate nohighlight">\(\alpha=\frac{1}{\lambda n}\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Pegasos</span><span class="p">(</span><span class="n">Perceptron</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Pagasos, online SVM with linear kernel&quot;&quot;&quot;</span>
    
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">D</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">D</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># counter for rate scheduling</span>
        
    <span class="k">def</span><span class="w"> </span><span class="nf">learn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">yt</span><span class="p">,</span> <span class="n">lamb</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;learn from (x, y) pair&quot;&quot;&quot;</span>
        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n</span> <span class="o">+=</span> <span class="mi">1</span>  <span class="c1"># data count</span>
        <span class="n">alpha</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="n">lamb</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">n</span><span class="p">)</span>  <span class="c1"># adapt learning rate</span>
        <span class="n">u</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="nd">@np</span><span class="o">.</span><span class="n">r_</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="n">x</span><span class="p">]</span>  <span class="c1"># input sum</span>
        <span class="c1"># hinge loss and regularization except bias</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">+=</span> <span class="n">alpha</span><span class="o">*</span><span class="p">(((</span><span class="n">yt</span><span class="o">*</span><span class="n">u</span><span class="p">)</span><span class="o">&lt;</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">yt</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">r_</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="n">x</span><span class="p">]</span> <span class="o">-</span> <span class="n">lamb</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">r_</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">[</span><span class="mi">1</span><span class="p">:]])</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">vis</span><span class="p">:</span>  <span class="c1"># for first 2D</span>
            <span class="c1"># target output</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;y*&quot;</span> <span class="k">if</span> <span class="n">yt</span><span class="o">&gt;</span><span class="mi">0</span> <span class="k">else</span> <span class="s2">&quot;b*&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">plot_boundary</span><span class="p">(</span><span class="n">u</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s2">&quot;g&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">plot_boundary</span><span class="p">(</span><span class="n">u</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">plot_boundary</span><span class="p">(</span><span class="n">u</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s2">&quot;m&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">yt</span><span class="o">*</span><span class="n">u</span><span class="p">)</span>  <span class="c1"># hinge loss</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create a Pagasos</span>
<span class="n">pega</span> <span class="o">=</span> <span class="n">Pegasos</span><span class="p">(</span><span class="n">D</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># 2D input</span>
<span class="n">pega</span><span class="o">.</span><span class="n">vis</span> <span class="o">=</span> <span class="kc">True</span>  <span class="c1"># turn on visualization</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Learning: run this cell several times</span>
<span class="n">err</span> <span class="o">=</span> <span class="mi">0</span>
<span class="c1"># shuffle data order</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
    <span class="n">err</span> <span class="o">+=</span> <span class="n">pega</span><span class="o">.</span><span class="n">learn</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">Yt</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">lamb</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;n =&quot;</span><span class="p">,</span> <span class="n">pega</span><span class="o">.</span><span class="n">n</span><span class="p">,</span> <span class="s2">&quot;; err =&quot;</span><span class="p">,</span> <span class="n">err</span><span class="o">/</span><span class="n">N</span><span class="p">,</span> <span class="s2">&quot;; w =&quot;</span><span class="p">,</span> <span class="n">pega</span><span class="o">.</span><span class="n">w</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>n = 20 ; err = 0.35082838347647305 ; w = [-0.99111259  0.54852323  0.0562829 ]
</pre></div>
</div>
<img alt="_images/3d3fe0019c34ff29c644570aa612883b652e7a87d5b29dc3e0a0d53a98902d3b.png" src="_images/3d3fe0019c34ff29c644570aa612883b652e7a87d5b29dc3e0a0d53a98902d3b.png" />
</div>
</div>
</section>
</section>
<section id="logistic-regression">
<h2>Logistic Regression<a class="headerlink" href="#logistic-regression" title="Link to this heading">#</a></h2>
<p>In Perceptron and SVM, the output is binary. We sometimes want to express the certainty in the output by the probability of the data belonging to a class <span class="math notranslate nohighlight">\(p(C_k|\x)\)</span>.</p>
<p><em>Logistic regression</em> is a probabilistic generalization of linear regression (Bishop 2006, Section 4.3). Its probabilistic output is given by</p>
<div class="math notranslate nohighlight">
\[
    p(C_1|\x) = y = g(\w^T\x))
\]</div>
<p>and <span class="math notranslate nohighlight">\(p(C_0|\x) = 1 - p(C_1|\x)\)</span>.</p>
<p>The function <span class="math notranslate nohighlight">\(g(\ )\)</span> is called <em>logistic sigmoid function</em></p>
<div class="math notranslate nohighlight">
\[
    g(u) = \frac{1}{1+e^{-u}}.
\]</div>
<blockquote>
<div><p>The derivative of the logistic sigmoid function is represented as</p>
<div class="math notranslate nohighlight">
\[
    g'(u) = \frac{-1}{(1+e^{-u})^2}(-e^{-u})
    = \frac{1}{1+e^{-u}}\frac{e^{-u}}{1+e^{-u}}
    = g(u)(1-g(u))
\]</div>
</div></blockquote>
<section id="maximum-likelihood-by-iterative-least-squares">
<h3>Maximum likelihood by iterative least squares<a class="headerlink" href="#maximum-likelihood-by-iterative-least-squares" title="Link to this heading">#</a></h3>
<p>We can find the weights of logistic regression by the principle of maximum likelihood, the probability of reproducing the data.</p>
<p>The likelihood of weights <span class="math notranslate nohighlight">\(\w\)</span> for a set of observed data <span class="math notranslate nohighlight">\((X,\y^*)\)</span> is</p>
<div class="math notranslate nohighlight">
\[
    L(\w) = p(\y^*|X,\w)
    = \prod_{y^*_n=1} y_n \prod_{y^*_n=0}(1-y_n)
    = \prod_{n=1}^N y_n^{y^*_n} (1-y_n)^{1-y^*_n}.
\]</div>
<p>The negative log likelihood is often called <em>cross entropy error</em></p>
<div class="math notranslate nohighlight">
\[
    E(\w) = -l(\w) = -\log p(\y^*|X,\w)
    = -\sum_{n=1}^N\{y^*_n\log y_n + (1-y^*_n)\log(1-y_n)\}.
\]</div>
<p>From</p>
<div class="math notranslate nohighlight">
\[
    \p{y}{\w} = g'(\w^T\x)\p{\w^T\x}{\w} = y(1-y)\x,
\]</div>
<p>the gradient of the cross entropy error is given as</p>
<div class="math notranslate nohighlight">
\[
    \p{E(\w)}{\w} 
    = -\sum_{n=1}^N\{y^*_n(1-y_n)\x_n - (1-y^*_n)y_n\x_n\}
\]</div>
<div class="math notranslate nohighlight">
\[
    = \sum_{n=1}^N (y_n - y^*_n)\x_n
    = X^T(\y - \y^*) 
\]</div>
<p>which takes the same form as in linear regression.</p>
<p>Unlike in linear regression, <span class="math notranslate nohighlight">\(\p{E(\w)}{\w}=0\)</span> does not have a closed form solution as in linear regression.
Instead, we can apply Newton method using the <em>Hessian matrix</em></p>
<div class="math notranslate nohighlight">
\[
    H = \p{}{\w}\p{E(\w)}{\w} = X^T R X
\]</div>
<p>where <span class="math notranslate nohighlight">\(R\)</span> is a diagonal matrix made of the derivative of the sigmoid functions</p>
<div class="math notranslate nohighlight">
\[
    R = \mbox{diag}(y_1(1-y_1),...,y_n(1-y_n)).
\]</div>
<p>The update is made by</p>
<div class="math notranslate nohighlight">
\[
    \w := \w - (X^TRX)^{-1}X^T(\y - \y^*)
    = (X^TRX)^{-1}X^T R \b{z}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\b{z}\)</span> is an effective target value</p>
<div class="math notranslate nohighlight">
\[
    \b{z} = X\w - R^{-1}(\y - \y^*).
\]</div>
<p>This algorithm is called <em>iterative reweighted least squares</em>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">LogisticRegression</span><span class="p">(</span><span class="n">Perceptron</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Logistic regression&quot;&quot;&quot;</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">u</span><span class="p">):</span>
        <span class="k">return</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">u</span><span class="p">))</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">output</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;output for vector/matrix input x&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>  <span class="c1"># x is a vector</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">r_</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="n">x</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>   <span class="c1"># x is a matrix</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span><span class="n">x</span><span class="p">]</span>
        <span class="n">u</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="nd">@self</span><span class="o">.</span><span class="n">w</span>
        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">u</span><span class="p">)</span>   <span class="c1"># sigmoid output</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">vis</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>  <span class="c1"># x is a vector</span>
                <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s2">&quot;yo&quot;</span> <span class="k">if</span> <span class="n">y</span><span class="o">&gt;</span><span class="mf">0.5</span> <span class="k">else</span> <span class="s2">&quot;bo&quot;</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>   <span class="c1"># x is a matrix</span>
                <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">y</span>
    
    <span class="k">def</span><span class="w"> </span><span class="nf">rls</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">yt</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;reweighted least square with (x, y) pairs&quot;&quot;&quot;</span>
        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># also set self.X</span>
        <span class="n">error</span> <span class="o">=</span> <span class="n">y</span> <span class="o">-</span> <span class="n">yt</span>
        <span class="n">R</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">y</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">y</span><span class="p">))</span> <span class="c1"># weighting matrix</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">-=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="nd">@R@self</span><span class="o">.</span><span class="n">X</span><span class="p">)</span> <span class="o">@</span> <span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="nd">@error</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">vis</span><span class="p">:</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">yt</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;*&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">plot_boundary</span><span class="p">(</span><span class="n">u</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s2">&quot;g&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">plot_boundary</span><span class="p">(</span><span class="n">u</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">plot_boundary</span><span class="p">(</span><span class="n">u</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">col</span><span class="o">=</span><span class="s2">&quot;m&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">error</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="online-logistic-regression">
<h3>Online logistic regression<a class="headerlink" href="#online-logistic-regression" title="Link to this heading">#</a></h3>
<p>From the gradient of negative log likelihood for each data point</p>
<div class="math notranslate nohighlight">
\[
    \p{E_n(\w)}{\w} = (y_n - y^*_n)\x_n,
\]</div>
<p>we can also apply stochastic gradient descent</p>
<div class="math notranslate nohighlight">
\[
    \Delta \w = - \alpha \p{E_n(\w)}{\w} = \alpha (y^*_n - y_n)\x_n.
\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">logreg</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">D</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">logreg</span><span class="o">.</span><span class="n">vis</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">Yb</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">Yt</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># target in [0,1]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Repeat for Iterative Reweighted least squares</span>
<span class="n">err</span> <span class="o">=</span> <span class="n">logreg</span><span class="o">.</span><span class="n">rls</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Yb</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot; err =&quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">err</span><span class="o">**</span><span class="mi">2</span><span class="p">),</span> <span class="s2">&quot;; w =&quot;</span><span class="p">,</span> <span class="n">logreg</span><span class="o">.</span><span class="n">w</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> err = 0.25 ; w = [-1.21077512  0.9652673  -0.19782   ]
</pre></div>
</div>
<img alt="_images/f3e6b0755265a0d40b264da470fe55d3a8cbd7cd5061600a75077d880747738c.png" src="_images/f3e6b0755265a0d40b264da470fe55d3a8cbd7cd5061600a75077d880747738c.png" />
</div>
</div>
</section>
</section>
<section id="classical-conditioning">
<h2>3.3 Classical Conditioning<a class="headerlink" href="#classical-conditioning" title="Link to this heading">#</a></h2>
<p>Animals have innate behaviors to avoid dangers or to acquire rewards, such as blinking when a strong light or wind hits the eyes or drooling when a food is expected.
Such response is called a <em>unconditioned response (UR)</em> and the sensory cue to elicit UR is called <em>unconditioned stimulus (US)</em>.</p>
<p>When an US is repeatedly preceded by another stimulus, such as a sound, animals start to take the same response as UR even before US is presented.
In this case the response is called a <em>conditioned response (CR)</em> and the sensory cue to elicit CR is called <em>conditioned stimulus (CS)</em>.
This type of learning is called <em>classical conditioning</em> or <em>Pavlovian conditioning</em> and can be considered as an example of supervised learning.</p>
<p>The learned mapping can be CS to CR, or CS to US which elicits UR=CR.</p>
<section id="eye-blink-conditioning">
<h3>Eye Blink Conditioning<a class="headerlink" href="#eye-blink-conditioning" title="Link to this heading">#</a></h3>
<p>When an airpuff (US) is given around eyes, an animal would blink its eyes (UR). If the airpuff is preceded with a consistent interval by a cue, such as tone (CS), the animal learns to blink (CR) just before the airpuff is given.</p>
<p>Richard Thompson and colleagues investigated the neural circuit behind this eye-blink conditioning and identified the cerebellum as the major locus of learning (Thompson, 1986).</p>
<p>While blockade of the cerebellum does not affect eye-blink response itself (US-UR), it blocks learning (CS-CR). The activity of the output neurons of the cerebellum increase their activities as learning progress, as shown above.</p>
</section>
</section>
<section id="the-cerebellum">
<h2>3.4 The Cerebellum<a class="headerlink" href="#the-cerebellum" title="Link to this heading">#</a></h2>
<p>The neural circuit of the cerebellum has a distinctive orthogonal structure. There are a massive number of <em>granule cells</em>, each of which collects inputs from a major input to the cerebellum, <em>the mossy fibers</em>.
Granule cell axons run parallely in the lateral direction in the cerebellar cortex, called <em>parallel fibers</em>.
The output neurons of the cerebellum, <em>Purkinje cells</em>, spread fan-shaped dendrites in the longitudinal direction and receive a large number of parallel fibers.
Furthermore, each Purkinje cell is twined by a single axon called <em>climbing fiber</em> from a brain stem nucleus called the <em>inferior olive</em>.</p>
<blockquote>
<div><p><img alt="CerebellarCircuit" src="_images/DAngelo2013.jpg" /><br />
The circuit of the cerebellar cortex (from D’Anglo and Casali. 2013)</p>
</div></blockquote>
<section id="cerebellar-perceptron-hyopthesis">
<h3>Cerebellar Perceptron Hyopthesis<a class="headerlink" href="#cerebellar-perceptron-hyopthesis" title="Link to this heading">#</a></h3>
<p>This peculiar organization inspired theorists around 1970 to come up with an idea that the cerebellum may work similar to the Perceptron (Marr 1969; Albus 1971).
The massive number of granule cells may create a high-dimensional feature representation of the mossy fiber input.
The single climbing fiber input may sever as the supervising signal to induce plasticity in the parallel-fiber input synapses to the Purkinje cell.</p>
<blockquote>
<div><p><img alt="CerebellarPerceptron" src="_images/Albus1971.jpg" /></p>
<p>The cerebellar perceptron model (from Albus 1971).</p>
</div></blockquote>
<p>This hypothesis further motivated neurobiologists to test that experimentally.
Indeed, cerebellar synaptic plasticity guided by the climbing fiber input was experimentally confirmed by Masao Ito (1984, 2000).</p>
</section>
<section id="cerebellar-internal-model-hyopthesis">
<h3>Cerebellar Internal Model Hyopthesis<a class="headerlink" href="#cerebellar-internal-model-hyopthesis" title="Link to this heading">#</a></h3>
<p>The cerebellum is known to be important for movement control.
A possible way in which supervised learning can be helpful is to provide an <em>internal model</em> of the motor apparatus (Wolpert et al. 1998).</p>
<blockquote>
<div><img src="https://pub.mdpi-res.com/biomimetics/biomimetics-05-00019/article_deploy/html/images/biomimetics-05-00019-g005.png?1590743622" alt="Feedback Error Learning" width="500px">
<p>The feedback error learning model (Kawato et al. 1987). The innate feedback controller in the brainstem or spinal cord provides corrective motor signal, which is used as the error signal for supervised learning of <em>inverse model</em> in the cerebellum (from <a class="reference external" href="https://doi.org/10.3390/biomimetics5020019">Ellery 2020</a>)</p>
</div></blockquote>
<p>The molecular mechanism for the association of the parallel fiber input and the climbing fiber input (output error) at the Purkinje cell synapses has also been studied in detail (Ito 2000; Ogasawara et al. 2008).</p>
</section>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Bishop CM (2006) Pattern Recognition and Machine Learning. Springer.  <a class="reference external" href="https://www.microsoft.com/en-us/research/people/cmbishop/prml-book/">https://www.microsoft.com/en-us/research/people/cmbishop/prml-book/</a></p>
<ul>
<li><p>Chapter 3: Linear models for regression</p></li>
<li><p>Chapter 4: Linear models for classification</p></li>
</ul>
</li>
</ul>
<section id="perceptron">
<h3>Perceptron<a class="headerlink" href="#perceptron" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Rosenblatt F (1958). The perceptron: a probabilistic model for information storage and organization in the brain. Psychol Rev, 65, 386-408. <a class="reference external" href="https://doi.org/10.1037/h0042519">https://doi.org/10.1037/h0042519</a></p></li>
<li><p>Rosenblatt F (1962) Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms. Spartan. <a class="reference external" href="https://babel.hathitrust.org/cgi/pt?id=mdp.39015039846566&amp;amp;view=1up&amp;amp;seq=5">https://babel.hathitrust.org/cgi/pt?id=mdp.39015039846566&amp;view=1up&amp;seq=5</a></p></li>
</ul>
</section>
<section id="id1">
<h3>Classical conditioning<a class="headerlink" href="#id1" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>McCormick DA, Thompson RF (1984) Neuronal responses of the rabbit cerebellum during acquisition and performance of a classically conditioned nictitating membrane-eyelid response. Journal of Neuroscience, 4 (11) 2811-2822. <a class="reference external" href="https://doi.org/10.1523/JNEUROSCI.04-11-02811.1984">https://doi.org/10.1523/JNEUROSCI.04-11-02811.1984</a></p></li>
<li><p>Thompson RF (1986) The neurobiology of learning and memory. Sicence, 233, 941-947. <a class="reference external" href="https://doi.org/10.1126/science.3738519">https://doi.org/10.1126/science.3738519</a></p></li>
</ul>
</section>
<section id="cerebellar-perceptron-hypothesis">
<h3>Cerebellar perceptron hypothesis<a class="headerlink" href="#cerebellar-perceptron-hypothesis" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Marr D (1969) A theory of cerebellar cortex. Journal of Physiology, 202:437–470.  <a class="reference external" href="https://doi.org/10.1113/jphysiol.1969.sp008820">https://doi.org/10.1113/jphysiol.1969.sp008820</a></p></li>
<li><p>Albus JS (1971) A theory of cerebellar function. Mathematical Bioscience, 10:25–61, 1971. <a class="reference external" href="https://doi.org/10.1016/0025-5564(71)90051-4">https://doi.org/10.1016/0025-5564(71)90051-4</a></p></li>
<li><p>Ito M (1984) The Cerebellum and Neural Control, Raven Press. <a class="reference external" href="https://archive.org/details/cerebellumneural0000itom">https://archive.org/details/cerebellumneural0000itom</a></p></li>
</ul>
</section>
<section id="cerebellar-internal-model-hypothesis">
<h3>Cerebellar internal model hypothesis<a class="headerlink" href="#cerebellar-internal-model-hypothesis" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Kawato M, Furukawa K, Suzuki R (1987). A hierarchical neural-network model for control and learning of voluntary movement. Biol Cybern, 57, 169-85. <a class="reference external" href="https://doi.org/10.1007/BF00364149">https://doi.org/10.1007/BF00364149</a></p></li>
<li><p>Wolpert DM, Miall RC, Kawato M (1998) Internal models in the cerebellum. Trends in Cognitive Sciences, 2:338–347. <a class="reference external" href="https://doi.org/10.1016/S1364-6613(98)01221-2">https://doi.org/10.1016/S1364-6613(98)01221-2</a></p></li>
<li><p>Ito M (2000) Mechanisms of motor learning in the cerebellum. Brain Research 886, 237–245. <a class="reference external" href="https://doi.org/10.1016/S0006-8993(00)03142-5">https://doi.org/10.1016/S0006-8993(00)03142-5</a></p></li>
<li><p>Ogasawara H, Doi T, Kawato M (2008) Systems biology perspectives on cerebellar long-term depression. Neurosignals, 16, 300–317.
<a class="reference external" href="https://doi.org/10.1159/000123040">https://doi.org/10.1159/000123040</a></p></li>
<li><p>D’Angelo E, Casali S (2013). Seeking a unified framework for cerebellar function and dysfunction: from circuit operations to cognition. Front Neural Circuits, 6, 116. <a class="reference external" href="https://doi.org/10.3389/fncir.2012.00116">https://doi.org/10.3389/fncir.2012.00116</a></p></li>
</ul>
</section>
</section>
<div class="toctree-wrapper compound">
</div>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="Neurons_Exercise.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Exercise: Neural Modeling and Analysis</p>
      </div>
    </a>
    <a class="right-next"
       href="Supervised_Exercise.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Exercise: Supervised Learning</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#contents">Contents</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#function-approximation">3.1 Function Approximation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regression">Linear Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#least-square-solution">Least square solution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-likelihood-estimate">Maximum likelihood estimate</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-arm-dynamics">Example: Arm dynamics</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#online-learning">Online Learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#basis-functions">Basis Functions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#radial-basis-functions">Radial Basis Functions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pattern-recognition">3.2 Pattern Recognition</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-perceptron">The Perceptron</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#perceptron-learning-rule">Perceptron Learning Rule</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-perceptron-in-2d-feature-space">Example: Perceptron in 2D feature space</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#support-vector-machine">Support Vector Machine</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#online-svm">Online SVM</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression">Logistic Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#maximum-likelihood-by-iterative-least-squares">Maximum likelihood by iterative least squares</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#online-logistic-regression">Online logistic regression</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#classical-conditioning">3.3 Classical Conditioning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#eye-blink-conditioning">Eye Blink Conditioning</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-cerebellum">3.4 The Cerebellum</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cerebellar-perceptron-hyopthesis">Cerebellar Perceptron Hyopthesis</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cerebellar-internal-model-hyopthesis">Cerebellar Internal Model Hyopthesis</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#perceptron">Perceptron</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Classical conditioning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cerebellar-perceptron-hypothesis">Cerebellar perceptron hypothesis</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cerebellar-internal-model-hypothesis">Cerebellar internal model hypothesis</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Kenji Doya
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>