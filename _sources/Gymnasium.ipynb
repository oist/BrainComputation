{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix: Gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Gymnasium* is a collection of environments for testing reinforcement learning algorithms.\n",
    "It was initially developed by OpenAI, but as the company shifted its focus to large language models, it was left stranded. Now it is maintained by the Farama Foundation.  \n",
    "https://gymnasium.farama.org/  \n",
    "https://github.com/Farama-Foundation/Gymnasium\n",
    "\n",
    "Gymnasium can be installed by `pip` or `conda`. We also use `pygame` for simple 2D rendering.\n",
    "\n",
    "```pip install gymnasium pygame```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CliffWalking\n",
    "\n",
    "Let us try the *CliffWalking* environment introduced in Barto & Sutton textbook>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new enviornment\n",
    "env = gym.make(\"CliffWalking-v0\", render_mode=\"human\") # with animation\n",
    "# check the observation/action spaces\n",
    "print(env.spec)\n",
    "print(\"observations:\", env.observation_space.n)  # 4x12\n",
    "print(\"actions:\", env.action_space.n)  # 0=up; 1=right; 2=down; 3=left\n",
    "# Reset the environment state\n",
    "obs, info = env.reset()\n",
    "print(\"observation:\", obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Episode with random actions\n",
    "T = 20\n",
    "sar = np.zeros((T+1,3))  # state, action, reward\n",
    "sar[0,0] = obs\n",
    "for t in range(T):\n",
    "    # Choose a random action\n",
    "    action = env.action_space.sample()\n",
    "    # Environment dynamics\n",
    "    obs, reward, term, trunc, info = env.step(action)\n",
    "    sar[t,1:] = [action, reward]; sar[t+1,0] = obs\n",
    "    if term or trunc:  # terminal stete or timeout\n",
    "        break\n",
    "plt.plot(sar[:,0]%12); plt.plot(3-(sar[:,0]//12))  # x, y\n",
    "plt.plot(sar[:-1,1], \"o\")  # action\n",
    "plt.plot(np.maximum(sar[:-1,2],-5))  # reward\n",
    "plt.legend(['x','y','a','r']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dispose the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning and SARSA\n",
    "\n",
    "Let us test Q-learning and SARSA agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RL:\n",
    "    \"\"\"Reinforcement learning with gymnasium environment\"\"\"\n",
    "\n",
    "    def __init__(self, environment, agent):\n",
    "        \"\"\"After environment=gym.make()\"\"\"\n",
    "        self.env = environment\n",
    "        self.Ns = environment.observation_space.n\n",
    "        self.Na = environment.action_space.n\n",
    "        self.agent = agent(self.Ns, self.Na)\n",
    "    \n",
    "    def episode(self, tmax=50):\n",
    "        \"\"\"One episode\"\"\"\n",
    "        # Reset state\n",
    "        state, info = self.env.reset()\n",
    "        reward = None\n",
    "        # Record of state, action, reward\n",
    "        sar = np.zeros((tmax+1, 3))\n",
    "        sar[0,0] = state\n",
    "        # Repeat interactoin\n",
    "        for t in range(0, tmax):\n",
    "            # take an action and learn\n",
    "            action = self.agent.step(state, reward)\n",
    "            # environmental dynamics\n",
    "            state, reward, term, trunc, info = self.env.step(action)\n",
    "            sar[t,1:] = [action, reward]\n",
    "            sar[t+1,0] = state  # new state\n",
    "            if term or trunc:\n",
    "                # learn from terminal reward\n",
    "                self.agent.step(state, reward)\n",
    "                break\n",
    "        self.sar = sar[:t+2]\n",
    "        return self.sar\n",
    "    \n",
    "    def run(self, nrun=10, tmax=50):\n",
    "        \"\"\"Multiple runs of episodes\"\"\"\n",
    "        Return = np.zeros(nrun)\n",
    "        for n in range(nrun):\n",
    "            r = self.episode(tmax)[:,-1]  # reward sequence\n",
    "            Return[n] = sum(r)\n",
    "        return Return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAgent():\n",
    "    \"\"\"Class for a Q-learning/SARSA agent\"\"\"\n",
    "\n",
    "    def __init__(self, nstate, naction):\n",
    "        self.Ns = nstate   # number of states\n",
    "        self.Na = naction  # number of actions\n",
    "        # allocate Q table\n",
    "        self.Q = np.zeros((nstate, naction))\n",
    "        # default parameters\n",
    "        self.alpha = 0.1  # learning rate\n",
    "        self.beta = 1.0   # inverse temperature\n",
    "        self.gamma = 0.9  # discount factor\n",
    "        self.onpolicy = False  # Q-learning\n",
    "    \n",
    "    def boltzmann(self, q):\n",
    "        \"\"\"Boltzmann selection\"\"\"\n",
    "        pr = np.exp( self.beta*q)   # unnormalized probability\n",
    "        pr = pr/sum(pr)    # probability\n",
    "        return np.random.choice(len(pr), p=pr)\n",
    "\n",
    "    def step(self, state, reward=None):\n",
    "        \"\"\"learn by reward and take an action\"\"\"\n",
    "        # Boltzmann action selection\n",
    "        action = self.boltzmann( self.Q[state,:])\n",
    "        if reward != None:\n",
    "            # TD error: self.state/action retains the previous ones\n",
    "            Qnew = self.Q[state,action] if self.onpolicy else max(self.Q[state,:])\n",
    "            delta = reward + self.gamma*Qnew - self.Q[self.state,self.action]\n",
    "            # Update the value for previous state and action\n",
    "            self.Q[self.state,self.action] += self.alpha*delta\n",
    "        # Boltzmann action selection\n",
    "        self.action = self.boltzmann( self.Q[state,:])\n",
    "        # remember the state\n",
    "        self.state = state\n",
    "        self.action = action\n",
    "        return self.action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CliffWalking environment and Q-learning agent\n",
    "env = gym.make('CliffWalking-v0', render_mode='rgb_array')\n",
    "cwql = RL(env, QAgent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run an episode\n",
    "sar = cwql.episode(100)\n",
    "plt.plot(sar[:,0]%12); plt.plot(3-(sar[:,0]//12)) # x, y\n",
    "plt.plot(sar[:-1,1], 'o')  # action\n",
    "plt.plot(np.maximum(sar[:-1,2],-5)) # reward\n",
    "plt.legend(['x','y','a','r']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Qmap(Q, w, h, a=(0,1,2,3)):\n",
    "    \"\"\"2D map of action values\n",
    "    action index:(N,E,S,W)\"\"\"\n",
    "    Q4 = np.zeros((h,w,3,3)) # 4D array\n",
    "    Q4[:,:,0,1] = Q[:,a[0]].reshape((h,w))  # N\n",
    "    Q4[:,:,1,2] = Q[:,a[1]].reshape((h,w))  # E\n",
    "    Q4[:,:,2,1] = Q[:,a[2]].reshape((h,w))  # S\n",
    "    Q4[:,:,1,0] = Q[:,a[3]].reshape((h,w))  # W\n",
    "    Q2 = np.transpose(Q4, (0,2,1,3))\n",
    "    plt.imshow(Q2.reshape((h*3, w*3)), extent=(-.5,w-.5,h-.5,-.5))\n",
    "    plt.colorbar(shrink=0.4);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def track(s, w):\n",
    "    \"\"\"show the state track in 2D\"\"\"\n",
    "    x = s%w; y = s//w\n",
    "    plt.plot(x, y-np.linspace(0,0.2,len(y)), lw=0.5, c='r') # gradually shift up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Qmap(cwql.agent.Q, 12, 4)\n",
    "track(cwql.sar[:,0], 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat\n",
    "R = cwql.run(nrun=1000, tmax=100)\n",
    "plt.plot(R);\n",
    "plt.xlabel('episode'); plt.ylabel('return');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Qmap(cwql.agent.Q, 12, 4)\n",
    "track(cwql.sar[:,0], 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CliffWalking environment and SARSA agent\n",
    "env = gym.make('CliffWalking-v0', render_mode='rgb_array')\n",
    "cwsa = RL(env, QAgent)\n",
    "cwsa.agent.onpolicy = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat\n",
    "R = cwsa.run(nrun=1000, tmax=100)\n",
    "plt.plot(R);\n",
    "plt.xlabel('episode'); plt.ylabel('return');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Qmap(cwsa.agent.Q, 12, 4)\n",
    "track(cwsa.sar[:,0], 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stable baseline\n",
    "\n",
    "A goodness of Gymnasium is that there are many RL algorithms tested and shared by the community. There are several well curated collectioin sites, such as\n",
    "\n",
    "* https://stable-baselines.readthedocs.io\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
