{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8. Multiple Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ % Latex macros\n",
    "\\newcommand{\\mat}[1]{\\begin{pmatrix} #1 \\end{pmatrix}}\n",
    "\\newcommand{\\p}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\b}[1]{\\boldsymbol{#1}}\n",
    "\\newcommand{\\c}[1]{\\mathcal{#1}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "* Evolutionary algorithms\n",
    "\n",
    "\n",
    "* Game theory\n",
    "\n",
    "\n",
    "* Evolutionary game theory\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evolutionary Algorithms\n",
    "\n",
    "Our brain is a product of the long history of evolution.\n",
    "In order to understand the present organization and functioning of our brain, it is mandatory to look back into its evolutionary history.\n",
    "\n",
    "Learning and evolution are complementary mechanisms for adaptation in different time and spatial scales.\n",
    "Learning happens within the lifetime of each single agent, within days to seconds.\n",
    "Evolution occurs across multiple generations of a group of agents constituting a species or a colony.\n",
    "\n",
    "The mechanisms of learning in the brain, such as the network architecture, dynamics of neurons, and synaptic plasticity rules, are shaped by evolution to suite the features of the environment and the agents' sensory-motor sytems.\n",
    "Learning can also affect the course of evolution, by providing a wider variety of phenotypes through interaction with the physical and social environments.\n",
    "\n",
    "*Evolutionary algorithms* are a class of adaptation/optimization algorithms that mimick the process of biological evolution. The major components are:\n",
    "* diversity in population\n",
    "* selection by fitness\n",
    "* variation by mutation/crossover\n",
    "\n",
    "Compared to other optimization methods like Newton's method or gradient descent, their major features are\n",
    "* global optimization\n",
    "* little assumption on the objective function\n",
    "* no gradient computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variants of evolutionary algorithms\n",
    "\n",
    "* Genetic algorithm (GA) \t(Holland 1976)\n",
    "    * binary coding\n",
    "\n",
    "\n",
    "* Evolutionary strategy\t\t(Rechenberg & Schwefel 1965)\n",
    "    * real numbers\n",
    "\n",
    "\n",
    "* Evolutionary programming\t(Fogel 1966)\n",
    "    * genotype is a finite state machine\n",
    "\n",
    "\n",
    "* Genetic programming (GP)\t(Koza 1992)\n",
    "    * phenotype is a program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genotype/phenotype coding\n",
    "\n",
    "* Binary coding: $b_N,...,b_1$\n",
    "\n",
    "$$ x = \\sum_{i=1}^N b_n 2^{n–1} $$\n",
    "Some flip of a bit can cuase a big jump.\n",
    "\n",
    "* Real numbers: $(x_1,...,x_N)$\n",
    "\n",
    "* Genotype to phenotype mapping\n",
    "    * input to a fitness function\n",
    "    * parameters of a controller for behavior\n",
    "\n",
    "### Selection\n",
    "\n",
    "* Roulett wheel selection (RWS):\n",
    "The selection probability $p_i$ for an individual of fitness $f_i$ is given by\n",
    "\n",
    "$$ p_i = \\frac{f_i}{\\sum_{i=1}^P f_i} $$\n",
    "where $P$ is the population size.\n",
    "All $f_i$ have to be non-negative.\n",
    "\n",
    "* Selection by ranking:\n",
    "\n",
    "$$ p_i \\propto P - rank_i $$\n",
    "\n",
    "* Tournament selection:\n",
    "Keep winners of subsets made of $k$ individuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutation\n",
    "\n",
    "* Binary coding:\n",
    "random filp of each bit\n",
    "\n",
    "* Real coding:\n",
    "add random noise,\n",
    "e.g. Gaussian: $x’ = x + \\c{N}(0,\\sigma^2)$\n",
    "\n",
    "### Crossover\n",
    "\n",
    "* Binary coding:\n",
    "spliciing of bit sequences\n",
    "\n",
    "* Blend crossover:\n",
    "\n",
    "$$ x’ \\in [x_1–\\alpha(x_2–x_1), x_2+\\alpha(x_2–x_1)] $$\n",
    "\n",
    "* Sampling from parents’ distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from mpl_toolkits.mplot3d import Axes3D # for 3D plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EvoAlg:\n",
    "    \"\"\"A basic class of evolutionary algorithms\"\"\"\n",
    "\n",
    "    def __init__(self, fitfun, genes, amut=0.1, pcross=0.1):\n",
    "        \"\"\"Create a new population\n",
    "        genes: (P,C) array of int or float\n",
    "        sigma: mutation size\"\"\"\n",
    "        self.fitfun = fitfun   # fitness function\n",
    "        self.genes = np.array(genes)  # initial genotypes\n",
    "        self.P = self.genes.shape[0]  # population size\n",
    "        self.C = self.genes.shape[1]  # genetic code length\n",
    "        self.amut = amut  # mutation amplitude\n",
    "        self.pcross = pcross  # crossover probability\n",
    "        self.fits = np.zeros(self.P)  # individual fitnesses\n",
    "        self.best = []    # current best solution\n",
    "        self.record = []  # record of best fitness\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"Evaluate individuals\"\"\"\n",
    "        for i, g in enumerate(self.genes):\n",
    "            self.fits[i] = self.fitfun(g)\n",
    "        self.best = self.genes[np.argmax(self.fits)]  # best solution\n",
    "        self.record.append(max(self.fits))  # record best fitness\n",
    "\n",
    "    def select(self):\n",
    "        \"\"\"Roulette selection\"\"\"\n",
    "        prob = self.fits - min(self.fits)  # make sure non-negative\n",
    "        prob = prob/sum(prob)  # normalize\n",
    "        selected = np.zeros(self.P, dtype=np.int)  # indices of selected\n",
    "        for i in range(self.P):\n",
    "            selected[i] = list(np.random.multinomial(1,prob)).index(1) # multinulli\n",
    "        self.genes = self.genes[selected]  # update the genome\n",
    "            \n",
    "    def crossover(self, pcross=None):\n",
    "        \"\"\"Point crossover\"\"\"\n",
    "        if pcross is None:\n",
    "            pcross = self.pcross  # use the default\n",
    "        # make random pairs (assume even population)\n",
    "        pairs = np.random.permutation(self.P).reshape((-1,2))\n",
    "        ncross = int(self.P*pcross)  # number of crossed pairs\n",
    "        for p, q in pairs[:ncross]:\n",
    "            #if np.random.random() > self.pcross:\n",
    "            #    break  # no crossover\n",
    "            cp = np.random.randint(self.C-1) + 1  # cross point\n",
    "            #print(p, q, cp)\n",
    "            gene0 = np.r_[self.genes[p,:cp], self.genes[q,cp:]]\n",
    "            gene1 = np.r_[self.genes[q,:cp], self.genes[p,cp:]]\n",
    "            self.genes[p] = gene0\n",
    "            self.genes[q] = gene1\n",
    "\n",
    "    def mutate(self, amut=None):\n",
    "        \"\"\"Mutation by gaussian noise\"\"\"\n",
    "        if amut is None:\n",
    "            amut = self.amut  # use the default\n",
    "        self.genes += amut*np.random.randn(self.P, self.C)\n",
    "\n",
    "    def generation(self, amut=None, pcross=None):\n",
    "        \"\"\"One generation\"\"\"\n",
    "        self.evaluate()\n",
    "        self.select()\n",
    "        self.crossover(pcross)\n",
    "        self.mutate(amut)\n",
    "        return self.fits  # fitness distribution\n",
    "\n",
    "    def evolve(self, gen=100, amut=None, pcross=None):\n",
    "        \"\"\"Evolve multiple generations\"\"\"\n",
    "        for t in range(gen):\n",
    "            self.generation(amut, pcross)\n",
    "        return self.record  # record of best fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Example of Ex-Or neural network\n",
    "def exor_net(w):\n",
    "    \"\"\"Fitness of 2-2-1 network to ExOr; w[9]\"\"\"\n",
    "    X = np.array([[0,0], [0,1], [1,0], [1,1]])  # input\n",
    "    T = np.array([0, 1, 1, 0])  # target\n",
    "    w = w.reshape((3,3))  # 3 units with bias and two weights\n",
    "    err = 0\n",
    "    for x, t in zip(X, T):\n",
    "        h = 1/(1+np.exp(-(w[:2,0] + w[:2,1:]@x)))  # hidden units\n",
    "        y = w[-1,0] + np.dot(w[-1,1:],h)  # output\n",
    "        err += (y - t)**2\n",
    "        #print(h, y, t)\n",
    "    return 1 - err  # fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create an instance of evolving ExOr network\n",
    "evexor = EvoAlg(exor_net, genes=np.random.randn(100,9), amut=0.05, pcross=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run one generation\n",
    "fits = evexor.generation()\n",
    "plt.hist(fits)\n",
    "plt.xlabel(\"fitness\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize weight distribution\n",
    "evexor.evaluate()  # \n",
    "for i in range(3):\n",
    "    plt.subplot(1,3,i+1)  # plot input weight pairs\n",
    "    plt.scatter(evexor.genes[:,3*i+1], evexor.genes[:,3*i+2], c=evexor.fits);\n",
    "    plt.axis('square')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evolve generations\n",
    "# You may need to tun this many times, or restart if gets stuck\n",
    "record = evexor.evolve(amut=0.05, pcross=0.05)\n",
    "plt.plot(record)\n",
    "plt.xlabel(\"generation\"); plt.ylabel(\"fitness\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning and evolution\n",
    "\n",
    "Most learning algorithms have some parameters that need to be tuned.\n",
    "In reinforcement learning, design of the reward function is a critical issue.\n",
    "One way to design them is to apply an evolutionary algorithm.\n",
    "\n",
    "Elfwing et al. (2011) developed a distributed evolutionary framework for a group of robots and demonstrated that reward functions as well as reinforcement learning paramters can be optimized for the task of survival and reproduction.\n",
    "\n",
    ":::{figure-md} Fig_ElfwingRewards\n",
    " ![Elfwing11rewards](figures/Elfwing11rewards.png)\n",
    "\n",
    "Evolution of reward functions for finding a battery pack in the survival task (left) and for the face of another robot in the mating task (right).\n",
    ":::\n",
    "\n",
    ":::{figure-md} Fig_ElfwingParameters\n",
    "![Elfwing11parameters](figures/Elfwing11parameters.png)\n",
    "\n",
    "Evolution of parameters for reinforcement learning: $\\alpha$: learning rate; $\\tau$: temperature for exploration; $\\gamma$: temporal discounting; $\\lambda$: eligibility trace decay.\n",
    ":::\n",
    "\n",
    "The mechanisms for learning, such as network architecutres and synaptic plasticity rules, are also products of evolution. \n",
    "Niv et al. (2002) showed in a reinforcement learning task that reward-dependent plastcity rule can be evoloved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game Theory\n",
    "\n",
    "*Game theory* is a mathematical framework for modeling the interactions among multiple agents.\n",
    "\n",
    "In game theory, what we call *agents* in reinforcement learning is called *players*, *policy* is called *strategy*, and *reward* or *fitness* is called *pay-off*.\n",
    "\n",
    "* player $i \\in I = \\{1,...,n\\}$\n",
    "\n",
    "* *pure strategy* $s \\in S = \\{1,...,m\\}$\n",
    "\n",
    "* *pure strategy profile* for all players ($m^n$ cases):\n",
    "\n",
    "$$ \\b{s} = (s_1,...,s_n) $$\n",
    "\n",
    "* *payoff* for player $i$:\n",
    "\n",
    "$$ \\pi_i(\\b{s}) = \\pi_i(s_1,...,s_n) $$\n",
    "\n",
    "* payoff function for all players:\n",
    "\n",
    "$$ \\pi(\\b{s}) = (\\pi_1(\\b{s}),...,\\pi_n(\\b{s})) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Prisoner’s dilemma\n",
    "\n",
    "Two prisoners are under interrogation.\n",
    "They can *cooperate* by remaining silent to be discharged with no punishment.\n",
    "If both *defect* and tell the truth, they are each given a small punshment.\n",
    "If only one of them defects for plea bargaining, he/she is given a small reward while the other who tries to cooperate by remaining silent is given a big punishment.\n",
    "\n",
    "|you get/buddy gets | buddy cooperates | buddy defects |\n",
    "|-------------:|:---------:|:-------:|\n",
    "| you cooperate |  0 / 0  | -4/1 |\n",
    "| you defect | 1 / -4 | -1 / -1 |\n",
    "\n",
    "What is the best thing to do in this case?\n",
    "Is cooperation a viable strategy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixted strategy\n",
    "\n",
    "A player's strategy can be stochastic. That is called *mixed strategy* in game theory.\n",
    "\n",
    "* *mixed strategy* : probability of pure strategies \n",
    "\n",
    "$$\\b{x} = (x_{1},...,x_{m}) $$\n",
    "\n",
    "* *mixed strategy profile* for all players: \n",
    "\n",
    "$$X = \\mat{\\b{x}_1 \\\\ \\vdots \\\\ \\b{x}_n} \n",
    "   = \\mat{x_{11} & \\cdots & x_{1m}\\\\\n",
    "    \\vdots & & \\vdots\\\\\n",
    "    x_{n1} & \\cdots & x_{nm}}$$\n",
    "\n",
    "\n",
    "* probability of a pure strategy profile $\\b{s}$:\n",
    "\n",
    "$$ x(\\b{s}) = p(s_1,...,s_n|X) = x_{1,s_1}\\cdots x_{n,s_n} $$\n",
    "\n",
    "* payoff for player $i$ under mixed strategy prifile $X$: \n",
    "\n",
    "$$ u_i(X) = \\sum_\\b{s} x(\\b{s}) \\pi_i(\\b{s})\n",
    "   = \\sum_\\b{s} x_{1,s_1}\\cdots x_{n,s_n} \\pi_i(s_1,...,s_n) $$\n",
    "\n",
    "* combined payoff function\n",
    "\n",
    "$$ \\b{u}(X) = (u_1(X),...,u_n(X)) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-Player Game\n",
    "\n",
    "Pay-off matrix for *row player*\n",
    "\n",
    "$$ A: a_{s_1,s_2}=\\pi_1(s_1,s_2) $$\n",
    "\n",
    "Pay-off matrix for *column player*\n",
    "\n",
    "$$ B: b_{s_1,s_2}=\\pi_2(s_1,s_2) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples\n",
    "\n",
    "* Prisoner’s dilemma: $S=\\{cooperate, defect\\}$\n",
    "\n",
    "$$ A = \\mat{0 & -4 \\\\ 1 & -1}, \\  B = \\mat{0 & 1 \\\\ -4 & -1} $$\n",
    "\n",
    "* Matching pennies:  $S=\\{head, tail\\}$\n",
    "\n",
    "$$ A = \\mat{1 & -1 \\\\ -1 & 1}, \\  B = \\mat{-1 & 1 \\\\ 1 & -1} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nash Equilibrium\n",
    "\n",
    "If a player knows other players' strategies, what would he/she do to maximize own payoff?\n",
    "\n",
    "*Best reply* for player $i$ is a mixed strategy $\\b{x}_i$ that maximizes the payoff given a mixed strategy profile of all players $X=\\mat{\\b{x}_1 \\\\ \\vdots \\\\ \\b{x}_n}$.\n",
    "\n",
    "*Nash equilibrium* is defined as a mixed strategy profile $X$ that is a best reply to itself.\n",
    "\n",
    "In other words, any strategy other than $\\b{x}_i$ does not give a higher payoff to player $i$ while others keep their strategies $\\b{x}_j (j\\neq i)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples\n",
    "\n",
    "* Prisoner’s dilemma: $S=\\{cooperate, defect\\}$\n",
    "\n",
    "$$ A = \\mat{0 & -4 \\\\ 1 & -1}, \\  B = \\mat{0 & 1 \\\\ -4 & -1} $$\n",
    "\n",
    "both defect $ \\b{x}_1 = \\b{x}_2 = (0,1) $\n",
    "\n",
    "\n",
    "* Matching pennies: $S=\\{head, tail\\}$\n",
    "\n",
    "$$ A = \\mat{1 & -1 \\\\ -1 & 1}, \\  B = \\mat{-1 & 1 \\\\ 1 & -1} $$\n",
    "\n",
    "stochastic $ \\b{x}_1 = \\b{x}_2 = (\\frac{1}{2},\\frac{1}{2}) $\n",
    "\n",
    "\n",
    "* Coordination game: $S=\\{left, right\\}$\n",
    "\n",
    "$$ A = \\mat{2 & 0 \\\\ 0 & 1}, \\  B = A $$\n",
    "\n",
    "both go left: $ \\b{x}_1 = \\b{x}_2 = (1,0) $\n",
    "\n",
    "both go right: $ \\b{x}_1 = \\b{x}_2 = (0,1) $\n",
    "\n",
    "stochastic: $ \\b{x}_1 = \\b{x}_2 = (\\frac{2}{3},\\frac{1}{3}) $\n",
    "\n",
    "* Hawk-Dove game:  $S=\\{fight, yield\\}$\n",
    "    * $v$: value of winning\n",
    "    * $c$: cost of fighting\n",
    "\n",
    "$$ A = \\mat{\\frac{v-c}{2} & v \\\\ 0 & \\frac{v}{2}}, \\  B = A^T $$\n",
    "\n",
    "For examples, $v=2$ and $c=6$:\n",
    "\n",
    "$$ A = \\mat{-2 & 2 \\\\ 0 & 1}, \\ B = \\mat{-2 & 0 \\\\ 2 & 1} $$\n",
    "\n",
    "player 1 is hawk, player 2 is dove: $ \\b{x}_1 = (1,0), \\b{x}_2 = (0,1) $\n",
    "\n",
    "player 1 is dove, player 2 is hawk: $ \\b{x}_1 = (0,1), \\b{x}_2 = (1,0) $\n",
    "\n",
    "sometimes hawk, sometimes dove: $ \\b{x}_1 = \\b{x}_2 = (\\frac{1}{3},\\frac{2}{3}) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Game:\n",
    "    \"\"\"Visualization and simulation for game theory\"\"\"\n",
    "\n",
    "    def __init__(self, paymat):\n",
    "        \"\"\"Setup a game with payoff matrices\"\"\"\n",
    "        self.paymat = paymat  # list of payoff matrices\n",
    "        self.N = len(paymat)  # number of players\n",
    "    \n",
    "    def payoff(self, X):\n",
    "        \"\"\"Payoffs for a mixed strategy profile X\"\"\"\n",
    "        pay0 = X[0]@self.paymat[0]@X[1]\n",
    "        pay1 = X[0]@self.paymat[1]@X[1]\n",
    "        return [pay0, pay1]\n",
    "        \n",
    "    def best_reply(self, X):\n",
    "        \"\"\"Best pure strategies for a mixed strategy profile X\"\"\"\n",
    "        pay0 = self.paymat[0]@X[1]  # payoffs for pure strategies\n",
    "        pay1 = X[0]@self.paymat[1]\n",
    "        return [np.argmax(pay0), np.argmax(pay1)]\n",
    "        # this misses mixed strategies of two best pure strategies\n",
    "      \n",
    "    def plot_payoff(self):\n",
    "        \"\"\"visualize payoff for mixed strategiec for a two-player game\"\"\"\n",
    "        # xg, yg = np.mgrid[0:1.01:0.05, 0:1.01:0.05]\n",
    "        p = np.linspace(0., 1., 21) # prob of taking a strategy\n",
    "        x = np.array([1-p, p])   # mixed strategies from [1,0] to [0,1]\n",
    "        # Payoff surfaces\n",
    "        pay0 = x.T@self.paymat[0]@x \n",
    "        pay1 = x.T@self.paymat[1]@x \n",
    "        # best reply lines\n",
    "        best_reps = np.zeros((len(p), 2))\n",
    "        for j, xj in enumerate(x.T):\n",
    "            best_reps[j] = self.best_reply([xj, xj])\n",
    "        #print(best_reps)\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.imshow(pay0, extent=(0,1, 1,0))\n",
    "        plt.plot(p, best_reps[:,0], c=\"r\", lw=4)  # best replies for player 0\n",
    "        plt.title(\"P1 payoff\"); plt.ylabel(\"P1 strategy\");  plt.xlabel(\"P2 strategy\");\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.imshow(pay1, extent=(0,1, 1,0))\n",
    "        plt.plot(best_reps[:,1], p, c=\"r\", lw=4)  # best replies for player 1\n",
    "        plt.title(\"P2 Payoff\"); plt.xlabel(\"P2 strategy\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prisoner's dilemma\n",
    "A = np.array([[0,-4], [1,-1]])\n",
    "pd = Game([A, A.T])\n",
    "pd.plot_payoff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matching pennies\n",
    "A = np.array([[1,-1], [-1,1]])\n",
    "B = np.array([[-1,1], [1,-1]])\n",
    "mp = Game([A, B])\n",
    "mp.plot_payoff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coordination game\n",
    "A = np.array([[2,0], [0,1]])\n",
    "cg = Game([A, A])\n",
    "cg.plot_payoff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hawk-Dove game\n",
    "A = np.array([[-2,2], [0,1]])\n",
    "hd = Game([A, A.T])\n",
    "hd.plot_payoff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symmetric two-player games\n",
    "\n",
    "If the same rule applies to both players:\n",
    "\n",
    "$$ \\pi_2(s_1,s_2) = \\pi_1(s_2,s_1) $$\n",
    "\n",
    "or\n",
    "\n",
    "$$ B = A^T $$\n",
    "\n",
    "it is called a *symmetric game*.  \n",
    "In the above examples, the three except matching pennies are symmetric games.\n",
    "\n",
    "If two players share the same payoff in a symmetric game:\n",
    "\n",
    "$$ A = B = A^T $$\n",
    "\n",
    "it is called a *partnership game* or *doubly symmetric game*.  \n",
    "In the above examples, coordination game is such an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal form of symmetric two-player games\n",
    "\n",
    "The best reply does not change if the same amount is added/subtracted from each column of the payoff matrix of a row player.\n",
    "\n",
    "Thus any symmetric two-player game\n",
    "\n",
    "$$ A = \\mat{a_{11} & a_{12} \\\\ a_{21} & a_{22}}, \\  B = A^T $$\n",
    "\n",
    "can be converted into a normal form\n",
    "\n",
    "$$ A' = \\mat{a_1 & 0 \\\\ 0 & a_2} $$\n",
    "\n",
    "by shifting the payoffs as\n",
    "\n",
    "$$ a_1 = a_{11}-a_{21}, \\  a_2 = a_{22}-a_{12}. $$\n",
    "\n",
    "Then we can classify all symmetric two-player games into three categories.\n",
    "\n",
    "#### Category I: $a_1<0$, $a_2>0$ or $a_1>0$, $a_2<0$\n",
    "\n",
    "e.g. Prisoner's dilemma:\n",
    "\n",
    "$$ A = \\mat{0 & -4 \\\\ 1 & -1} \\rightarrow A' = \\mat{-1 & 0 \\\\ 0 & 3} $$ \n",
    "\n",
    "* One symmetric Nash equilibrium:\n",
    "    * $ \\b{x}_1 = \\b{x}_2 = (1,0) $ for $a_1>0$\n",
    "    * $ \\b{x}_1 = \\b{x}_2 = (0,1) $ for $a_2>0$\n",
    "\n",
    "#### Category II: $a_1>0$, $a_2>0$\n",
    "\n",
    "e.g. Coordination game:\n",
    "\n",
    "$$ A = \\mat{2 & 0 \\\\ 0 & 1} $$\n",
    "\n",
    "* Three symmetric Nash equilibria\n",
    "    * $ \\b{x}_1 = \\b{x}_2 = (1,0) $\n",
    "    * $ \\b{x}_1 = \\b{x}_2 = (0,1) $\n",
    "    * $ \\b{x}_1 = \\b{x}_2 = (\\frac{a_1}{a_1+a_2},\\frac{a_2}{a_1+a_2}) $\n",
    "\n",
    "#### Category III: $a_1<0$, $a_2<0$\n",
    "\n",
    "e.g. Hawk-Dove game:\n",
    "\n",
    "$$A = \\mat{-2 & 2 \\\\ 0 & 1}\n",
    "  \\rightarrow A' =  \\mat{-2 & 0 \\\\ 0 & -1} $$\n",
    "\n",
    "* Two asymmetric and one symmetric Nash equilibria\n",
    "    * $ \\b{x}_1 = (1,0), \\b{x}_2 = (0,1) $\n",
    "    * $ \\b{x}_1 = (0,1), \\b{x}_2 = (1,0) $\n",
    "    * $ \\b{x}_1 = \\b{x}_2 = (\\frac{a_2}{a_1+a_2},\\frac{a_1}{a_1+a_2}) $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evolutionary game theory\n",
    "\n",
    "The Nash equilibrium in the game theory is an important tool for assessing what strategies a rational (selfish) player would take.\n",
    "However, some Nash equilibriua like the mixed strategy in the Coordination game (sometimes left, sometimes right) does not look like a good strategy.\n",
    "\n",
    "*Evolutionary Game Theory* considers what strategy is stable against introduction of mutants.\n",
    "\n",
    "## Evolutionarily stable strategy (ESS)\n",
    "\n",
    "Here we consider a large population of players. \n",
    "Pairs are randomly drawn and play a symmetric two-player game\n",
    "\n",
    "$$ B = A^T $$\n",
    "\n",
    "When most players take a mixed strategy $\\b{x}$, \n",
    "can a small mutant population taking another strategy $\\b{y}$ outperform others?\n",
    "\n",
    "When a strategy $\\b{x}$ outperforms any mutant starategy $\\b{x}$,\n",
    "it is called an *Evolutionarily stable strategy (ESS)*.\n",
    "\n",
    "We denote the payoff for a strategy $\\b{x}$ against $\\b{y}$ as \n",
    "\n",
    "$$ u(\\b{x},\\b{y}) = \\b{x} A \\b{y}^T $$\n",
    "\n",
    "With a proportion of mutants $0<\\epsilon<<1$, the average population strategy is\n",
    "\n",
    "$$ \\b{w} = \\epsilon\\b{y} + (1-\\epsilon)\\b{x} $$\n",
    "\n",
    "ESS is a stragety $\\b{x}$ that satisfy\n",
    "\n",
    "$$ u(\\b{x},\\b{w})>u(\\b{y},\\b{w}) $$\n",
    "for any other strategy $\\b{y}$. \n",
    "\n",
    "ESS can be the first order:\n",
    "\n",
    "$$ u(\\b{x},\\b{x})>u(\\b{y},\\b{x}) $$\n",
    "performing better with incumbents, \n",
    "or the second order:\n",
    "\n",
    "$$ u(\\b{x},\\b{x})=u(\\b{y},\\b{x}) \\mbox{ and } u(\\b{x},\\b{y})>u(\\b{y},\\b{y}) $$\n",
    "performing equally with incumbents but better with mutants than themselves.\n",
    "\n",
    "ESS does not require\n",
    "\n",
    "$$ u(\\b{x},\\b{x}) > u(\\b{y},\\b{y}), $$\n",
    "meaning that in another population where $\\b{y}$ is a majority, $\\b{y}$ can outperform $\\b{x}$ and be another ESS.\n",
    "\n",
    "ESS is the best reply to itself, so that $(\\b{x},\\b{x})$ is a Nash equilibrium, but not all Nash equilibria are ESS, as we see below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EES in symmetric two-player games\n",
    "\n",
    "Based on the categorization by the normal forms, let us see which Nash equilibrim is ESS.\n",
    "\n",
    "#### Category I: $a_1<0$, $a_2>0$ or $a_1>0$, $a_2<0$\n",
    "\n",
    "e.g. Prisoner's dilemma:\n",
    "\n",
    "$$ A = \\mat{0 & -4 \\\\ 1 & -1} \\rightarrow A' = \\mat{-1 & 0 \\\\ 0 & 3} $$ \n",
    "\n",
    "* One symmetric Nash equilibrium, which is ESS.\n",
    "    * $ \\b{x}_1 = \\b{x}_2 = (1,0) $ for $a_1>0$  \n",
    "    * $ \\b{x}_1 = \\b{x}_2 = (0,1) $ for $a_2>0$\n",
    "\n",
    "#### Category II: $a_1>0$, $a_2>0$\n",
    "\n",
    "e.g. Coordination game:\n",
    "\n",
    "$$ A = \\mat{2 & 0 \\\\ 0 & 1} $$\n",
    "\n",
    "* Three symmetric Nash equilibria:\n",
    "    * $ \\b{x}_1 = \\b{x}_2 = (1,0) $: ESS\n",
    "    * $ \\b{x}_1 = \\b{x}_2 = (0,1) $: ESS\n",
    "    * $ \\b{x}_1 = \\b{x}_2 = (\\frac{a_1}{a_1+a_2},\\frac{a_2}{a_1+a_2}) $: not ESS\n",
    "\n",
    "#### Category III: $a_1<0$, $a_2<0$\n",
    "\n",
    "e.g. Hawk-Dove game:\n",
    "\n",
    "$$A = \\mat{-2 & 2 \\\\ 0 & 1}\n",
    "  \\rightarrow A' =  \\mat{-2 & 0 \\\\ 0 & -1} $$\n",
    "\n",
    "* Two asymmetric and one symmetric Nash equilibria: \n",
    "    * $ \\b{x}_1 = (1,0), \\b{x}_2 = (0,1) $: not ESS\n",
    "    * $ \\b{x}_1 = (0,1), \\b{x}_2 = (1,0) $: not ESS\n",
    "    * $ \\b{x}_1 = \\b{x}_2 = (\\frac{a_2}{a_1+a_2},\\frac{a_1}{a_1+a_2}) $: ESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replicator Dynamics\n",
    "\n",
    "In addition to the analyzing stablilty a given strategy, evolutionary game theory allows us to predict how strategies would evolve in time.\n",
    "\n",
    "For simplicity we assume that each subpopulation takes one of $K$ pure strategies.\n",
    "* $p_i(t)$: number of players with strategy $i\\in K$\n",
    "* total population: $p(t)=\\sum_{i=1}^K p_i(t)$\n",
    "* population share: $x_i(t)=\\frac{p_i(t)}{p(t)}$\n",
    "* population state: \n",
    "\n",
    "$$ x(t) = (x_1(t),...,x_K(t)) $$\n",
    "\n",
    "If the reproduction rate is given by the payoff and fitness, the death rate is given by $d$, the growth of sub-population is given by\n",
    "\n",
    "$$\n",
    "    \\frac{dp_i(t)}{dt} = (\\b{e}_i A\\b{x}(t)^T–d)p_i(t)\n",
    "$$\n",
    "where $\\b{e}_i=(0,...,1,...,0)$ with $1$ in $i$-th component.\n",
    "\n",
    "Then the dynamics of the population share is given by\n",
    "\n",
    "$$\n",
    "    \\frac{dx_i(t)}{dt} = \\frac{d}{dt} \\frac{p_i(t)}{p(t)}\n",
    "    = \\frac{1}{p(t)}\\frac{dp_i(t)}{dt} - \\frac{p_i(t)}{p(t)^2}\\frac{dp(t)}{dt}\n",
    "$$\n",
    "\n",
    "$$\n",
    "    = (\\b{e}_i A\\b{x}(t)^T–d)x_i(t) - x_i(t)\\sum_{j=1}^K(\\b{e}_j A\\b{x}(t)^T–d)x_j(t)\n",
    "$$\n",
    "\n",
    "$$\n",
    "    = \\b{e}_i A\\b{x}(t)^T x_i(t) –\\b{x}(t)A\\b{x}(t)^T x_i(t) \n",
    "$$\n",
    "which leads us to the *replicator equation*.\n",
    "\n",
    "$$\n",
    "    \\frac{dx_i(t)}{dt} = (\\b{e}_i–\\b{x}(t))A\\b{x}(t)^T x_i(t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.integrate import odeint\n",
    "\n",
    "class RepGame(Game):\n",
    "    \"\"\"Replicator dynamics\"\"\"\n",
    "    \n",
    "    def repeq(self, x, t=0):\n",
    "        \"\"\"replicator equaiton for odeint\"\"\"\n",
    "        dx = np.ravel(self.paymat[0]@x - x@self.paymat[0]@x)*x\n",
    "        return dx\n",
    "    \n",
    "    def replicator(self, x0, tt):\n",
    "        \"\"\"Replicator dynamimcs\n",
    "        x0: initial population, tt: array of time points\"\"\"\n",
    "        xt = odeint(self.repeq, x0, tt)\n",
    "        return xt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prisonner's dilemma\n",
    "A = np.matrix([[0,-4], [1,-1]])  # payoff matrix\n",
    "pd = RepGame([A, A.T])\n",
    "tt = np.arange(0, 10, 0.1)\n",
    "xt = pd.replicator(np.array([0.9, 0.1]), tt)\n",
    "plt.plot(tt, xt)\n",
    "plt.xlabel(\"time\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coordination game\n",
    "A = np.array([[2,0], [0,1]])\n",
    "cg = RepGame([A, A])\n",
    "tt = np.arange(0, 10, 0.1)\n",
    "xt = cg.replicator(np.array([0.4, 0.6]), tt)\n",
    "plt.plot(tt, xt)\n",
    "xt = cg.replicator(np.array([0.3, 0.7]), tt)\n",
    "plt.plot(tt, xt, ':')\n",
    "plt.xlabel(\"time\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hawk-Dove game\n",
    "A = np.array([[-2,2], [0,1]])\n",
    "hd = RepGame([A, A.T])\n",
    "tt = np.arange(0, 10, 0.1)\n",
    "xt = hd.replicator(np.array([0.9, 0.1]), tt)\n",
    "plt.plot(tt, xt)\n",
    "plt.xlabel(\"time\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rock-scissors-paper game\n",
    "Now consider a symmetric two-player game with three pure strategies\n",
    "\n",
    "$$ A = \\mat{0 & 1+a & -1 \\\\ -1 & 0 & 1+a \\\\ 1+a & -1 & 0} $$\n",
    "\n",
    "The parameter $a$ controls the importance of winning versus not losing.\n",
    "\n",
    "There is a Nash equilibrium at $\\b{x}=(\\frac{1}{3},\\frac{1}{3},\\frac{1}{3})$. The replicator dynamics is known to show oscillatory behaviors around the equilibrium depending on the parameter $a$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rock-scissors-paper game\n",
    "a = 1  # also try a = 1, a= -0.5\n",
    "A = np.array([[0, 1+a, -1], [-1, 0, 1+a], [1+a, -1, 0]])\n",
    "rsp = RepGame([A, A.T])\n",
    "tt = np.arange(0, 50, 0.1)\n",
    "xt = rsp.replicator(np.array([0.4, 0.3, 0.3]), tt)\n",
    "plt.plot(tt, xt)\n",
    "plt.xlabel(\"time\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(xt[0,0], xt[0,1], \"ro\")  # initial point\n",
    "plt.plot(xt[:,0], xt[:,1])\n",
    "plt.xlabel(\"x_1\"); plt.ylabel(\"x_2\"); plt.axis(\"square\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning by multiple agents\n",
    "\n",
    "In evolutionary game thoery, each player is assumed to have a fixed strategy for its liftime.\n",
    "What if players can change their strategies during their lifetime depending on their experiences.\n",
    "\n",
    "Reinforcement learning of *multiple agents* is an active field of research.\n",
    "For each agent, other agents can be considered as a part of the environment, but because the policies of other agents change by learning, it is not a stationary Markov decision process.\n",
    "We can simply apply standard reinforcement learning like Q-learning, but in that case convergence is not theoretically guaranteed.\n",
    "In learning the rock-scissors-paper game, appearance of chaotic dynamics has been reported (Sato et al. 2002).\n",
    "\n",
    "Another possible approach is to learn a model of the behaviors of other agents, or even a model of how other agents change their behavior by learning, and the apply model-based approach to find appropriate actions (Suzuki et al. 2012)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game thoery in neuroscience \n",
    "\n",
    "Game theoretic settings have been used extensively in cognitive neuroscience research to assess the brain's mechanisms for cooperateive or selfish behaviors.\n",
    "\n",
    "* Prisoner’s Dilemma\n",
    "    * Activation of reward circuit for cooperation (Rilling et al., 2002)\n",
    "    * Reduced serotonin reduces cooperation (Wood et al., 2006)\n",
    "\n",
    "\n",
    "* Ultimatum Game (Sanfey et al., 2003)  \n",
    "Proposer: split money for self and other  \n",
    "Responder: accept or reject\n",
    "\n",
    "## Theory of mind\n",
    "\n",
    "The capacity for estimating and predicting other's mental state is called *theory of mind*.\n",
    "Human children acquire such capability during the course of development, but it has been suggested that that capacity is impaired in autistic children.\n",
    "\n",
    "There are computational models about how model-bases reinforcement learning agents can perform cooperative behaviors (Yoshida et al. 2008).\n",
    "There are also brain imaging exeperiments to assess what brain circuits are involved in predicting other's behaviors (Yoshida et al. 2010; Suzuki et al. 2014)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "1) Try evolutionary algorithm for an optimization of your interest while varying the methods and parameters for mutaion, crossover, and selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Model some kind of interaction between people in a game theoretic framework, define payoff matrices, and identify Nash equilbria and ESSs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) See how the replicator dynamics of the rock-scissors-paper game changes with the parameter $a$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## References\n",
    "\n",
    "* Evolutionary algorithms\n",
    "    * Yu X, Gen M (2012) Introduction to Evolutionary Algorithms. Springer.\n",
    "    * Reisinger J, Miikkulainen R (2007) Acquiring Evolvability through Adaptive Representations. GECCO 2007.\n",
    "    * Elfwing S, Uchibe E, Doya K, Christensen HI (2011) Darwinian embodied evolution of the learning ability for survival. Adaptive Behavior 19:101-120.\n",
    "    * Niv Y, Joel D, Meilijson I, Ruppin E (2002) Evolution of reinforcement learning in uncertain environments: a simple explanation for complex foraging behaviors. Adaptive Behavior.\n",
    "\n",
    "\n",
    "* Game theory\n",
    "    * Weibull J (1995) Evolutionary Game Theory. MIT Press.\n",
    "    * Rilling JK, Gutman DA, Zeh TR, Pagnoni G, Berns GS, Kilts CD (2002) A neural basis for social cooperation. Neuron 35:395-405.\n",
    "    * Sanfey AG, Rilling JK, Aronson JA, Nystrom LE, Cohen JD (2003) The neural basis of economic decision-making in the Ultimatum Game. Science 300:1755-1758.\n",
    "    * Barraclough DJ, Conroy ML, Lee D (2004) Prefrontal cortex and decision making in a mixed-strategy game. Nature neuroscience 7:404-410.\n",
    "    \n",
    "    \n",
    "* Multi-agent RL\n",
    "    * Sato Y, Akiyama E, Farmer JD (2002) Chaos in learning a simple two-person game. Proceedings of the National Academy of Sciences, USA, 99:4748-4751.\n",
    "    * Jaderberg M, Czarnecki WM, Dunning I, Marris L, Lever G, Castaneda AG, Beattie C, Rabinowitz NC, Morcos AS, Ruderman A, Sonnerat N, Green T, Deason L, Leibo JZ, Silver D, Hassabis D, Kavukcuoglu K, Graepel T (2019). Human-level performance in 3D multiplayer games with population-based reinforcement learning. Science, 364, 859-865. https://doi.org/10.1126/science.aau6249\n",
    "    \n",
    "    \n",
    "* Theory of mind\n",
    "    * Yoshida W, Dolan RJ, Friston KJ (2008) Game theory of mind. PLoS Computational Biology 4:e1000254.\n",
    "    * Yoshida W, Seymour B, Friston KJ, Dolan RJ (2010) Neural mechanisms of belief inference during cooperative games. The Journal of Neuroscience, 30:10744-10751.\n",
    "    * Suzuki S, Harasawa N, Ueno K, Gardner JL, Ichinohe N, Haruno M, Cheng K, Nakahara H (2012) Learning to simulate others' decisions. Neuron, 74:1125-1137.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author to do list\n",
    "* Evolutionar robotics examples, e.g., Nolfi and Floreano\n",
    "* Game learning issues and examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
