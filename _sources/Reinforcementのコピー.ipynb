{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4. Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ % Latex macros\n",
    "\\newcommand{\\mat}[1]{\\begin{pmatrix} #1 \\end{pmatrix}}\n",
    "\\newcommand{\\p}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\b}[1]{\\boldsymbol{#1}}\n",
    "\\newcommand{\\w}{\\boldsymbol{w}}\n",
    "\\newcommand{\\x}{\\boldsymbol{x}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "* Bandit problem\n",
    "* Markov decision process (MDP)\n",
    "* Value functions\n",
    "* Dynamic programming (DP)\n",
    "* Q-learning and SARSA\n",
    "* Actor-Critic\n",
    "* Applications\n",
    "\n",
    "\n",
    "* Neural Mechanisms\n",
    "    * Dopamine neurons\n",
    "    * Basal ganglia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In reinforcement learning, an *agent* interacts with the *environment* by observing its state, sending an action, and receiving some reward.\n",
    "\n",
    "> ![ReinforcementLearning](figures/RL.jpg)\n",
    ">Reinforcement learning in an agent-environment loop.\n",
    "\n",
    "The aim of reinforcement learning is to find a *policy*, a mapping $\\pi$ from any state $s$ to an action $a$, that maximize the reward acquired.\n",
    "\n",
    "* a deterministic policy $a=\\pi(s)$\n",
    "* a stochastic policy $\\pi(s,a)=p(a|s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bandit problem\n",
    "A simple case is that the state does not change, or change irrespective of the agent's action.\n",
    "In this case, the problem is simply to estimate the expected reward for each action and pick the one with highest expected reward.\n",
    "\n",
    "$$R(s,a) = E[r|s,a]$$\n",
    "$$\\pi(s) = \\arg\\max_a R(s,a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov decision process (MDP)\n",
    "A general setup for reinforcement learning is the Markov decision process (MDP), which is characterized by \n",
    "* finite state $s \\in S$  \n",
    "* finite actions $a\\in A(s)$\n",
    "* state transition rule $P(s'|s,a)$\n",
    "* reward function $R(s,a,s') = E[r|s,a,s']$\n",
    "\n",
    "There are two conventions for the time index of reward. The reward following $a_t$ is denoted as $r_t$ or $r_{t+1}$ depending on the literature. Here we take the convention by Sutton and Barto (2018):\n",
    "$$(s_t,a_t)\\rightarrow (r_{t+1},s_{t+1})$$\n",
    "\n",
    "In a dynamical environment, an action at time $t$ may affect the reward at later times $r_\\tau$, $\\tau > t$ through the state dynamics.  \n",
    "From the other viewpoint, a reward at time $t$ may depend on all the past states and actions at $\\tau<t$.\n",
    "\n",
    "Thus in selecting an action, an agent needs to consider maximizing expected cumulative reward, called *return*\n",
    "* finite horizon till time $T$: $R_t = r_{t+1} + r_{t+2} + ... + r_T$ \n",
    "\n",
    "* infinite horizon: $R_t = r_{t+1} + \\gamma r_{t+2} + \\gamma^2 r_{t+3} + ...$  \n",
    "where $0\\le\\gamma<1$ is a *discount factor* to guarantee the sum to be finite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Functions\n",
    "To evaluate the goodness of states, actions, and policy, a critical tool is the *value function*. There are two types.\n",
    "\n",
    "* State value function: expected return starting from a state $s$ by following a policy $\\pi$\n",
    "\n",
    "$$V^\\pi(s) \\equiv E_\\pi[R_t|s_t=s]\\\\ = E_\\pi[\\sum_k^\\infty \\gamma^k r_{t+k+1}|s_t=s]$$\n",
    "\n",
    "* (State-)Action value function: expected return by taking an action $a$ at a state $s$, and then following a policy $\\pi$\n",
    "\n",
    "$$Q^\\pi(s,a) \\equiv E_\\pi[R_t|s_t=s,a_t=a]\\\\ = E_\\pi[\\sum_k^\\infty \\gamma^k r_{t+k+1}|s_t=s,a_t=a]$$\n",
    "\n",
    "An optimal policy $\\pi^*$ for and MDP is defined as a policy that satisfies\n",
    "\n",
    "$$V^{\\pi^*}(s) \\ge V^{\\pi}(s)$$\n",
    "for any state $s\\in S$ and any policy $\\pi$.\n",
    "\n",
    "The value functions for an optimal policy is called *optimal value function* and denoted as $V^*(s)=V^{\\pi^*}(s)$.  An MDP can have multiple optimal policies, but the optimal value function is unique.\n",
    "\n",
    "An optimal state value function and action value function are related by\n",
    "\n",
    "$$V^*(s) = \\max_aQ^*(s,a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving MDPs\n",
    "There are two main approaches in solving MDP problems\n",
    "\n",
    "### Model-based approach: Dynamic programming\n",
    "When the state transition function $p(s'|s,a)$ and the reward function $r(s,a)$ are known or learned:\n",
    "\n",
    "* Solve the *Bellman equation* for the optimal value function:\n",
    "\n",
    "$$V^*(s) = \\max_a E[ r(s,a) + \\gamma V^*(s')]\\\\\n",
    "= \\max_a [r(s,a) + \\sum_{s'}p(s'|s,a)\\gamma V^*(s')] $$\n",
    "\n",
    "* Use the optimal policy\n",
    "\n",
    "$$a = \\pi^*(s) = \\arg\\max_a E[ r(s,a) + \\gamma V^*(s')]$$\n",
    "\n",
    "* Representative algorithms:\n",
    "    * Policy iteration\n",
    "    * Value iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-free approach: Reinforcement learning\n",
    "Do not assume that $p(s'|s,a)$ and $r(s,a)$ are unknown\n",
    "\n",
    "* Learn from the sequence of experience:\n",
    "\n",
    "$${s,a,r,s,a,r,â€¦}$$\n",
    "\n",
    "* Representative algorithms:\n",
    "    * SARSA\n",
    "    * Q-learning\n",
    "    * Actor-Critic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARSA and Q Learning\n",
    "* Estimate the action value function $Q(s,a)$ using a table or ANN.\n",
    "\n",
    "* Select an action using the action value function\n",
    "    * greedy: $a = \\arg\\max_a Q(s,a)$\n",
    "    * $\\epsilon$-greedy: random action with prob. $\\epsilon$ and greedy with prob. $1-\\epsilon$\n",
    "    * Boltzman: \n",
    "    \n",
    "    $$p(a|s) = \\frac{e^{\\beta Q(s,a)}}{\\sum_{b\\in A}e^{\\beta Q(s,b)}}$$\n",
    "\n",
    "* Check the inconsistency of action value function estimates by the temporal difference (TD) error:\n",
    "\n",
    "SARSA (on-policy):\n",
    "\n",
    "$$\\delta_t = r_{t+1} + \\gamma Q(s_{t+1},a_{t+1}) - Q(s_t,a_t)$$\n",
    "\n",
    "Q learning (off-policy): assuming greedy policy from the next state\n",
    "\n",
    "$$\\delta_t = r_{t+1} + \\gamma \\max_{a'\\in A}Q(s_{t+1},a') - Q(s_t,a_t)$$\n",
    "\n",
    "* Update the action value function of the previous state and action in proportion to the TD error\n",
    "\n",
    "$$\\Delta Q(s_t,a_t) = \\alpha \\delta_t$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes for minimum environment and agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    \"\"\"Class for a reinforcement learning environment\"\"\"\n",
    "    \n",
    "    def __init__(self, nstate=3, naction=2):\n",
    "        \"\"\"Create a new environment\"\"\"\n",
    "        self.Ns = nstate   # number of states\n",
    "        self.Na = naction  # number of actions\n",
    "        \n",
    "    def start(self):\n",
    "        \"\"\"start an episode\"\"\"\n",
    "        # randomly pick a state\n",
    "        self.state = np.random.randint(self.Ns)\n",
    "        return(self.state)\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"step by an action\"\"\"\n",
    "        # random reward\n",
    "        self.reward = np.random.random()  # between 0 and 1\n",
    "        # random next state\n",
    "        self.state = np.random.randint(self.Ns)\n",
    "        return(self.reward, self.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"Class for a reinforcement learning agent\"\"\"\n",
    "    \n",
    "    def __init__(self, nstate, naction):\n",
    "        \"\"\"Create a new agent\"\"\"\n",
    "        self.Ns = nstate   # number of states\n",
    "        self.Na = naction  # number of actions\n",
    "        \n",
    "    def start(self, state):\n",
    "        \"\"\"first action, without reward feedback\"\"\"\n",
    "        # randomly pick an action\n",
    "        self.action = np.random.randint(self.Na)\n",
    "        return(self.action)\n",
    "    \n",
    "    def step(self, reward, state):\n",
    "        \"\"\"learn by reward and take an action\"\"\"\n",
    "        # do nothing for reward\n",
    "        # randomly pick an action\n",
    "        self.action = np.random.randint(self.Na)\n",
    "        return(self.action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinforcement non-learning\n",
    "# Create the environment and the agent\n",
    "env = Environment()\n",
    "agent = Agent(env.Ns, env.Na)\n",
    "# First contact\n",
    "state = env.start()\n",
    "action = agent.start(state)\n",
    "print([state, action])\n",
    "# Repeat interactoin\n",
    "for t in range(10):\n",
    "    reward, state = env.step(action)\n",
    "    action = agent.step(reward, state)\n",
    "    print([t, reward, state, action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RL:\n",
    "    \"\"\"Reinforcement learning by interacton of Environment and Agent\"\"\"\n",
    "\n",
    "    def __init__(self, environment, agent, nstate, naction):\n",
    "        \"\"\"Create the environment and the agent\"\"\"\n",
    "        self.env = environment(nstate, naction)\n",
    "        self.agent = agent(nstate, naction)\n",
    "    \n",
    "    def episode(self, tmax=50):\n",
    "        \"\"\"One episode\"\"\"\n",
    "        # First contact\n",
    "        state = self.env.start()\n",
    "        action = self.agent.start(state)\n",
    "        # Table of t, r, s, a\n",
    "        Trsa = np.zeros((tmax+1, 4))\n",
    "        Trsa[0] = [0, 0, state, action]\n",
    "        # Repeat interactoin\n",
    "        for t in range(1, tmax+1):\n",
    "            reward, state = self.env.step(action)\n",
    "            action = self.agent.step(reward, state)\n",
    "            Trsa[t,:] = [t, reward, state, action]\n",
    "        return(Trsa)\n",
    "    \n",
    "    def run(self, nrun=10, tmax=50):\n",
    "        \"\"\"Multiple runs of episodes\"\"\"\n",
    "        Return = np.zeros(nrun)\n",
    "        for n in range(nrun):\n",
    "            r = self.episode(tmax)[:,1]  # reward sequence\n",
    "            Return[n] = sum(r)\n",
    "        return(Return)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Pain-Gain environment\n",
    "\n",
    ">![PainGain](figures/PainGain.jpg)\n",
    ">The Pain-Gain environment.\n",
    "\n",
    "This is a Markov Decision Process that was designed for a functional MRI experiment (Tanaka et al. 2004).\n",
    "The environment has four states and two possible actions.\n",
    "By taking action 1, the state shift to the left and a positive reward is given, except at the leftmost state where it a big negative reward is given.\n",
    "By action 2, the state shift to the right and a negative reward is given, but at the rightmost state, a big positive reward is obtained.\n",
    "What is the right policy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PainGain(Environment):\n",
    "    \"\"\"Pain-Gain environment \"\"\"\n",
    "    \n",
    "    def __init__(self, nstate=4, naction=2, gain=6):\n",
    "        super().__init__(nstate, naction)\n",
    "        # setup the reward function as an array\n",
    "        self.R = np.ones((self.Ns, self.Na)) # small gains\n",
    "        self.R[:,1] = -1   # small pains for 2nd action (a=1)\n",
    "        self.R[0,0] = -gain  # large pain\n",
    "        self.R[-1,1] = gain  # large gain\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"step by an action\"\"\"\n",
    "        # reward by the reward matrix\n",
    "        self.reward = self.R[self.state, action]\n",
    "        # move left or right and circle\n",
    "        self.state = (self.state + 2*action-1)%self.Ns  \n",
    "        return(self.reward, self.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QL(Agent):\n",
    "    \"\"\"Class for a Q-learning agent\"\"\"\n",
    "\n",
    "    def __init__(self, nstate, naction):\n",
    "        super().__init__(nstate, naction)\n",
    "        # allocate Q table\n",
    "        self.Q = np.zeros((nstate, naction))\n",
    "        # default parameters\n",
    "        self.alpha = 0.1  # learning rate\n",
    "        self.beta = 1.0   # inverse temperature\n",
    "        self.gamma = 0.9  # discount factor\n",
    "    \n",
    "    def boltzmann(self, q):\n",
    "        \"\"\"Boltzmann selection\"\"\"\n",
    "        p = np.exp( self.beta*q)   # unnormalized probability\n",
    "        p = p/sum(p)    # probability\n",
    "        # sample by multinoulli (categorical) distribution\n",
    "        s = np.random.multinomial(1,p)\n",
    "        return list(s).index(1)  # find the index of 1\n",
    "        #return(np.searchsorted( np.cumsum(p), np.random.random()))\n",
    "\n",
    "    def start(self, state):\n",
    "        \"\"\"first action, without reward feedback\"\"\"\n",
    "        # Boltzmann action selection\n",
    "        self.action = self.boltzmann( self.Q[state,:])\n",
    "        # remember the state\n",
    "        self.state = state\n",
    "        return(self.action)\n",
    "    \n",
    "    def step(self, reward, state):\n",
    "        \"\"\"learn by reward and take an action\"\"\"\n",
    "        # TD error: self.state/action retains the previous ones\n",
    "        delta = reward + self.gamma*max(self.Q[state,:]) - self.Q[self.state,self.action]\n",
    "        # Update the value for previous state and action\n",
    "        self.Q[self.state,self.action] += self.alpha*delta\n",
    "        # Boltzmann action selection\n",
    "        self.action = self.boltzmann( self.Q[state,:])\n",
    "        # remember the state\n",
    "        self.state = state\n",
    "        return(self.action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pain-Gain environment and Q-learning agent\n",
    "pgq = RL(PainGain, QL, 4, 2)\n",
    "# customize parameters\n",
    "pgq.agent.beta = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgq.agent.gamma = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run an episode\n",
    "trsa = pgq.episode(100)\n",
    "plt.plot(trsa[:,1:]);\n",
    "plt.legend((\"reward\", \"state\", \"action\"));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Q function\n",
    "plt.bar(np.arange(4)-0.2, pgq.agent.Q[:,0], 0.4 )  # action0\n",
    "plt.bar(np.arange(4)+0.2, pgq.agent.Q[:,1], 0.4 );  # action1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat episodes for learning curve\n",
    "pgq = RL(PainGain, QL, 4, 2)\n",
    "R = pgq.run(50, 10)\n",
    "plt.plot(R);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Actor-Critic\n",
    "Another popular class of RL is *Actor-Critic*.\n",
    "\n",
    "Actor implements a policy $\\pi_\\w=p(a|s;\\w)$, where $\\w$ is the parameter, such as the elements of a table or the weights of an ANN.\n",
    "\n",
    "Critic learns the state value function of the actor's policy $\\pi_\\w$:\n",
    "\n",
    "$$V^{\\pi_\\w}(s) = E_{\\pi_\\w}[ r_{t+1} + \\gamma r_{t+2} + \\gamma^2 r_{t+3} + ...|s_t=s]$$\n",
    "using a table or an ANN.\n",
    "\n",
    "Learning is based on the temporal difference (TD) error:\n",
    "\n",
    "$$\\delta_t = r_{t+1} + \\gamma V(s_{t+1}) - V(s_t)$$\n",
    "\n",
    "The TD error is used for learning of both critic and actor, but in different ways:\n",
    "\n",
    "Critic:\n",
    "\n",
    "$$\\Delta V(s_t) = \\alpha \\delta_t$$\n",
    "\n",
    "Actor: \n",
    "\n",
    "$$\\Delta \\w = \\alpha_w \\delta_t \\p{p(a_t|s_t;\\w)}{\\w}$$\n",
    "or\n",
    "\n",
    "$$\\Delta \\w = \\alpha_w \\delta_t \\p{\\log p(a_t|s_t;\\w)}{\\w}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dopamine neurons\n",
    "Dopamine neurons in the midbrain appear to code TD error (Schultz et al. 1997).\n",
    "\n",
    "% ![Dopamine](figures/Schultz97.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basal ganglia\n",
    "\n",
    ">![BGforRL](figures/BGRL.jpg)\n",
    ">A reinforcement learning model of the basal ganglia (Doya 2007)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## References\n",
    "\n",
    "* Sutton RS, Barto AG (2018). Reinforcement Learning: An Introduction, 2nd edition. MIT Press.  \n",
    "(http://incompleteideas.net/book/the-book-2nd.html)\n",
    "\n",
    "* Barto AG, Sutton RS, Andersen CW (1983). Neuronlike adaptive elements that can solve difficult learning control problems. IEEE Transactions on Systems, Man, and Cybernetics, 13, 834-846. \n",
    "http://doi.org/10.1109/TSMC.1983.6313077\n",
    "\n",
    "* Doya K (2007). Reinforcement learning: Computational theory and biological mechanisms. HFSP J, 1, 30-40.\n",
    "http://doi.org/10.2976/1.2732246/10.2976/1\n",
    "\n",
    "\n",
    "### Dopamine neurons\n",
    "\n",
    "* Schultz W, Dayan P, Montague PR (1997). A Neural Substrate of Prediction and Reward. Science, 275, 1593-1599. \n",
    "http://doi.org/10.1126/science.275.5306.1593\n",
    "\n",
    "* Nomoto K, Schultz W, Watanabe T, Sakagami M (2010). Temporally extended dopamine responses to perceptually demanding reward-predictive stimuli. J Neurosci, 30, 10692-702. \n",
    "http://doi.org/10.1523/JNEUROSCI.4828-09.2010\n",
    "\n",
    "\n",
    "### Basal ganglia\n",
    "\n",
    "* Samejima K, Ueda Y, Doya K, Kimura M (2005). Representation of action-specific reward values in the striatum. Science, 310, 1337-40.\n",
    "http://doi.org/10.1126/science.1115270\n",
    "\n",
    "* Ito M, Doya K (2015). Distinct neural representation in the dorsolateral, dorsomedial, and ventral parts of the striatum during fixed- and free-choice tasks. Journal of Neuroscience, 35, 3499-3514. http://doi.org/10.1523/JNEUROSCI.1962-14.2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
