{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5. Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ % Latex macros\n",
    "\\newcommand{\\mat}[1]{\\begin{pmatrix} #1 \\end{pmatrix}}\n",
    "\\newcommand{\\p}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\b}[1]{\\boldsymbol{#1}}\n",
    "\\newcommand{\\c}[1]{\\mathcal{#1}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents    \n",
    "* Clustering (Bishop, Chater 9):\n",
    "    * $k$-means clustering\n",
    "    * mixtures of Gaussians\n",
    "    * self-organizing maps (SOM)\n",
    "\n",
    "\n",
    "* Subspace mapping (Biship, Chapter 12):\n",
    "    * principal component analysis (PCA)\n",
    "    * singular value decomposition (SVD)\n",
    "    * independent component analysis (ICA)\n",
    "    \n",
    "    \n",
    "* Neural Mechanisms\n",
    "    * Receptive field formation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we grow, we learn that there are different things and creatures in the world, such as plants and animals, and in more detail, dogs, cats and humans. \n",
    "What is remarkable is that most of such learning is done spontaneously without explicit teaching about what is a dog, or labels specifying which is a dog or which is a cat.\n",
    "\n",
    "Learning categories without explicit labels is an example of *unspervised learning*.\n",
    "\n",
    "The key in unsupervised learning is to find a certain structure in the distribution of the observed data $\\{\\b{x}_1,...,\\b{x}_N\\}$, and come up with a probabilistic model $p(\\b{x})$ that produced the data.\n",
    "\n",
    "Typical cases are:\n",
    "\n",
    "* Dividing into clusters:\n",
    "    * $k$-means clustering\n",
    "    * mixtures of Gaussians\n",
    "    * hierarchical clustering  \n",
    "\n",
    "* Decomposing into components:\n",
    "    * principal component analysis (PCA)\n",
    "    * singular value decomposition (SVD)\n",
    "    * independent component analysis (ICA)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scikit-learn\n",
    "[scikit-learn](http://scikit-learn.org/stable/) is a popular Pyhon libraries of supervised and unsupervised learning algorithms and sample datasets.\n",
    "\n",
    "You can explore the algorithms in the tutorial:\n",
    "[tutorials](http://scikit-learn.org/stable/tutorial/machine_learning_map)\n",
    "\n",
    "Here we take a classic dataset `iris` from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "# the iris dataset\n",
    "iris = datasets.load_iris()\n",
    "#print(iris.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data  # flower features\n",
    "C = iris.target  # flower class\n",
    "N, D = X.shape\n",
    "print(N, D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a visualization of the 4D data in 2D each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X[:,0], X[:,1], c=C);  # sepal length, width\n",
    "plt.xlabel('X[0]'); plt.ylabel('X[1]'); \n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X[:,2], X[:,3], c=C);  # petal length, width\n",
    "plt.xlabel('X[2]'); plt.ylabel('X[3]'); \n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means Clustering\n",
    "\n",
    "The most basic method of clustering is *$K$-means clustering*, \n",
    "which divides a data set $\\{\\b{x}_1,…,\\b{x}_N\\}$ into $K$ clusters.\n",
    "\n",
    "We define prototypes $\\b{\\mu}_k$ for $k=1,...,K$ clusters and specify belonging of data points by binary *indicator variables* $r_{nk}\\in\\{0,1\\}$.\n",
    "\n",
    "A good clustering is achieved by minimizing the *distortion measure*\n",
    "\n",
    "$$ J = \\sum_{n=1}^N \\sum_{k=1}^K r_{nk}||\\b{x}_n-\\b{\\mu}_k||^2. $$\n",
    "\n",
    "which is a sum of distances of the data point from its prototype.\n",
    "\n",
    "We do that by repeating the following steps:\n",
    "\n",
    "* For the current prototypes $\\b{\\mu}_k$, re-assign each data point $\\b{x}_n$ by finding the nearest $\\b{\\mu}_k$ and setting $r_{nk} = 1$ and $r_{n j\\ne k} = 0$.\n",
    "    \n",
    "* For the current assignment by $r_{nk}$, update the prototypes by\n",
    "\n",
    "$$ \\b{\\mu}_k = \\frac{\\sum_{n=1}^N r_{nk}\\b{x}_n}{\\sum_{n=1}^N r_{nk}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a sample implementation of K-means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeans:\n",
    "    \"\"\"K-Means clustering\"\"\"\n",
    "    def __init__(self, data, K=2):\n",
    "        \"\"\"data: NxD data matrix\n",
    "        K: number of clusters\"\"\"\n",
    "        self.X = data  # store dataset\n",
    "        self.N, self.D = data.shape  # data dimensions\n",
    "        self.K = K  # number of clusters\n",
    "        self.C = np.zeros(self.N, dtype=int)   # cluster indices for data points\n",
    "        self.R = np.zeros((self.N,self.K), dtype=int)   # indicator matrix\n",
    "        self.Mu = data[np.random.randint(0, self.N, self.K),:]   # pick K points randomly\n",
    "        \n",
    "    def update(self):\n",
    "        # Re-assign to clusters\n",
    "        self.R[:] = 0  # clear the indicator variables\n",
    "        for n in range(self.N):\n",
    "            # check the distances\n",
    "            dist = [ np.sum((self.X[n]-self.Mu[k])**2) for k in range(self.K)]\n",
    "            # find the nearest mean\n",
    "            self.C[n] = np.argmin(dist)  # new cluster assignment\n",
    "            self.R[n, self.C[n]] = 1  # new indicator variable\n",
    "        # Update the means\n",
    "        for k in range(self.K):\n",
    "            self.Mu[k] = self.R[:,k]@self.X/np.sum(self.R[:,k])\n",
    "    \n",
    "    def run(self, imax=20):\n",
    "        \"\"\"repeat until convergence\"\"\"\n",
    "        for i in range(imax):\n",
    "            R0 = self.R.copy() # previous indicators\n",
    "            self.update()\n",
    "            if np.array_equal(self.R, R0):\n",
    "                print('converged with iteration', i)\n",
    "                break\n",
    "        return self.C, self.R, self.Mu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try applying it to the last 2D of the iris dataset for easy visualization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the algorithm\n",
    "km = KMeans(X[:,2:], K=3)\n",
    "print( km.N, km.D, km.K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See step by step\n",
    "km.update()\n",
    "#print(km.C)\n",
    "plt.scatter(X[:,2], X[:,3], c=km.C, marker='.')  # data colored by cluster\n",
    "plt.scatter(km.Mu[:,0], km.Mu[:,1], c=range(km.K), marker='+', s=200);  # means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(X[:,2:], K=3)\n",
    "up = 6  # updates\n",
    "for u in range(up):\n",
    "    km.update()\n",
    "    plt.subplot(2, int(np.ceil(up/2)), u+1)\n",
    "    plt.title(f\"update {u+1}\")\n",
    "    plt.scatter(X[:,2], X[:,3], c=km.C, marker='.')  # data colored by cluster\n",
    "    plt.scatter(km.Mu[:,0], km.Mu[:,1], c=range(km.K), marker='+', s=200)  # means\n",
    "plt.tight_layout()\n",
    "print(km.C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please try K smaller or larger than 3, or original 4D space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixtures of Gaussians\n",
    "\n",
    "It is often the case that clusters have some overlaps and assignment is probabilistic. *Mixtures of Gaussians* is a probabilistic extention of $K$-means clustering.\n",
    "\n",
    "*Gaussian mixture distribution* has a form\n",
    "\n",
    "$$ p(\\b{x}) = \\sum_{k=1}^K \\pi_{k} \\mathcal{N}(\\b{x};\\b{\\mu}_k, \\Sigma_k) $$\n",
    "\n",
    "where $\\b{\\mu}_k$ and $\\Sigma_k$ are the mean vector and the covariance matrix of $k$-th Gaussian distribution and $\\pi_k$ is the mixture probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability density function (PDF) of a d-dimension Gaussian distribution is given by:\n",
    "\n",
    "$$ \\mathcal{N}(\\b{x};\\b{\\mu}, \\Sigma) = \n",
    "\\frac{1}{\\sqrt{(2\\pi)^d \\det\\Sigma}} \\exp[-\\frac{1}{2}(\\b{x}-\\b{\\mu})^T\\Sigma^{-1}(\\b{x}-\\b{\\mu})].$$\n",
    "\n",
    "Here we use the `multivariate_normal` function in scipy to visualize the PDF of a Mixture of Gaussians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probability density of a Mixture of Gaussians\n",
    "def mixgauss_pdf(x, pi, mu, sigma):\n",
    "    K, D = np.array(mu).shape  # classes and data dimension\n",
    "    p = np.zeros(x.shape[:-1])\n",
    "    for k in range(K):\n",
    "        p += pi[k]*multivariate_normal.pdf(x, mu[k], sigma[k])\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = [0.3, 0.7]    # mixture probability\n",
    "mu = [[0,0], [4,2]]     # means\n",
    "sigma = [[[1,-1],[-1,2]], [[2,1],[1,1]]]   # variances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.linspace(-3, 8)\n",
    "x1 = np.linspace(-5, 7)\n",
    "X0, X1 = np.meshgrid(x0, x1)\n",
    "P = mixgauss_pdf(np.stack((X0,X1), axis=2), pi, mu, sigma) \n",
    "plt.contour(X0, X1, P);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us generate data from this Mixture Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample from a Mixture Gaussian distribution\n",
    "def mixgauss_sample(pi, mu, sigma):\n",
    "    K = len(pi)\n",
    "    k = np.random.choice(K, p=pi)\n",
    "    x = np.random.multivariate_normal(mu[k], sigma[k])\n",
    "    return x, k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 500\n",
    "X = np.zeros((N,2))\n",
    "C = np.zeros(N, dtype=int)\n",
    "for n in range(N):\n",
    "    X[n,:], C[n] = mixgauss_sample(pi, mu, sigma)\n",
    "plt.contour(X0, X1, P);\n",
    "plt.scatter(X[:,0], X[:,1], c=C, marker='.');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expectation-Maximization (EM) Algorithm\n",
    "\n",
    "For finding the parameters $\\pi_k$, $\\mu_k$ and $\\Sigma_k$ $(k=1,...,K)$ for a give dataset, a standard way is to repeat the following steps, called the *expectation-maximization (EM)* algorithm:\n",
    "\n",
    "### E (expectation) step\n",
    "\n",
    "We consider a binary stochastic variable\n",
    "\n",
    "$$ \\b{z}=(z_1,…,z_K) $$\n",
    "\n",
    "where $z_k\\in\\{0,1\\}$ and $\\sum_{k=1}^K z_k=1$, indicating which Gaussian a data point belongs to.\n",
    "\n",
    "For the current parameters $(\\pi_k,\\mu_k$,$\\Sigma_k)$, evaluate the posterior probability of $\\b{z}$ from the prior probability $\\pi$ and the likelihood for the data $\\b{x}$, called *responsibility*:\n",
    "\n",
    "$$ r_{nk} = p(z_k=1|\\b{x}_n) = \\frac{\\pi_k\\c{N}(\\b{x}_n|\\b{\\mu}_k,\\Sigma_k)}{\\sum_{j=1}^K \\pi_j\\c{N}(\\b{x}_n|\\b{\\mu}_j,\\Sigma_j)} $$\n",
    "\n",
    "### M (maximization) step\n",
    "\n",
    "For the current responsibility, update the parameters as the maximum likelihood estimate for the data weighted by the responsibility:\n",
    "\n",
    "$$ N_k = \\sum_{n=1}^N r_{nk} $$\n",
    "\n",
    "$$ \\pi_k = \\frac{N_k}{N} $$\n",
    "\n",
    "$$ \\b{\\mu}_k = \\frac{1}{N_k}\\sum_{n=1}^N r_{nk}\\b{x}_n $$\n",
    "\n",
    "$$ \\Sigma_k = \\frac{1}{N_k}\\sum_{n=1}^N r_{nk}(\\b{x}_n-\\b{\\mu}_k)(\\b{x}_n-\\b{\\mu}_k)^T $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixGauss:\n",
    "    \"\"\"MixGauss: Mixture of Gaussians\"\"\"\n",
    "    def __init__(self, data, K=2):\n",
    "        \"\"\"X: NxD data matrix\n",
    "        K: number of Gaussians\"\"\"\n",
    "        self.X = data\n",
    "        self.N, self.D = data.shape\n",
    "        self.K = K\n",
    "        self.Pi = np.ones(self.K)/self.K   # cluster probability\n",
    "        self.R = np.zeros((self.N,self.K))   # responsibility matrix\n",
    "        self.Mu = data[np.random.randint(0, self.N, self.K),:]   # pick K points randomly\n",
    "        self.Sig = np.repeat(np.cov(data.T).reshape(1,self.D,self.D), self.K, axis=0)   # covariance for entire data\n",
    "\n",
    "    def update(self):\n",
    "        pr = np.zeros(self.K)   # data probability for each cluster\n",
    "        \"\"\"EM update\"\"\"\n",
    "        # Expectation step\n",
    "        for n in range(self.N):\n",
    "            for k in range(self.K):\n",
    "                pr[k] = multivariate_normal.pdf(self.X[n], self.Mu[k], self.Sig[k])\n",
    "            # responsibility\n",
    "            self.R[n,:] = pr/np.sum(pr)    # responsibility p(z)\n",
    "        # Maximization step\n",
    "        num = np.sum(self.R, axis=0);    # effective numbers for each class\n",
    "        self.Pi = num/self.N    # class prior\n",
    "        for k in range(self.K):\n",
    "            self.Mu[k,:] = np.sum(self.R[:,k]*self.X.T, axis=1)/num[k]\n",
    "            dX = self.X - self.Mu[k,:]\n",
    "            self.Sig[k] = self.R[:,k]/num[k]*dX.T@dX  # cluster covariance    \n",
    "\n",
    "    def fit(self, imax=100, conv=1e-6):\n",
    "        \"\"\"repeat until convergence\"\"\"\n",
    "        for i in range(imax):\n",
    "            R0 = self.R.copy()\n",
    "            self.update()\n",
    "            #print(self.R, R0)\n",
    "            if np.sum(abs(self.R-R0))/self.N < conv:\n",
    "                print('converged with iteration', i)\n",
    "                break\n",
    "        return self.R, self.Mu, self.Sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mg = MixGauss(X, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mg.update()\n",
    "#print(\"mu =\", mg.Mu, \"\\nsig = \", mg.Sig)\n",
    "plt.scatter(X[:,0], X[:,1], c=np.dot(mg.R,np.arange(mg.K)), marker='.');\n",
    "plt.scatter(mg.Mu[:,0], mg.Mu[:,1], c=range(mg.K), marker='+', s=200);  # means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mg.fit()\n",
    "plt.scatter(X[:,0], X[:,1], c=np.dot(mg.R,np.arange(mg.K)), marker='.');\n",
    "plt.scatter(mg.Mu[:,0], mg.Mu[:,1], c=range(mg.K), marker='+', s=200);  # means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try with the iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data  # flower features\n",
    "C = iris.target  # flower class\n",
    "mg = MixGauss(X, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mg.update()\n",
    "plt.scatter(X[:,2], X[:,3], c=np.dot(mg.R,np.arange(mg.K)), marker='.');\n",
    "plt.scatter(mg.Mu[:,2], mg.Mu[:,3], c=range(mg.K), marker='+', s=200);  # means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mg.fit()\n",
    "plt.scatter(X[:,2], X[:,3], c=np.dot(mg.R,np.arange(mg.K)), marker='.');\n",
    "plt.scatter(mg.Mu[:,2], mg.Mu[:,3], c=range(mg.K), marker='+', s=200);  # means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dirichlet Process Mixture\n",
    "One problem in $K$-means and Mixture of Gaussians is that you have to assume the number of clusters $K$ beforehand.\n",
    "\n",
    "There is an extension called *Dirichlet process mixture* that decides the number of clusters based on the data. See Kevin Murphy (2012) Ch. 25.2 for details.\n",
    "\n",
    "It is available from scikit-learn as `mixture.BayesianGaussianMixture`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical clustering\n",
    "\n",
    "Another popular clustering algorithm is hierarchical clustering, which makes a cluster by the closest pair and gradually grow clusters. The results are often shown by a *dendrogram**.\n",
    "\n",
    "These are available in scipy as `cluster.hierarchy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis\n",
    "Grasping the distribution of a high-dimensional data is not easy for human eyes. We often try to find a low-dimensional projection of the data that characteirizes the distribution.\n",
    "\n",
    "*Principal componet analysis (PCA)* finds the directions of the data distribution with the largest variance. \n",
    "\n",
    "Consider a projection of $D$-dimensional vector $\\b{x} = (x_1,...,x_D)^T$ to \n",
    "$M$-dimensional vector $\\b{y} = (y_1,...,y_M)^T$ by \n",
    "\n",
    "$$ \\b{y} = V \\b{x} $$\n",
    "\n",
    "where $V = (\\b{v}_1,...,\\b{v}_M)^T \\in\\mathbb{R}^{M\\times D}$, $||\\b{v}_m||=1$.\n",
    "\n",
    "For a data set $X=(x_1,...,x_N)^T\\in\\mathbb{R}^{N\\times D}$ with zero mean (mean subtracted),\n",
    "we try to find the projection by $V$ that make the variance of $\\b{y}$ the largest.\n",
    "\n",
    "Using the covariance matrix of the original data\n",
    "\n",
    "$$ C = \\frac{1}{N}X^T X = \\frac{1}{N}\\sum_{n=1}^N \\b{x}_n\\b{x}_n^T \\in\\mathbb{R}^{D\\times D}$$\n",
    "\n",
    "the covariance matrix of the projected data $Y=X V^T$ is given as \n",
    "\n",
    "$$ \\frac{1}{N}Y^T Y = \\frac{1}{N}V X^T X V^T = V C V^T\\in\\mathbb{R}^{M\\times M}.$$\n",
    "\n",
    "The variance of projected data is maximized by solving the eigenvalue problem $C\\b{v}=\\lambda \\b{v}$ and setting the projection matrix \n",
    "$V=(\\b{v}_1,...,\\b{v}_M)^T$ made of the eigenvectors with the largest eigenvalues $\\lambda_1,...,\\lambda_M$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn.datasets offers a wide range of public datasets\n",
    "from sklearn import datasets\n",
    "# the iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data  # flower features\n",
    "T = iris.target  # flower types\n",
    "N, D = X.shape\n",
    "print(N, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the data in 3D\n",
    "from mpl_toolkits.mplot3d import Axes3D # for 3D plotting\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111,projection='3d')\n",
    "ax.scatter(X[:,0], X[:,1], X[:,2], c=T);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data covariance\n",
    "X = X - np.mean(X, axis=0)  # subtract the mean\n",
    "C = X.T@X/N\n",
    "# eigenvalue L[i] and normal eigenvector V[:,i]\n",
    "L, Vt = np.linalg.eigh( C)   # for symmetric matrix\n",
    "#L, V = linalg.eig( C)\n",
    "# in matrix form: C*V=V*L, i.e. C=V*L*V' from V'*V=I\n",
    "print(L)   # eigenvalues\n",
    "print(Vt)   # columns are eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = np.argsort(-L)  # largest first\n",
    "L = L[ind]  # reorder\n",
    "V = Vt.T[ind]  \n",
    "print(V)\n",
    "plt.plot(L)\n",
    "plt.xlabel(\"index m\")\n",
    "plt.ylabel(r\"$\\lambda_m$\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Projection of data to the PC space\n",
    "Z = X@V.T\n",
    "# First two PC\n",
    "plt.scatter(Z[:,0], Z[:,1], c=T);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last two PC\n",
    "plt.scatter(Z[:,2], Z[:,3], c=T);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singular value decomposition (SVD)\n",
    "\n",
    "Singular value decomposition (SVD) tries to represent a $N\\times D$ matrix ($N>D$) by a weighted sum of products of column vectors $\\b{u}_i \\in\\mathbb{R}^N$ and row vectors $\\b{v}_i^T \\in\\mathbb{R}^D$.\n",
    "For example, data may be a mixture of specific patterns $\\b{v}_i$ following certain time courses $\\b{u}_i$.\n",
    "\n",
    "The decomposition is represented as\n",
    "\n",
    "$$ X = \\sum_{i=1}^D s_i \\b{u}_i \\b{v}_i^T = U S V^T$$\n",
    "\n",
    "where \n",
    "\n",
    "$$ U = (\\b{u}_1,...,\\b{u}_D) \\in\\mathbb{R}^{N\\times D} $$\n",
    "\n",
    "$$ S = \\mbox{diag}(s_1,...,s_D) \\in\\mathbb{R}^{D\\times D} $$\n",
    "\n",
    "$$ V = (\\b{v}_1,...,\\b{v}_D) \\in\\mathbb{R}^{D\\times D}. $$\n",
    "\n",
    "This decomposition is achieved by selecting $\\b{v}_i$ as the eigenvectors of $X^T X$ and $s_i= \\sqrt{\\lambda_i}$ by their corresponding eigenvalues ($\\lambda_1 \\ge ... \\ge\\lambda_D$).\n",
    "\n",
    "Because $V$ is the same as that for PCA, the SVD algorithm can also be used for PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# X = U*S*V' where S is a diagonal matrix\n",
    "# i.e. C = X'*X/N = V*S^2/N*V'\n",
    "U, S, Vs = np.linalg.svd(X)\n",
    "# See if they match those derived from covariance matrix\n",
    "print(S**2/N)\n",
    "print(L)\n",
    "print(Vs)\n",
    "print(V)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online PCA\n",
    "\n",
    "It has been shown that a simple linear neural network can perform computation similar to PCA (Sanger 1989). \n",
    "\n",
    "Let us consider a two-layer network \n",
    "\n",
    "$$ \\b{y} = W \\b{x} $$\n",
    "\n",
    "with input $\\b{x}=(x_1,...,x_D)^T$, output $\\b{y}=(y_1,...,y_M)^T$, and $M\\times D$ connection weights $W$ ($M<D$).\n",
    "\n",
    "The *generalized Hebbian algorithm* considers an ordered suppression of the input by the components already captured by other units.\n",
    "\n",
    "In a componet form, it is represented as\n",
    "\n",
    "$$ \\Delta w_{ij} = \\alpha(y_i x_j - y_i\\sum_{k\\le i} w_{kj}y_k) $$\n",
    "\n",
    "$$ = \\alpha[y_i(x_j - \\sum_{k<i} w_{kj}y_k) - y_i^2 w_{ij}] $$\n",
    "\n",
    "In a matrix form, it is represented as\n",
    "\n",
    "$$ \\Delta W = \\alpha(\\b{y}\\b{x}^T - LT[\\b{y}\\b{y}^T]W) $$\n",
    "\n",
    "where $LT[\\ ]$ takes the lower triangle (including the diagonal) of a matrix.\n",
    "\n",
    "It has been shown that the rows of matrix $W$ converges to the $M$ eigenvectors with the largest eigen values of the covariance matrix $E[\\b{x}\\b{x}^T ]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gha(X, W, alpha=0.01):\n",
    "    \"\"\"Generalized Hebbian Alogrithm by Sanger (1989)\"\"\"\n",
    "    N, D = X.shape\n",
    "    for n in range(N):\n",
    "        y = W@X[n,:]\n",
    "        W += alpha*(np.outer(y, X[n,:]) - np.tril(np.outer(y,y))@W)\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iris example\n",
    "M = 2\n",
    "W = np.random.randn(M*D).reshape(M,D)\n",
    "for k in range(10):\n",
    "    W = gha(X, W, alpha=0.01)\n",
    "    print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W/np.linalg.norm(W, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Projection of data to the PC space\n",
    "Z = X@W.T\n",
    "# First two PC\n",
    "plt.scatter(Z[:,0], Z[:,1], c=T);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infomax principle\n",
    "The *infomax* principle is to find a mapping from input $\\b{x}$ to the output $\\b{y}$ so that the mutual information\n",
    "\n",
    "$$ I(\\b{y};\\b{x}) = H(\\b{y}) – H(\\b{y}|\\b{x}) $$\n",
    "\n",
    "is maximized.\n",
    "\n",
    "PCA is an example of infomax by a linear transformation \n",
    "\n",
    "$$\\b{y}=V\\b{x} $$ \n",
    "\n",
    "while keeping the norm of the row vectors of matrix $V$ to be one.\n",
    "\n",
    "For a deterministic mapping or a mapping with homogenetous additive noise \n",
    "\n",
    "$$ \\b{y}=f(\\b{x})+\\b{\\epsilon}, $$\n",
    "\n",
    "$H(\\b{y}|\\b{x})$ is constant, so that\n",
    "* maximization of information transfer $I(\\b{y};\\b{x})$\n",
    "\n",
    "is equivalent to  \n",
    "* maximization of output entropy $H(\\b{y})$.\n",
    "\n",
    "Note that $H(\\b{y})\\le\\sum_i H(y_i)$\n",
    "and the equality is achieved when the output components are independent: $p(\\b{y}) = \\prod_i p(y_i).$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Independent component analysis (ICA)\n",
    "We can listen to a particular speaker's voice or a sound of an instrument among mixed sound.\n",
    "This is called *cocktail party effect* or *blind separation*.\n",
    "\n",
    "For a $D$ independent signal sources $\\b{s}=(s_1,...,s_D)^T$, we consider mixture signal\n",
    "\n",
    "$$ x_j = \\sum_{i=1}^D a_{ji} s_i, $$\n",
    "\n",
    "or in vector form\n",
    "\n",
    "$$ \\b{x} = A \\b{s}. $$\n",
    "\n",
    "*Independent component analysis (ICA)* tries to recover the original signals by\n",
    "\n",
    "$$ \\b{y} = W\\b{x} $$\n",
    "\n",
    "where components of $\\b{y}=(y_1,...,y_D)$ are independent, i.e.,\n",
    "\n",
    "$$ p(\\b{y}) = \\prod_{i=1}^D p(y_i), $$\n",
    "\n",
    "When the amplitude of $y_i$ is bounded, \n",
    "the output entropy $H(\\b{y})\\le\\sum_i H(y_i)$ is maximized when $y_i$ are independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ICA by Infomax\n",
    "If the signal distribution $p(s_i)$ is *supragaussian*, having a sharp peak and long tails, ICA is achieved by a network with saturating output function, such as \n",
    "\n",
    "$$ \\b{u} = W \\b{x} $$\n",
    "\n",
    "$$ y_i = \\tanh(u_i). $$\n",
    "\n",
    "where $\\tanh(u)=\\frac{e^u-e^{-u}}{e^u+e^{-u}}$ is a sigmoid function within $(-1,1)$.\n",
    "\n",
    "In this case, the derivative of the output entropy is given by (Bell & Sejnowski 1995)\n",
    "\n",
    "$$ \\p{H(\\b{y})}{W} = {W^T}^{-1} + \\b{y}\\b{x}^T. $$\n",
    "\n",
    "The entropy is maximized by the *natural gradient* method (Amari 1998),\n",
    "\n",
    "$$ \\Delta W = \\alpha \\p{H(\\b{y})}{W} W^TW $$\n",
    "\n",
    "$$ = \\alpha({W^T}^{-1} - \\b{y}\\b{x}^T) W^TW $$\n",
    "\n",
    "$$ = \\alpha(W-\\b{y}\\b{u}^TW). $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ica(X, W=None, alpha=0.01, online=True):\n",
    "    \"\"\"ICA by max entropy with tanh()\"\"\"\n",
    "    N, D = X.shape\n",
    "    if W is None:\n",
    "        W = np.eye(D)  # initial guess\n",
    "    if online:\n",
    "        for n in range(N):\n",
    "            u = W@X[n,:]  \n",
    "            y = np.tanh(u)\n",
    "            W += alpha*(W - np.outer(y,u@W))\n",
    "    else:  # batch update\n",
    "        U = X@W.T\n",
    "        Y = np.tanh(U)\n",
    "        W += alpha*(W - Y.T@U@W/N)\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make mixed signals of two decaying tones\n",
    "D = 2  # channels\n",
    "N = 2000  # signal length\n",
    "# Source signal\n",
    "S = np.zeros((N, D))\n",
    "for i in range(D):\n",
    "    T = 200  # tone length\n",
    "    M = 10  # number of tones\n",
    "    t = np.arange(T)\n",
    "    s = np.exp(-t/50)*np.sin(0.5*(2+i)*t)  # single tone\n",
    "    ts = np.random.randint(0, N-T, M)  # start timing\n",
    "    for m in range(M):\n",
    "        S[ts[m]:ts[m]+T,i] += s  # add shifted tone\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(S)\n",
    "plt.ylabel(\"S\");\n",
    "# Mixed signal\n",
    "A = np.random.random((D,D))   # random matrix\n",
    "print(A)\n",
    "X = S@A.T\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(X)\n",
    "plt.ylabel(\"X\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applyy ICA to the mixture\n",
    "W = np.eye(D)\n",
    "for k in range(10):\n",
    "    W = ica(X, W, alpha=0.001)\n",
    "    print(W@A)\n",
    "U = X@W.T\n",
    "Y = np.tanh(U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the original and separated signals\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(S)\n",
    "plt.ylabel(\"S\")\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(U)\n",
    "plt.ylabel(\"U\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot t2D signal distributions\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.plot(S[:,0], S[:,1], '.')\n",
    "plt.axis('square')\n",
    "plt.title(\"S\")\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.plot(X[:,0], X[:,1], '.')\n",
    "plt.axis('square')\n",
    "plt.title(\"X\")\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.plot(U[:,0], U[:,1], '.')\n",
    "plt.axis('square')\n",
    "plt.title(\"U\")\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.plot(Y[:,0], Y[:,1], '.')\n",
    "plt.axis('square')\n",
    "plt.title(\"Y\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see how the mixing and separation weights are aligned to mixed data\n",
    "plt.plot(X[:,0], X[:,1], '.')\n",
    "plt.axis('square')\n",
    "plt.title(\"X\")\n",
    "# estimated mixing vectors\n",
    "a = np.linalg.inv(W)\n",
    "a = a/np.max(abs(a))\n",
    "plt.plot([0, a[0,0]], [0, a[0,1]])\n",
    "plt.plot([0, a[1,0]], [0, a[1,1]])\n",
    "# separating vectors\n",
    "w = W/np.max(W)\n",
    "plt.plot([0,w[0,0]], [0,w[0,1]])\n",
    "plt.plot([0,w[1,0]], [0,w[1,1]]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing sound\n",
    "\n",
    "Do you want to hear the mixed and separated sounds?\n",
    "\n",
    "You can use IPython.display library to play sound.  \n",
    "https://ipython.readthedocs.io/en/stable/api/generated/IPython.display.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source sounds\n",
    "for i in range(D):\n",
    "    ipd.display(ipd.Audio(S[:,i], rate=4000))  # 4kHz sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mixed sounds\n",
    "for i in range(D):\n",
    "    ipd.display(ipd.Audio(X[:,i], rate=4000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separated sounds\n",
    "for i in range(D):\n",
    "    ipd.display(ipd.Audio(U[:,i], rate=4000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Receptive fields of visual cortex neurons\n",
    "\n",
    "* Cat visual cortex (Hubel & Wiesel)\n",
    "\n",
    "* Self-organization (von der Malsburgh)\n",
    "\n",
    "* by Informax (Linsker)\n",
    "* by SOM (Obermayer et al.)\n",
    "* by PCA (Sanger et al.)\n",
    "* by ICA (Bell & Sejnowski)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replicating receptive field tuning\n",
    "\n",
    "Sanger (1989) applied *generalized Hebbian learning* to patches from natural images as inputs. \n",
    "The PCs learned as the input weights were similar to the receptive field propertis of visual cortical neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* Bishop CM (2006) Pattern Recognition and Machine Learning. Springer. https://www.microsoft.com/en-us/research/people/cmbishop/prml-book/  \n",
    "    * Chapter 9: Mixture models and EM  \n",
    "    * Chapter 12: Continuous latent variables\n",
    "\n",
    "* Murphy K (2012) Machine learning: A probabilistic perspective. MIT press. https://probml.github.io/pml-book/\n",
    "    * Chapter 25.2: Dirichlet process mixture models \n",
    "\n",
    "### Self Organization\n",
    "\n",
    "* Hubel DH, Wiesel TN (1959). Receptive fields of single neurones in the cat's striate cortex. Journal of Physiology, 148, 574-91. http://doi.org/10.1113/jphysiol.1959.sp006308\n",
    "* Hubel DH, Wiesel TN (1962). Receptive fields, binocular interaction and functional architecture in the cat's visual cortex. Journal of Physiology, 160, 106-154. http://doi.org/10.1113/jphysiol.1962.sp006837\n",
    "\n",
    "* von der Malsburg C (1973). Self-organization of orientation sensitive cells in the striate cortex. Kybernetik, 14, 85-100. https://doi.org/10.1007/BF00288907\n",
    "* Kohonen T (1982). Self-Organized Formation of Topologically Correct Feature Maps. Biol Cybern, 43, 59-69. https://doi.org/10.1007/BF00337288\n",
    "\n",
    "* Olshausen BA, Field DJ (1996). Emergence of simple-cell receptive field properties by learning a sparse code for natural images. Nature, 381, 607-9. https://doi.org/10.1038/381607a0\n",
    "* Rao RP, Ballard DH (1999). Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects. Nat Neurosci, 2, 79-87. https://doi.org/10.1038/4580\n",
    "\n",
    "### PCA\n",
    "\n",
    "* Sanger TD (1989). Optimal unsupervised learning in a single-layer linear feedforward neural network. Neural Networks, 2, 459-473. http://doi.org/10.1016/0893-6080(89)90044-0\n",
    "* Turk M, Pentland A (1991). Eigenfaces for recognition. Journal of Cognitive Neurosciece, 3, 71-86. http://doi.org/10.1162/jocn.1991.3.1.71\n",
    "\n",
    "### Infomax and ICA\n",
    "\n",
    "* Linsker R (1986). From basic network principles to neural architecture. Proceedings of the National Academy of Sciences, USA, 83, 7508-7512, 8390-8394, 8779-8783.  http://doi.org/10.1073/pnas.83.19.7508\n",
    "* Bell AJ, Sejnowski TJ (1995). An information-maxization approach to blind separation and blind deconvolution. Neural Comput, 7, 1129-1159. http://doi.org/10.1162/neco.1995.7.6.1129 \n",
    "* Bell AJ, Sejnowski TJ (1997). The “independent components” of natural scenes are edge filters. Vision Research, 37, 3327-3338. http://doi.org/10.1016/s0042-6989(97)00121-1\n",
    "* Amari S (1998). Natural gradient works efficiently in learning. Neural Comput, 10, 251-276. http://doi.org/10.1162/089976698300017746 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
